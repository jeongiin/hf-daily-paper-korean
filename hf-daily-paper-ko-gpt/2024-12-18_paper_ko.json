[
    {
        "paper": {
            "id": "2412.13147",
            "authors": [
                {
                    "_id": "67623bb9d44ba09e9119fe12",
                    "user": {
                        "_id": "643d26979347842571bc9613",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
                        "isPro": false,
                        "fullname": "Junnan Liu",
                        "user": "jnanliu",
                        "type": "user"
                    },
                    "name": "Junnan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:55:23.904Z",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe13",
                    "name": "Hongwei Liu",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe14",
                    "user": {
                        "_id": "64f58f279eaf9d8fb746cd0e",
                        "avatarUrl": "/avatars/227db72e089026012694ff16cdf102d0.svg",
                        "isPro": false,
                        "fullname": "LinchenXiao",
                        "user": "LinchenXiao",
                        "type": "user"
                    },
                    "name": "Linchen Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:55:51.678Z",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe15",
                    "name": "Ziyi Wang",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe16",
                    "user": {
                        "_id": "63fd691794cc8f815d50c112",
                        "avatarUrl": "/avatars/87305d1cbfcc717e910ccdfaf0568f80.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "Harold-lkk",
                        "type": "user"
                    },
                    "name": "Kuikun Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-18T09:52:14.336Z",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe17",
                    "user": {
                        "_id": "650ab54e23196fb2d86b486b",
                        "avatarUrl": "/avatars/e0506393589695b553ec9ee3fe99b93a.svg",
                        "isPro": false,
                        "fullname": "SongYang Gao",
                        "user": "Wizardcoast",
                        "type": "user"
                    },
                    "name": "Songyang Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:56:48.903Z",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe18",
                    "user": {
                        "_id": "64e8505321540e1da3226b54",
                        "avatarUrl": "/avatars/18958b8406d1ce492b54c1c839f18c54.svg",
                        "isPro": false,
                        "fullname": "Wenwei Zhang",
                        "user": "ZwwWayne",
                        "type": "user"
                    },
                    "name": "Wenwei Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:56:56.862Z",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe19",
                    "user": {
                        "_id": "630716d11801ecc7d2595021",
                        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                        "isPro": false,
                        "fullname": "Songyang Zhang",
                        "user": "zsytony",
                        "type": "user"
                    },
                    "name": "Songyang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-18T09:52:35.610Z",
                    "hidden": false
                },
                {
                    "_id": "67623bb9d44ba09e9119fe1a",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-17T18:12:47.000Z",
            "title": "Are Your LLMs Capable of Stable Reasoning?",
            "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.",
            "upvotes": 54,
            "discussionId": "67623bb9d44ba09e9119fe50"
        },
        "translation_title": "당신의 LLMs는 안정적인 추론 능력을 갖추고 있나요?",
        "translation_summary": "대형 언어 모델(LLMs)의 빠른 발전은 복잡한 추론 작업에서 놀라운 성과를 보여주었습니다. 그러나 벤치마크 성능과 실제 응용 프로그램 간에 상당한 차이가 여전히 존재합니다. 우리는 이 격차가 현재의 평가 프로토콜과 메트릭에서 주로 비롯되며, 특히 정확성과 일관성이 중요한 복잡한 추론 작업에서 LLM의 능력을 충분히 포착하지 못하는 데에서 발생한다고 판단합니다. 본 연구는 두 가지 주요 기여를 합니다. 첫째, 모델 성능의 여러 샘플링 시도에 걸쳐 지속적인 평가를 제공하는 새로운 평가 메트릭 G-Pass@k를 소개합니다. 이 메트릭은 모델의 최상 성능 잠재력과 안정성을 정량적으로 측정합니다. 둘째, 우리는 데이터 유출 위험을 최소화하기 위해 설계된 현대적인 수학 문제로 구성된 동적 벤치마크 LiveMathBench를 제시합니다. G-Pass@k를 사용한 LiveMathBench에 대한 광범위한 실험을 통해 LLM의 최대 능력과 운영 일관성에 대한 포괄적인 통찰력을 제공합니다. 우리의 발견은 LLM의 '현실적인' 추론 능력에 상당한 개선의 여지가 있음을 보여주며, 보다 강 robust한 평가 방법의 필요성을 강조합니다. 벤치마크와 자세한 결과는 https://github.com/open-compass/GPassK 에서 확인할 수 있습니다.",
        "purpose": "복잡한 추론 작업에서 LLM의 성능과 안정성을 정확하게 평가하기 위한 새로운 메트릭과 벤치마크 개발",
        "advertising_copy": "대형 언어 모델의 안정적인 추론 능력에 대한 평가 방법이 필요하신 분들께 추천합니다!",
        "keywords": [
            "Large Language Models",
            "Natural Language Processing",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2412.12606",
            "authors": [
                {
                    "_id": "676240aeb57a82f47d1a81f2",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "YiFan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:54:29.971Z",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f3",
                    "name": "Shanglin Lei",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f4",
                    "name": "Runqi Qiao",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f5",
                    "name": "Zhuoma GongQue",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f6",
                    "name": "Xiaoshuai Song",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f7",
                    "user": {
                        "_id": "61cd4b833dd34ba1985e0753",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png",
                        "isPro": false,
                        "fullname": "KABI",
                        "user": "dongguanting",
                        "type": "user"
                    },
                    "name": "Guanting Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:53:40.027Z",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f8",
                    "user": {
                        "_id": "66615dd1c8bbe034b4b82029",
                        "avatarUrl": "/avatars/7ef1ddf2fa356efbeb2b3aa12284a1f0.svg",
                        "isPro": false,
                        "fullname": "Zhang YiFan",
                        "user": "zhangyifan666",
                        "type": "user"
                    },
                    "name": "Qiuna Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2024-12-18T14:32:33.953Z",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81f9",
                    "name": "Zhe Wei",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81fa",
                    "user": {
                        "_id": "6513aae6330c55fdc5462ca8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EDhpiTqCBMNPmMGrOKcvY.jpeg",
                        "isPro": false,
                        "fullname": "pq-yang",
                        "user": "PeiqingYang",
                        "type": "user"
                    },
                    "name": "Peiqing Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:52:52.896Z",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81fb",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81fc",
                    "user": {
                        "_id": "64d4b5808b65d477e68f2fba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/upUtVjLXZ9lAulngeFh_i.jpeg",
                        "isPro": false,
                        "fullname": "Xue Yadong",
                        "user": "ataraxy3",
                        "type": "user"
                    },
                    "name": "Yadong Xue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2024-12-18T12:52:43.539Z",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81fd",
                    "name": "Xiaofei Wang",
                    "hidden": false
                },
                {
                    "_id": "676240aeb57a82f47d1a81fe",
                    "name": "Honggang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-17T07:06:10.000Z",
            "title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models",
            "summary": "The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/",
            "upvotes": 26,
            "discussionId": "676240b0b57a82f47d1a8254"
        },
        "translation_title": "다차원 통찰: 대규모 멀티모달 모델에서 실제 개인화 벤치마킹",
        "translation_summary": "빠르게 발전하고 있는 대규모 멀티모달 모델(LMMs) 분야는 뛰어난 능력을 가진 다양한 모델의 출현으로 이어졌습니다. 그러나 기존의 벤치마크는 LMMs가 실제 시나리오에서 인간의 다양한 요구에 부합하는지를 포괄적이고 객관적이며 정확하게 평가하지 못합니다. 이러한 격차를 해소하기 위해, 우리는 Multi-Dimensional Insights (MDI) 벤치마크를 제안합니다. 이 벤치마크는 인간 생활의 여섯 가지 일반적인 시나리오를 포괄하는 500개 이상의 이미지를 포함합니다. 특히, MDI-벤치마크는 기존 평가에 비해 두 가지 주요 이점을 제공합니다: (1) 각 이미지에는 모델의 이미지 이해도를 평가하는 간단한 질문과 기본 콘텐츠를 넘어 분석하고 추론할 수 있는 능력을 평가하는 복잡한 질문의 두 가지 유형이 함께 제공됩니다. (2) 각 연령대가 동일한 시나리오에 직면할 때 서로 다른 요구와 관점을 가지는 점을 인식하여, 우리의 벤치마크는 질문을 세 가지 연령 범주로 계층화합니다: 젊은이, 중년, 노인. 이러한 설계를 통해 LMMs가 다양한 연령대의 선호도와 요구를 충족시키는 능력을 세밀하게 평가할 수 있습니다. MDI-벤치마크를 통해, GPT-4o와 같은 강력한 모델이 연령 관련 작업에서 79%의 정확도를 달성하며, 이는 기존 LMMs가 실제 응용과제를 다루는 데 있어 여전히 상당한 개선 여지가 있음을 나타냅니다. 앞으로 MDI-벤치마크가 LMMs에서 실제 개인화를 조정하는 새로운 경로를 열어줄 것으로 기대합니다. MDI-벤치마크 데이터 및 평가 코드는 https://mdi-benchmark.github.io/에서 사용할 수 있습니다.",
        "purpose": "대규모 멀티모달 모델의 실제 응용에 대한 개인화 문제 해결을 위한 보다 정확한 벤치마크 개발",
        "advertising_copy": "실제 시나리오에서의 개인화 문제를 효과적으로 평가하고 개선하고자 하는 연구자 및 개발자에게 추천합니다!",
        "keywords": [
            "Multimodal Learning",
            "Large Language Models",
            "Image Understanding"
        ]
    }
]