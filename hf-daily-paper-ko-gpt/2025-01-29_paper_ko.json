[
    {
        "paper": {
            "id": "2501.17161",
            "authors": [
                {
                    "_id": "6799b39b15f4661561c22968",
                    "name": "Tianzhe Chu",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c22969",
                    "name": "Yuexiang Zhai",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c2296a",
                    "name": "Jihan Yang",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c2296b",
                    "name": "Shengbang Tong",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c2296c",
                    "name": "Saining Xie",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c2296d",
                    "name": "Dale Schuurmans",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c2296e",
                    "name": "Quoc V. Le",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c2296f",
                    "name": "Sergey Levine",
                    "hidden": false
                },
                {
                    "_id": "6799b39b15f4661561c22970",
                    "name": "Yi Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-28T18:59:44.000Z",
            "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model\n  Post-training",
            "summary": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used\npost-training techniques for foundation models. However, their roles in\nenhancing model generalization capabilities remain unclear. This paper studies\nthe difference between SFT and RL on generalization and memorization, focusing\non text-based rule variants and visual variants. We introduce GeneralPoints, an\narithmetic reasoning card game, and adopt V-IRL, a real-world navigation\nenvironment, to assess how models trained with SFT and RL generalize to unseen\nvariants in both textual and visual domains. We show that RL, especially when\ntrained with an outcome-based reward, generalizes across both rule-based\ntextual and visual variants. SFT, in contrast, tends to memorize training data\nand struggles to generalize out-of-distribution scenarios. Further analysis\nreveals that RL improves the model's underlying visual recognition\ncapabilities, contributing to its enhanced generalization in the visual domain.\nDespite RL's superior generalization, we show that SFT remains essential for\neffective RL training; SFT stabilizes the model's output format, enabling\nsubsequent RL to achieve its performance gains. These findings demonstrates the\ncapability of RL for acquiring generalizable knowledge in complex, multi-modal\ntasks.",
            "upvotes": 23,
            "discussionId": "6799b39d15f4661561c229e6"
        },
        "translation_title": "SFT는 기억하고, RL은 일반화한다: 기초 모델의 포스트 트레이닝 비교 연구",
        "purpose": "Supervised fine-tuning(SFT)와 Reinforcement Learning(RL)이 기초 모델의 일반화 능력을 향상시키는 역할을 명확히 이해하기 위한 연구",
        "method": [
            "SFT와 RL의 일반화 및 암기 차이를 연구하고, 텍스트 기반과 시각적 변형 모두에 초점을 맞춤(This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants.)",
            "GeneralPoints라는 산술 추론 카드 게임과 V-IRL이라는 실제 내비게이션 환경을 도입하여 SFT와 RL로 학습된 모델의 일반화 능력을 평가함(We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains.)",
            "RL이 규칙 기반 텍스트 및 시각적 변형을 가로질러 일반화됨을 보여줌(We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants.)",
            "SFT는 교육 데이터를 기억하는 경향이 있음을 강조함(SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios.)"
        ],
        "conclusion": "RL은 복잡한 다중 모달 작업에서 일반화 가능한 지식을 습득하는 능력을 보여준 반면, SFT는 RL 훈련의 효율성을 위해 여전히 필수적임.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2501.17116",
            "authors": [
                {
                    "_id": "6799b367d30dc065a2d51592",
                    "name": "Ruizhe Wang",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51593",
                    "name": "Yeyun Gong",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51594",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51595",
                    "name": "Guoshuai Zhao",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51596",
                    "name": "Ziyue Yang",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51597",
                    "name": "Baining Guo",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51598",
                    "name": "Zhengjun Zha",
                    "hidden": false
                },
                {
                    "_id": "6799b367d30dc065a2d51599",
                    "user": {
                        "_id": "653feb7ccf1f9c88f4928910",
                        "avatarUrl": "/avatars/23a6a6818116683ea9485e1470a0062f.svg",
                        "isPro": false,
                        "fullname": "Peng Cheng",
                        "user": "cp5555",
                        "type": "user"
                    },
                    "name": "Peng Cheng",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-29T04:49:44.372Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-28T18:04:50.000Z",
            "title": "Optimizing Large Language Model Training Using FP4 Quantization",
            "summary": "The growing computational demands of training large language models (LLMs)\nnecessitate more efficient methods. Quantized training presents a promising\nsolution by enabling low-bit arithmetic operations to reduce these costs. While\nFP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge\ndue to significant quantization errors and limited representational capacity.\nThis work introduces the first FP4 training framework for LLMs, addressing\nthese challenges with two key innovations: a differentiable quantization\nestimator for precise weight updates and an outlier clamping and compensation\nstrategy to prevent activation collapse. To ensure stability, the framework\nintegrates a mixed-precision training scheme and vector-wise quantization.\nExperimental results demonstrate that our FP4 framework achieves accuracy\ncomparable to BF16 and FP8, with minimal degradation, scaling effectively to\n13B-parameter LLMs trained on up to 100B tokens. With the emergence of\nnext-generation hardware supporting FP4, our framework sets a foundation for\nefficient ultra-low precision training.",
            "upvotes": 12,
            "discussionId": "6799b368d30dc065a2d515bf"
        },
        "translation_title": "FP4 양자화를 통한 대형 언어 모델 훈련 최적화",
        "purpose": "대형 언어 모델 학습 시 필요한 계산 자원을 줄이기 위한 효율적인 방법 연구",
        "method": [
            "FP4 훈련 프레임워크를 도입하여 양자화 오류를 줄이고 표현 용량을 개선함(This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse.)",
            "안정성을 보장하기 위해 혼합 정밀도 훈련 방식과 벡터 단위 양자화를 통합함(To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization.)",
            "실험 결과, FP4 프레임워크가 BF16 및 FP8과 유사한 정확도를 달성하고, 최소한의 성능 저하로 13B 매개변수를 가진 LLM을 100B 토큰으로 훈련할 수 있음을 보여줌(Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens.)"
        ],
        "conclusion": "FP4 훈련 프레임워크는 다음 세대 하드웨어에서 효율적인 초저정밀 훈련의 기초를 마련함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2501.16975",
            "authors": [
                {
                    "_id": "6799b345a66ae6b357bef986",
                    "name": "Hongzhi Huang",
                    "hidden": false
                },
                {
                    "_id": "6799b345a66ae6b357bef987",
                    "name": "Defa Zhu",
                    "hidden": false
                },
                {
                    "_id": "6799b345a66ae6b357bef988",
                    "name": "Banggu Wu",
                    "hidden": false
                },
                {
                    "_id": "6799b345a66ae6b357bef989",
                    "name": "Yutao Zeng",
                    "hidden": false
                },
                {
                    "_id": "6799b345a66ae6b357bef98a",
                    "name": "Ya Wang",
                    "hidden": false
                },
                {
                    "_id": "6799b345a66ae6b357bef98b",
                    "name": "Qiyang Min",
                    "hidden": false
                },
                {
                    "_id": "6799b345a66ae6b357bef98c",
                    "name": "Xun Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-28T14:15:42.000Z",
            "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
            "summary": "Tokenization is a fundamental component of large language models (LLMs), yet\nits influence on model scaling and performance is not fully explored. In this\npaper, we introduce Over-Tokenized Transformers, a novel framework that\ndecouples input and output vocabularies to improve language modeling\nperformance. Specifically, our approach scales up input vocabularies to\nleverage multi-gram tokens. Through extensive experiments, we uncover a\nlog-linear relationship between input vocabulary size and training loss,\ndemonstrating that larger input vocabularies consistently enhance model\nperformance, regardless of model size. Using a large input vocabulary, we\nachieve performance comparable to double-sized baselines with no additional\ncost. Our findings highlight the importance of tokenization in scaling laws and\nprovide practical insight for tokenizer design, paving the way for more\nefficient and powerful LLMs.",
            "upvotes": 8,
            "discussionId": "6799b346a66ae6b357bef9e3"
        },
        "translation_title": "Over-Tokenized Transformer: 어휘는 일반적으로 확장 가치가 있다",
        "purpose": "대형 언어 모델의 성능을 향상시키기 위해 어휘의 크기를 조절하기 위한 새로운 프레임워크 제안",
        "method": [
            "입력과 출력 어휘를 분리하여 Over-Tokenized Transformers 프레임워크를 도입함(In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance.)",
            "입력 어휘 크기를 확장하여 다중-그램 토큰을 활용함(Specifically, our approach scales up input vocabularies to leverage multi-gram tokens.)",
            "광범위한 실험을 통해 입력 어휘의 크기와 훈련 손실 간의 로그-선형 관계를 발견함(Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss.)",
            "더 큰 입력 어휘를 사용하여 추가 비용 없이 기본 모델의 두 배 성능을 달성함(Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost.)"
        ],
        "conclusion": "어휘화(tokenization)가 확장 법칙에서 중요한 역할을 하며, 더 효율적이고 강력한 LLM을 위한 설계 통찰을 제공함.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2501.16496",
            "authors": [
                {
                    "_id": "6799b2fbfe3c29ec219d7d99",
                    "name": "Lee Sharkey",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7d9a",
                    "user": {
                        "_id": "64ad563f4beffa272de6efac",
                        "avatarUrl": "/avatars/f1a4902a95830cc3936058449626f8e4.svg",
                        "isPro": false,
                        "fullname": "Bilal Chughtai",
                        "user": "bilalchughtai",
                        "type": "user"
                    },
                    "name": "Bilal Chughtai",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-29T04:47:56.702Z",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7d9b",
                    "name": "Joshua Batson",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7d9c",
                    "name": "Jack Lindsey",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7d9d",
                    "name": "Jeff Wu",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7d9e",
                    "name": "Lucius Bushnaq",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7d9f",
                    "name": "Nicholas Goldowsky-Dill",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da0",
                    "name": "Stefan Heimersheim",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da1",
                    "name": "Alejandro Ortega",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da2",
                    "name": "Joseph Bloom",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da3",
                    "name": "Stella Biderman",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da4",
                    "name": "Adria Garriga-Alonso",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da5",
                    "name": "Arthur Conmy",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da6",
                    "name": "Neel Nanda",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da7",
                    "name": "Jessica Rumbelow",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da8",
                    "name": "Martin Wattenberg",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7da9",
                    "name": "Nandi Schoots",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7daa",
                    "name": "Joseph Miller",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7dab",
                    "name": "Eric J. Michaud",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7dac",
                    "name": "Stephen Casper",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7dad",
                    "name": "Max Tegmark",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7dae",
                    "name": "William Saunders",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7daf",
                    "name": "David Bau",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7db0",
                    "name": "Eric Todd",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7db1",
                    "name": "Atticus Geiger",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7db2",
                    "name": "Mor Geva",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7db3",
                    "name": "Jesse Hoogland",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7db4",
                    "name": "Daniel Murfet",
                    "hidden": false
                },
                {
                    "_id": "6799b2fbfe3c29ec219d7db5",
                    "name": "Tom McGrath",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-27T20:57:18.000Z",
            "title": "Open Problems in Mechanistic Interpretability",
            "summary": "Mechanistic interpretability aims to understand the computational mechanisms\nunderlying neural networks' capabilities in order to accomplish concrete\nscientific and engineering goals. Progress in this field thus promises to\nprovide greater assurance over AI system behavior and shed light on exciting\nscientific questions about the nature of intelligence. Despite recent progress\ntoward these goals, there are many open problems in the field that require\nsolutions before many scientific and practical benefits can be realized: Our\nmethods require both conceptual and practical improvements to reveal deeper\ninsights; we must figure out how best to apply our methods in pursuit of\nspecific goals; and the field must grapple with socio-technical challenges that\ninfluence and are influenced by our work. This forward-facing review discusses\nthe current frontier of mechanistic interpretability and the open problems that\nthe field may benefit from prioritizing.",
            "upvotes": 6,
            "discussionId": "6799b2fcfe3c29ec219d7dca"
        },
        "translation_title": "기계적 해석 가능성의 개방 문제들",
        "purpose": "AI 시스템의 행동을 이해하고 과학적 목표를 달성하기 위한 기계적 해석 가능성 연구의 진전을 이루기 위한 개방 문제 파악",
        "method": [
            "기계적 해석 가능성 분야의 현재 진전을 논의하고 해결이 필요한 문제들을 식별함(This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.)",
            "방법론의 개념적 및 실제적 개선이 필요함(Our methods require both conceptual and practical improvements to reveal deeper insights.)",
            "특정 목표를 달성하기 위한 방법의 최적 적용 방안을 모색함(we must figure out how best to apply our methods in pursuit of specific goals.)"
        ],
        "conclusion": "기계적 해석 가능성을 향상시키기 위해서는 해결해야 할 여러 문제들이 있으며, 이를 통해 AI의 행동과 지능의 본질에 대한 통찰을 얻게 될 것이다.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2501.16764",
            "authors": [
                {
                    "_id": "6799aa5a311dbfe3c96724cd",
                    "name": "Chenguo Lin",
                    "hidden": false
                },
                {
                    "_id": "6799aa5a311dbfe3c96724ce",
                    "name": "Panwang Pan",
                    "hidden": false
                },
                {
                    "_id": "6799aa5a311dbfe3c96724cf",
                    "name": "Bangbang Yang",
                    "hidden": false
                },
                {
                    "_id": "6799aa5a311dbfe3c96724d0",
                    "name": "Zeming Li",
                    "hidden": false
                },
                {
                    "_id": "6799aa5a311dbfe3c96724d1",
                    "name": "Yadong Mu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-28T07:38:59.000Z",
            "title": "DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian\n  Splat Generation",
            "summary": "Recent advancements in 3D content generation from text or a single image\nstruggle with limited high-quality 3D datasets and inconsistency from 2D\nmulti-view generation. We introduce DiffSplat, a novel 3D generative framework\nthat natively generates 3D Gaussian splats by taming large-scale text-to-image\ndiffusion models. It differs from previous 3D generative models by effectively\nutilizing web-scale 2D priors while maintaining 3D consistency in a unified\nmodel. To bootstrap the training, a lightweight reconstruction model is\nproposed to instantly produce multi-view Gaussian splat grids for scalable\ndataset curation. In conjunction with the regular diffusion loss on these\ngrids, a 3D rendering loss is introduced to facilitate 3D coherence across\narbitrary views. The compatibility with image diffusion models enables seamless\nadaptions of numerous techniques for image generation to the 3D realm.\nExtensive experiments reveal the superiority of DiffSplat in text- and\nimage-conditioned generation tasks and downstream applications. Thorough\nablation studies validate the efficacy of each critical design choice and\nprovide insights into the underlying mechanism.",
            "upvotes": 5,
            "discussionId": "6799aa5c311dbfe3c9672542"
        },
        "translation_title": "DiffSplat: 확장 가능한 Gaussian Splat 생성을 위한 이미지 확산 모델 재사용",
        "purpose": "제한된 고품질 3D 데이터 세트를 극복하고 일관성 있는 3D 콘텐츠 생성을 위한 새로운 프레임워크 개발",
        "method": [
            "대규모 text-to-image diffusion 모델을 활용하여 3D Gaussian splat을 본래적으로 생성하는 DiffSplat 프레임워크를 소개함.(We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models.)",
            "경량 복원 모델을 제안하여 즉시 다중 뷰 Gaussian splat 그리드를 생성하고 데이터셋 큐레이션을 확장함.(a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation.)",
            "3D 일관성을 유지하기 위해 3D 렌더링 손실을 도입함.(a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views.)"
        ],
        "conclusion": "DiffSplat는 text 및 image 조건 생성 작업과 다운스트림 응용 프로그램에서 탁월한 성능을 보이며, 각 설계 선택의 효율성을 검증함.",
        "keywords": [
            "3D Vision",
            "Image Generation",
            "Multimodal Learning"
        ]
    }
]