[
    {
        "paper": {
            "id": "2501.01895",
            "authors": [
                {
                    "_id": "677b5c2478ac1cec9684059f",
                    "user": {
                        "_id": "634e4120038b5879133552f5",
                        "avatarUrl": "/avatars/34ec861b4bbf1aecf927a7d6e726c7a4.svg",
                        "isPro": true,
                        "fullname": "Siyuan",
                        "user": "SiyuanH",
                        "type": "user"
                    },
                    "name": "Siyuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-06T07:59:36.568Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a0",
                    "user": {
                        "_id": "640b00555a9c21b95c6449b3",
                        "avatarUrl": "/avatars/5fa43b956f3acc671f033e31b7ca76c5.svg",
                        "isPro": false,
                        "fullname": "Liliang Chen",
                        "user": "pathcn",
                        "type": "user"
                    },
                    "name": "Liliang Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:21:46.139Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a1",
                    "user": {
                        "_id": "65df481e530333731ea24617",
                        "avatarUrl": "/avatars/3ed41f5d7d0489193807b5e6260f16c9.svg",
                        "isPro": false,
                        "fullname": "ZHOU PENGFEI",
                        "user": "lyuukuu",
                        "type": "user"
                    },
                    "name": "Pengfei Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:22:24.033Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a2",
                    "user": {
                        "_id": "6575f9aeca03b6c514fe6e5c",
                        "avatarUrl": "/avatars/a6e9d428beaa124ee989d702b9bf4f85.svg",
                        "isPro": false,
                        "fullname": "Shengcong Chen",
                        "user": "Shengcong",
                        "type": "user"
                    },
                    "name": "Shengcong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:22:29.700Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a3",
                    "user": {
                        "_id": "67593dd0f522f4409e614ba0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67593dd0f522f4409e614ba0/cvb9w_8seu3Kbjg_XAnNj.jpeg",
                        "isPro": false,
                        "fullname": "Jiang Zhengkai",
                        "user": "jzzzzk",
                        "type": "user"
                    },
                    "name": "Zhengkai Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:22:42.112Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a4",
                    "name": "Yue Hu",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a5",
                    "name": "Peng Gao",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a6",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:23:37.965Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a7",
                    "user": {
                        "_id": "67739bfa64e8b7438ae68eb4",
                        "avatarUrl": "/avatars/15193bfbce487b2de4ce8c86bd18885a.svg",
                        "isPro": false,
                        "fullname": "Maoqing Yao",
                        "user": "AutobotZero",
                        "type": "user"
                    },
                    "name": "Maoqing Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:23:43.822Z",
                    "hidden": false
                },
                {
                    "_id": "677b5c2478ac1cec968405a8",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-06T12:37:40.659Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-03T17:00:33.000Z",
            "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
            "summary": "We introduce EnerVerse, a comprehensive framework for embodied future space\ngeneration specifically designed for robotic manipulation tasks. EnerVerse\nseamlessly integrates convolutional and bidirectional attention mechanisms for\ninner-chunk space modeling, ensuring low-level consistency and continuity.\nRecognizing the inherent redundancy in video data, we propose a sparse memory\ncontext combined with a chunkwise unidirectional generative paradigm to enable\nthe generation of infinitely long sequences. To further augment robotic\ncapabilities, we introduce the Free Anchor View (FAV) space, which provides\nflexible perspectives to enhance observation and analysis. The FAV space\nmitigates motion modeling ambiguity, removes physical constraints in confined\nenvironments, and significantly improves the robot's generalization and\nadaptability across various tasks and settings. To address the prohibitive\ncosts and labor intensity of acquiring multi-camera observations, we present a\ndata engine pipeline that integrates a generative model with 4D Gaussian\nSplatting (4DGS). This pipeline leverages the generative model's robust\ngeneralization capabilities and the spatial constraints provided by 4DGS,\nenabling an iterative enhancement of data quality and diversity, thus creating\na data flywheel effect that effectively narrows the sim-to-real gap. Finally,\nour experiments demonstrate that the embodied future space generation prior\nsubstantially enhances policy predictive capabilities, resulting in improved\noverall performance, particularly in long-range robotic manipulation tasks.",
            "upvotes": 36,
            "discussionId": "677b5c2978ac1cec96840687"
        },
        "translation_title": "EnerVerse: 로봇 조작을 위한 구현된 미래 공간 구상",
        "purpose": "로봇 조작 작업을 위해 설계된 포괄적인 미래 공간 생성 프레임워크 개발",
        "method": [
            "로봇 조작을 위한 공간 모델링에 컨볼루션 및 양방향 주의 메커니즘을 통합함(We introduce EnerVerse, a comprehensive framework for embodied future space generation specifically designed for robotic manipulation tasks.)",
            "비디오 데이터의 중복성을 인식하고, 희소 메모리 맥락과 덩어리 단위의 생성 패러다임을 결합하여 무한 긴 시퀀스를 생성할 수 있도록 함(Recognizing the inherent redundancy in video data, we propose a sparse memory context combined with a chunkwise unidirectional generative paradigm to enable the generation of infinitely long sequences.)",
            "모션 모델링의 모호성을 완화하고 로봇의 일반화 및 적응성을 개선하기 위해 Free Anchor View(FAV) 공간을 도입함(To further augment robotic capabilities, we introduce the Free Anchor View (FAV) space, which provides flexible perspectives to enhance observation and analysis.)"
        ],
        "conclusion": "EnerVerse는 정책 예측 능력을 상당히 향상시키고, 특히 장거리 로봇 조작 작업에서 전반적인 성능 개선을 이끌어냄.",
        "keywords": [
            "Robotics",
            "Image Generation",
            "3D Vision"
        ]
    },
    {
        "paper": {
            "id": "2501.01957",
            "authors": [
                {
                    "_id": "677b5979a54b76dcaa4991f9",
                    "name": "Chaoyou Fu",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa4991fa",
                    "user": {
                        "_id": "64ffd436d522560505a94b8e",
                        "avatarUrl": "/avatars/02d4faac40ac203cb5d635cfcb39780c.svg",
                        "isPro": false,
                        "fullname": "Haojia Lin",
                        "user": "linhaojia13",
                        "type": "user"
                    },
                    "name": "Haojia Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:24:54.539Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa4991fb",
                    "user": {
                        "_id": "664eaf0a98e93ef417c3cc42",
                        "avatarUrl": "/avatars/67fb44351cac8964410e5b6549817182.svg",
                        "isPro": false,
                        "fullname": "Xiong Wang",
                        "user": "xiongwang",
                        "type": "user"
                    },
                    "name": "Xiong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:25:10.743Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa4991fc",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "Yi-Fan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:25:17.351Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa4991fd",
                    "user": {
                        "_id": "6483143902f98c3f05aff915",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6483143902f98c3f05aff915/ZhWFFgrlRsQf4MXiInh5p.jpeg",
                        "isPro": false,
                        "fullname": "沈云航 Yunhang Shen",
                        "user": "shenyunhang",
                        "type": "user"
                    },
                    "name": "Yunhang Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:25:29.653Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa4991fe",
                    "name": "Xiaoyu Liu",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa4991ff",
                    "name": "Yangze Li",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499200",
                    "name": "Zuwei Long",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499201",
                    "user": {
                        "_id": "65ff5dcf82708115869da69a",
                        "avatarUrl": "/avatars/10edebbc559e9fb8b0e377c82eba66d4.svg",
                        "isPro": false,
                        "fullname": "Heting Gao",
                        "user": "hertin",
                        "type": "user"
                    },
                    "name": "Heting Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:26:18.115Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499202",
                    "name": "Ke Li",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499203",
                    "user": {
                        "_id": "665d85e35491b1e10d0d5221",
                        "avatarUrl": "/avatars/e46ea55b197e2bf8038871ae95f59585.svg",
                        "isPro": false,
                        "fullname": "Xiawu Zheng",
                        "user": "zhengxiawu",
                        "type": "user"
                    },
                    "name": "Xiawu Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:26:25.079Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499204",
                    "name": "Rongrong Ji",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499205",
                    "user": {
                        "_id": "647401e50da364bd0d002f2a",
                        "avatarUrl": "/avatars/f1586f610f21ddb4f868856208c2cfab.svg",
                        "isPro": false,
                        "fullname": "XING Sun",
                        "user": "tedsun",
                        "type": "user"
                    },
                    "name": "Xing Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:30:21.180Z",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499206",
                    "name": "Caifeng Shan",
                    "hidden": false
                },
                {
                    "_id": "677b5979a54b76dcaa499207",
                    "name": "Ran He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-03T18:59:52.000Z",
            "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
            "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
            "upvotes": 17,
            "discussionId": "677b597aa54b76dcaa499262"
        },
        "translation_title": "VITA-1.5: GPT-4o 수준의 실시간 비전 및 음성 상호작용을 향하여",
        "purpose": "비주얼 및 음성 정보를 모두 이해하여 부드러운 상호작용을 가능하게 하는 모델 개발",
        "method": [
            "다단계 훈련 방법론을 설계하여 LLM이 비주얼 및 음성 정보를 점진적으로 이해하도록 훈련함 (we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information.)",
            "강력한 비전-언어 능력을 유지하면서도 ASR 및 TTS 모듈 없이 효율적인 음성 간 대화 기능을 가능하게 함 (Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules.)",
            "이미지, 비디오 및 음성 작업에 대한 최신 벤치마크와 비교하여 모델의 강력한 성능을 입증함 (By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities.)"
        ],
        "conclusion": "우리의 모델은 강력한 비전 및 음성 기능을 갖추어 거의 실시간으로 상호작용을 가능하게 함.",
        "keywords": [
            "Multimodal Learning",
            "Vision-Language Models",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2501.01904",
            "authors": [
                {
                    "_id": "677b56b8b91035bc42259da5",
                    "user": {
                        "_id": "61d78857a21a9b49c7e8e4a9",
                        "avatarUrl": "/avatars/c7e7f84cad775be2d13fab8530bf21f5.svg",
                        "isPro": false,
                        "fullname": "Yifan Du",
                        "user": "Richard1999",
                        "type": "user"
                    },
                    "name": "Yifan Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:32:27.164Z",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259da6",
                    "user": {
                        "_id": "6448dcf1b6ac93fe6512e342",
                        "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
                        "isPro": false,
                        "fullname": "Zikang Liu",
                        "user": "JohnCage",
                        "type": "user"
                    },
                    "name": "Zikang Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:32:32.935Z",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259da7",
                    "user": {
                        "_id": "65407faf81a8731a1c134c39",
                        "avatarUrl": "/avatars/eddc718cf16d316223bcbbd2d13cf15d.svg",
                        "isPro": false,
                        "fullname": "Yifan Li",
                        "user": "yifanli",
                        "type": "user"
                    },
                    "name": "Yifan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:32:39.715Z",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259da8",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259da9",
                    "name": "Yuqi Huo",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259daa",
                    "name": "Bingning Wang",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259dab",
                    "user": {
                        "_id": "6501587887b370a56ad2608e",
                        "avatarUrl": "/avatars/6779baaa8ed9032de55a2f78e1f52e20.svg",
                        "isPro": false,
                        "fullname": "Wei-Peng Chen",
                        "user": "whenfra",
                        "type": "user"
                    },
                    "name": "Weipeng Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:33:31.102Z",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259dac",
                    "name": "Zheng Liu",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259dad",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "677b56b8b91035bc42259dae",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T08:34:39.799Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-03T17:14:16.000Z",
            "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
            "summary": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo.",
            "upvotes": 11,
            "discussionId": "677b56b9b91035bc42259df3"
        },
        "translation_title": "Virgo: o1 유사 MLLM 재현에 대한 초기 탐색",
        "purpose": "비주얼 추론을 통한 느린 사고 시스템 개발과 MLLM의 능력 향상",
        "method": [
            "MLLM을 소량의 긴 형태의 텍스트 데이터를 사용하여 파인튜닝함(we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data.)",
            "자연어로 표현된 긴 형태의 추론 과정이 MLLM에 효과적으로 전이될 수 있음을 발견함(we find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs.)",
            "텍스트 기반의 추론 데이터가 시각적 추론 데이터보다 느린 사고 능력을 이끌어내는 데 더 효과적인 것으로 보임(Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs.)"
        ],
        "conclusion": "이 연구는 느린 사고 능력이 언어 모델 구성 요소와 근본적으로 연관되어 있음을 보여주며, 이는 여러 양식이나 도메인 간에 전이될 수 있다.",
        "keywords": [
            "Large Language Models",
            "Multimodal Learning",
            "Natural Language Processing"
        ]
    },
    {
        "paper": {
            "id": "2501.01821",
            "authors": [
                {
                    "_id": "677b977e08c75e8046c38449",
                    "user": {
                        "_id": "649974df6a49e761b2944145",
                        "avatarUrl": "/avatars/5f149f87df89786807f3185a1e538362.svg",
                        "isPro": false,
                        "fullname": "Aobo Kong",
                        "user": "KAB1314",
                        "type": "user"
                    },
                    "name": "Aobo Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-06T12:37:42.697Z",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c3844a",
                    "name": "Wentao Ma",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c3844b",
                    "user": {
                        "_id": "62f4b84f2f63f904a0c5b355",
                        "avatarUrl": "/avatars/aa70c9709c736745246cadb7286c1e62.svg",
                        "isPro": false,
                        "fullname": "Shiwan Zhao",
                        "user": "shiwan",
                        "type": "user"
                    },
                    "name": "Shiwan Zhao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-01-06T12:16:57.707Z",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c3844c",
                    "name": "Yongbin Li",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c3844d",
                    "name": "Yuchuan Wu",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c3844e",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c3844f",
                    "name": "Xiaoqian Liu",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c38450",
                    "name": "Qicheng Li",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c38451",
                    "name": "Yong Qin",
                    "hidden": false
                },
                {
                    "_id": "677b977e08c75e8046c38452",
                    "name": "Fei Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-03T14:09:46.000Z",
            "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
            "summary": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex goal-oriented social\ndialogues. Direct Preference Optimization (DPO) has proven effective in\naligning LLM behavior with human preferences across a variety of agent tasks.\nExisting DPO-based approaches for multi-turn interactions are divided into\nturn-level and session-level methods. The turn-level method is overly\nfine-grained, focusing exclusively on individual turns, while session-level\nmethods are too coarse-grained, often introducing training noise. To address\nthese limitations, we propose Segment-Level Direct Preference Optimization\n(SDPO), which focuses on specific key segments within interactions to optimize\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
            "upvotes": 9,
            "discussionId": "677b977f08c75e8046c38485"
        },
        "translation_title": "SDPO: 사회 에이전트를 위한 세그먼트 수준 직접 선호 최적화",
        "purpose": "복잡한 목표 지향 사회 대화를 더 잘 처리할 수 있도록 LLM 기반 사회 에이전트의 행동을 인간의 선호에 맞게 조정하는 방법 연구",
        "method": [
            "기존 DPO 접근 방식의 한계를 해결하기 위해 상호작용의 특정 핵심 구간에 집중하는 SDPO 제안(Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise.)",
            "SDPO를 실제 평가 기준인 SOTOPIA에서 테스트하여 기존 DPO 기반 방법과 상용 LLM(GPT-4o)보다 더 나은 성능을 보임(Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o.)"
        ],
        "conclusion": "SDPO는 LLM 기반 에이전트의 사회적 지능 향상에 기여할 수 있는 가능성을 보여줌.",
        "keywords": [
            "Natural Language Processing",
            "Large Language Models",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2412.21059",
            "authors": [
                {
                    "_id": "677a6fb5deb62f25f4209c2a",
                    "user": {
                        "_id": "62d7b131f6e8ba66107af761",
                        "avatarUrl": "/avatars/f1c5df47aef69c824fd166722df8f670.svg",
                        "isPro": false,
                        "fullname": "Jiazheng Xu",
                        "user": "xujz0703",
                        "type": "user"
                    },
                    "name": "Jiazheng Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-06T08:00:26.452Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c2b",
                    "name": "Yu Huang",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c2c",
                    "user": {
                        "_id": "627626d42d26ac639e56f565",
                        "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
                        "isPro": false,
                        "fullname": "Jiale Cheng",
                        "user": "CCCCCC",
                        "type": "user"
                    },
                    "name": "Jiale Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:41:34.810Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c2d",
                    "user": {
                        "_id": "659b6c50b0f43ed69fe09d56",
                        "avatarUrl": "/avatars/8ea56c56263595a9f7555f2c2520641a.svg",
                        "isPro": false,
                        "fullname": "杨远明",
                        "user": "yangyuanming",
                        "type": "user"
                    },
                    "name": "Yuanming Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:41:48.700Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c2e",
                    "name": "Jiajun Xu",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c2f",
                    "name": "Yuan Wang",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c30",
                    "name": "Wenbo Duan",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c31",
                    "name": "Shen Yang",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c32",
                    "user": {
                        "_id": "65007e0870b6b05c5ad94a62",
                        "avatarUrl": "/avatars/c1ff713db5748db121738c601f8add85.svg",
                        "isPro": false,
                        "fullname": "Jin Qunlin",
                        "user": "kimtorch",
                        "type": "user"
                    },
                    "name": "Qunlin Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:42:43.541Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c33",
                    "name": "Shurun Li",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c34",
                    "user": {
                        "_id": "65228733377bffdc59a10117",
                        "avatarUrl": "/avatars/6eec07553658ab22f8058caa0bfbed49.svg",
                        "isPro": false,
                        "fullname": "tengjiayan",
                        "user": "tengjiayan",
                        "type": "user"
                    },
                    "name": "Jiayan Teng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:43:16.822Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c35",
                    "user": {
                        "_id": "6466d1640ed2f7a8cba87503",
                        "avatarUrl": "/avatars/652746e63dfeb5154ae7d34039d1a485.svg",
                        "isPro": false,
                        "fullname": "Zhuoyi Yang",
                        "user": "zyyangzy",
                        "type": "user"
                    },
                    "name": "Zhuoyi Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:43:40.186Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c36",
                    "user": {
                        "_id": "650aaaf1634e02df56dfd231",
                        "avatarUrl": "/avatars/4643ab23721d4ed6aeb1ebbc717adc43.svg",
                        "isPro": false,
                        "fullname": "Wendi Zheng",
                        "user": "zwd125",
                        "type": "user"
                    },
                    "name": "Wendi Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:44:00.832Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c37",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c38",
                    "name": "Ming Ding",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c39",
                    "name": "Xiaohan Zhang",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c3a",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c3b",
                    "user": {
                        "_id": "6406db5cd684369027166986",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6406db5cd684369027166986/Zl-orrGcbY0RbfjfKszn1.jpeg",
                        "isPro": false,
                        "fullname": "Shiyu Huang",
                        "user": "ShiyuHuang",
                        "type": "user"
                    },
                    "name": "Shiyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:39:28.802Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c3c",
                    "name": "Minlie Huang",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c3d",
                    "user": {
                        "_id": "640dff05474aa6f89556677e",
                        "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg",
                        "isPro": false,
                        "fullname": "Jie Tang",
                        "user": "jerytang",
                        "type": "user"
                    },
                    "name": "Jie Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:38:45.096Z",
                    "hidden": false
                },
                {
                    "_id": "677a6fb5deb62f25f4209c3e",
                    "user": {
                        "_id": "640e73bdfdeaae1390857b62",
                        "avatarUrl": "/avatars/cd6779e30f716002a7838ed93d5c0754.svg",
                        "isPro": false,
                        "fullname": "Yuxiao Dong",
                        "user": "yuxiaod",
                        "type": "user"
                    },
                    "name": "Yuxiao Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-06T12:38:37.430Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2024-12-30T16:24:09.000Z",
            "title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning\n  for Image and Video Generation",
            "summary": "We present a general strategy to aligning visual generation models -- both\nimage and video generation -- with human preference. To start with, we build\nVisionReward -- a fine-grained and multi-dimensional reward model. We decompose\nhuman preferences in images and videos into multiple dimensions, each\nrepresented by a series of judgment questions, linearly weighted and summed to\nan interpretable and accurate score. To address the challenges of video quality\nassessment, we systematically analyze various dynamic features of videos, which\nhelps VisionReward surpass VideoScore by 17.2% and achieve top performance for\nvideo preference prediction. Based on VisionReward, we develop a\nmulti-objective preference learning algorithm that effectively addresses the\nissue of confounding factors within preference data. Our approach significantly\noutperforms existing image and video scoring methods on both machine metrics\nand human evaluation. All code and datasets are provided at\nhttps://github.com/THUDM/VisionReward.",
            "upvotes": 9,
            "discussionId": "677a6fbadeb62f25f4209e46"
        },
        "translation_title": "VisionReward: 이미지 및 비디오 생성을 위한 세부적 다차원 인간 선호 학습",
        "purpose": "인간의 선호도에 맞춰 이미지와 비디오 생성 모델을 정렬하기 위한 방법론 개발",
        "method": [
            "VisionReward라는 세부적이고 다차원적인 보상 모델을 구축함(we build VisionReward -- a fine-grained and multi-dimensional reward model.)",
            "인간의 선호도를 여러 차원으로 분해하고 각각을 판단 질문으로 나타내어 해석 가능한 점수로 합산함(We decompose human preferences in images and videos into multiple dimensions, each represented by a series of judgment questions, linearly weighted and summed to an interpretable and accurate score.)",
            "비디오 품질 평가의 도전 과제를 해결하기 위해 다양한 동적 특성을 체계적으로 분석하여 VisionReward가 VideoScore보다 17.2% 향상됨을 확인함(To address the challenges of video quality assessment, we systematically analyze various dynamic features of videos, which helps VisionReward surpass VideoScore by 17.2%.)"
        ],
        "conclusion": "VisionReward를 기반으로 한 접근 방식은 기존 이미지 및 비디오 스코어링 방법들을 능가하며, 인간 평가에서도 우수한 결과를 보임.",
        "keywords": [
            "Image Generation",
            "Video Generation",
            "Multimodal Learning"
        ]
    }
]