[
    {
        "paper": {
            "id": "2501.08325",
            "authors": [
                {
                    "_id": "678719ac5333dfbf8e206077",
                    "user": {
                        "_id": "64105a6d14215c0775dfdd14",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105a6d14215c0775dfdd14/-VX-cUYOLjHIg7QnWhRGG.jpeg",
                        "isPro": false,
                        "fullname": "Jiwen Yu",
                        "user": "VictorYuki",
                        "type": "user"
                    },
                    "name": "Jiwen Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-15T08:49:01.033Z",
                    "hidden": false
                },
                {
                    "_id": "678719ac5333dfbf8e206078",
                    "name": "Yiran Qin",
                    "hidden": false
                },
                {
                    "_id": "678719ac5333dfbf8e206079",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T10:08:34.083Z",
                    "hidden": false
                },
                {
                    "_id": "678719ac5333dfbf8e20607a",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "678719ac5333dfbf8e20607b",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/bQFX1iFbXEBXcQvUNL811.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "di-zhang-fdu",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T10:08:59.963Z",
                    "hidden": false
                },
                {
                    "_id": "678719ac5333dfbf8e20607c",
                    "user": {
                        "_id": "65d5ec74cd05bc1eaa125040",
                        "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg",
                        "isPro": false,
                        "fullname": "Xihui Liu",
                        "user": "XihuiLiu",
                        "type": "user"
                    },
                    "name": "Xihui Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-21T10:07:46.546Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-14T18:57:21.000Z",
            "title": "GameFactory: Creating New Games with Generative Interactive Videos",
            "summary": "Generative game engines have the potential to revolutionize game development\nby autonomously creating new content and reducing manual workload. However,\nexisting video-based game generation methods fail to address the critical\nchallenge of scene generalization, limiting their applicability to existing\ngames with fixed styles and scenes. In this paper, we present GameFactory, a\nframework focused on exploring scene generalization in game video generation.\nTo enable the creation of entirely new and diverse games, we leverage\npre-trained video diffusion models trained on open-domain video data. To bridge\nthe domain gap between open-domain priors and small-scale game dataset, we\npropose a multi-phase training strategy that decouples game style learning from\naction control, preserving open-domain generalization while achieving action\ncontrollability. Using Minecraft as our data source, we release GF-Minecraft, a\nhigh-quality and diversity action-annotated video dataset for research.\nFurthermore, we extend our framework to enable autoregressive\naction-controllable game video generation, allowing the production of\nunlimited-length interactive game videos. Experimental results demonstrate that\nGameFactory effectively generates open-domain, diverse, and action-controllable\ngame videos, representing a significant step forward in AI-driven game\ngeneration. Our dataset and project page are publicly available at\nhttps://vvictoryuki.github.io/gamefactory/.",
            "upvotes": 44,
            "discussionId": "678719ae5333dfbf8e206106"
        },
        "translation_title": "GameFactory: 생성적 인터랙티브 비디오로 새로운 게임 만들기",
        "purpose": "게임 비디오 생성을 통해 장면 일반화 문제를 해결하고 다양한 게임을 자동으로 창출하기 위한 연구",
        "method": [
            "오픈 도메인 비디오 데이터를 기반으로 사전 훈련된 비디오 디퓨전 모델을 활용함(we leverage pre-trained video diffusion models trained on open-domain video data.)",
            "게임 스타일 학습과 행동 제어를 분리하는 다단계 훈련 전략을 제안함(to bridge the domain gap between open-domain priors and small-scale game dataset, we propose a multi-phase training strategy that decouples game style learning from action control.)",
            "Minecraft를 데이터 소스로 사용하여 GF-Minecraft라는 높은 품질과 다양성을 가진 행동 주석 비디오 데이터셋을 출시함(Using Minecraft as our data source, we release GF-Minecraft, a high-quality and diversity action-annotated video dataset for research.)",
            "자기 회귀 행동 제어가 가능한 게임 비디오 생성을 확장하여 무한 길이의 인터랙티브 게임 비디오 생산을 가능하게 함(furthermore, we extend our framework to enable autoregressive action-controllable game video generation, allowing the production of unlimited-length interactive game videos.)"
        ],
        "conclusion": "GameFactory는 화면의 다양한 행동 제어와 오픈 도메인 생성이 가능한 게임 비디오를 효과적으로 생성하여 AI 기반 게임 생성에서 중요한 발전을 이루었음을 보여줌.",
        "keywords": [
            "Video Generation",
            "Action Control",
            "Multimodal Learning"
        ]
    },
    {
        "paper": {
            "id": "2501.09781",
            "authors": [
                {
                    "_id": "678f5a57bbe3bed7b802c477",
                    "user": {
                        "_id": "64e1cabf12a5504dda7e4948",
                        "avatarUrl": "/avatars/53851eddb4e1cae773f3e3607181094b.svg",
                        "isPro": false,
                        "fullname": "rzw",
                        "user": "maverickrzw",
                        "type": "user"
                    },
                    "name": "Zhongwei Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-21T10:07:03.173Z",
                    "hidden": false
                },
                {
                    "_id": "678f5a57bbe3bed7b802c478",
                    "name": "Yunchao Wei",
                    "hidden": false
                },
                {
                    "_id": "678f5a57bbe3bed7b802c479",
                    "user": {
                        "_id": "64ae7ddf407a5cae8579c171",
                        "avatarUrl": "/avatars/f78e8958db16f5f5603ece527951ac23.svg",
                        "isPro": false,
                        "fullname": "Xun Guo",
                        "user": "xunguohf",
                        "type": "user"
                    },
                    "name": "Xun Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T14:20:14.976Z",
                    "hidden": false
                },
                {
                    "_id": "678f5a57bbe3bed7b802c47a",
                    "name": "Yao Zhao",
                    "hidden": false
                },
                {
                    "_id": "678f5a57bbe3bed7b802c47b",
                    "user": {
                        "_id": "647b5fef6a79fbf5e996c47c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
                        "isPro": false,
                        "fullname": "Bingyi Kang",
                        "user": "bykang",
                        "type": "user"
                    },
                    "name": "Bingyi Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T14:19:50.039Z",
                    "hidden": false
                },
                {
                    "_id": "678f5a57bbe3bed7b802c47c",
                    "user": {
                        "_id": "67298e44017b96a1d0101dc4",
                        "avatarUrl": "/avatars/1f8ed1a3e911e6a3021087b9371d284c.svg",
                        "isPro": false,
                        "fullname": "Jiashi Feng",
                        "user": "jshfeng",
                        "type": "user"
                    },
                    "name": "Jiashi Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T14:19:44.355Z",
                    "hidden": false
                },
                {
                    "_id": "678f5a57bbe3bed7b802c47d",
                    "name": "Xiaojie Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-16T18:59:10.000Z",
            "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
            "summary": "This work explores whether a deep generative model can learn complex\nknowledge solely from visual input, in contrast to the prevalent focus on\ntext-based models like large language models (LLMs). We develop VideoWorld, an\nauto-regressive video generation model trained on unlabeled video data, and\ntest its knowledge acquisition abilities in video-based Go and robotic control\ntasks. Our experiments reveal two key findings: (1) video-only training\nprovides sufficient information for learning knowledge, including rules,\nreasoning and planning capabilities, and (2) the representation of visual\nchange is crucial for knowledge acquisition. To improve both the efficiency and\nefficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key\ncomponent of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional\nlevel in the Video-GoBench with just a 300-million-parameter model, without\nrelying on search algorithms or reward mechanisms typical in reinforcement\nlearning. In robotic tasks, VideoWorld effectively learns diverse control\noperations and generalizes across environments, approaching the performance of\noracle models in CALVIN and RLBench. This study opens new avenues for knowledge\nacquisition from visual data, with all code, data, and models open-sourced for\nfurther research.",
            "upvotes": 5,
            "discussionId": "678f5a59bbe3bed7b802c4d6"
        },
        "translation_title": "VideoWorld: 라벨이 없는 비디오에서 지식 학습 탐구",
        "purpose": "비디오 데이터만으로 복잡한 지식을 학습할 수 있는지 연구",
        "method": [
            "비디오 생성 모델인 VideoWorld를 개발하여 라벨이 없는 비디오 데이터를 사용해 학습함(We develop VideoWorld, an auto-regressive video generation model trained on unlabeled video data.)",
            "비디오 Go와 로봇 제어 과제를 통해 모델의 지식 습득 능력을 테스트함(we test its knowledge acquisition abilities in video-based Go and robotic control tasks.)",
            "Latent Dynamics Model (LDM)을 사용하여 효율적이고 효과적인 지식 학습 프로세스를 향상시킴(we introduce the Latent Dynamics Model (LDM) as a key component of VideoWorld.)"
        ],
        "conclusion": "VideoWorld는 비디오 데이터만으로도 높은 수준의 성능을 달성하며, 새로운 연구 방향을 제시함.",
        "keywords": [
            "Video Understanding",
            "Robotics",
            "Large Language Models"
        ]
    },
    {
        "paper": {
            "id": "2501.09284",
            "authors": [
                {
                    "_id": "678dfb39f002f862857e90bf",
                    "user": {
                        "_id": "63d93667255ef6add20f9272",
                        "avatarUrl": "/avatars/99a3aeadcc81ef85164cdfb6ab186b17.svg",
                        "isPro": false,
                        "fullname": "Giyeong Oh",
                        "user": "BootsofLagrangian",
                        "type": "user"
                    },
                    "name": "Giyeong Oh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-20T09:28:57.015Z",
                    "hidden": false
                },
                {
                    "_id": "678dfb39f002f862857e90c0",
                    "user": {
                        "_id": "646d9f60eb9268aeebc55b8b",
                        "avatarUrl": "/avatars/6bf4ff17c340a622a7a847dddfe40ff2.svg",
                        "isPro": false,
                        "fullname": "SJKIM",
                        "user": "Steamout",
                        "type": "user"
                    },
                    "name": "Saejin Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-01-20T13:16:39.750Z",
                    "hidden": false
                },
                {
                    "_id": "678dfb39f002f862857e90c1",
                    "name": "Woohyun Cho",
                    "hidden": false
                },
                {
                    "_id": "678dfb39f002f862857e90c2",
                    "name": "Sangkyu Lee",
                    "hidden": false
                },
                {
                    "_id": "678dfb39f002f862857e90c3",
                    "user": {
                        "_id": "60d74d1affe0328e0167dc5f",
                        "avatarUrl": "/avatars/9b1a2df9402e9c26e1eb7c818af9bae0.svg",
                        "isPro": false,
                        "fullname": "Jiwan Chung",
                        "user": "jiwan-chung",
                        "type": "user"
                    },
                    "name": "Jiwan Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T14:22:48.531Z",
                    "hidden": false
                },
                {
                    "_id": "678dfb39f002f862857e90c4",
                    "user": {
                        "_id": "65ab767ca92a64ef5b9c8423",
                        "avatarUrl": "/avatars/00abf7a05fd86a08587f72cab5b3cff3.svg",
                        "isPro": false,
                        "fullname": "Dokyung Song",
                        "user": "dokyungs",
                        "type": "user"
                    },
                    "name": "Dokyung Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-01-21T14:22:42.282Z",
                    "hidden": false
                },
                {
                    "_id": "678dfb39f002f862857e90c5",
                    "user": {
                        "_id": "6504777fb1da3747a05160c4",
                        "avatarUrl": "/avatars/b777d98a5ff971ddb4c3e1060bb3e070.svg",
                        "isPro": false,
                        "fullname": "Youngjae Yu",
                        "user": "yjyu",
                        "type": "user"
                    },
                    "name": "Youngjae Yu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-01-20T07:28:58.615Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-01-16T04:17:56.000Z",
            "title": "SEAL: Entangled White-box Watermarks on Low-Rank Adaptation",
            "summary": "Recently, LoRA and its variants have become the de facto strategy for\ntraining and sharing task-specific versions of large pretrained models, thanks\nto their efficiency and simplicity. However, the issue of copyright protection\nfor LoRA weights, especially through watermark-based techniques, remains\nunderexplored. To address this gap, we propose SEAL (SEcure wAtermarking on\nLoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a\nsecret, non-trainable matrix between trainable LoRA weights, serving as a\npassport to claim ownership. SEAL then entangles the passport with the LoRA\nweights through training, without extra loss for entanglement, and distributes\nthe finetuned weights after hiding the passport. When applying SEAL, we\nobserved no performance degradation across commonsense reasoning,\ntextual/visual instruction tuning, and text-to-image synthesis tasks. We\ndemonstrate that SEAL is robust against a variety of known attacks: removal,\nobfuscation, and ambiguity attacks.",
            "upvotes": 1,
            "discussionId": "678dfb3af002f862857e912e"
        },
        "translation_title": "SEAL: 저랭크 적응을 위한 얽힌 화이트박스 워터마크",
        "purpose": "LoRA 가중치에 대한 저작권 보호 문제를 해결하기 위한 워터마킹 기술 개발",
        "method": [
            "LoRA 가중치 사이에 비가용성 텍스트 매트릭스를 삽입하여 소유권 주장을 위한 여권 역할을 함(To address this gap, we propose SEAL (SEcure wAtermarking on LoRA weights), the universal whitebox watermarking for LoRA.)",
            "여권과 LoRA 가중치를 훈련 과정에서 얽히도록 만들고, 추가 손실 없이 분배하도록 설계함(SEAL then entangles the passport with the LoRA weights through training, without extra loss for entanglement.).",
            "SEAL을 적용할 때 일반적인 추론, 텍스트/비주얼 지침 조정 및 text-to-image 합성 작업에서 성능 저하를 관찰하지 않음."
        ],
        "conclusion": "SEAL은 여러 가지 알려진 공격에 강력하며, LoRA 가중치의 저작권 보호를 효과적으로 수행함.",
        "keywords": [
            "Large Language Models",
            "Computer Vision",
            "Natural Language Processing"
        ]
    }
]