[
    "{'paper': {'id': '2501.11425', 'authors': [{'_id': '679080298ad1d8203a994f7f', 'user': {'_id': '62d62b333bf5e059f7d2b286', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg', 'isPro': False, 'fullname': 'Siyu Yuan', 'user': 'siyuyuan', 'type': 'user'}, 'name': 'Siyu Yuan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T11:00:12.151Z', 'hidden': False}, {'_id': '679080298ad1d8203a994f80', 'user': {'_id': '64892d31cbda0d1cdb956897', 'avatarUrl': '/avatars/3cdafe03a8295124636347d15a099aaf.svg', 'isPro': False, 'fullname': 'Zehui Chen', 'user': 'lovesnowbest', 'type': 'user'}, 'name': 'Zehui Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T11:00:20.946Z', 'hidden': False}, {'_id': '679080298ad1d8203a994f81', 'user': {'_id': '653a6e5cae155b92bae77b74', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg', 'isPro': False, 'fullname': 'Zhiheng Xi', 'user': 'WooooDyy', 'type': 'user'}, 'name': 'Zhiheng Xi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:04:23.785Z', 'hidden': False}, {'_id': '679080298ad1d8203a994f82', 'user': {'_id': '66384be673c2c55f2ded89fa', 'avatarUrl': '/avatars/1d8721074f0f51fab405f81474f2035f.svg', 'isPro': False, 'fullname': 'Junjie Ye', 'user': 'Junjie-Ye', 'type': 'user'}, 'name': 'Junjie Ye', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:02.519Z', 'hidden': False}, {'_id': '679080298ad1d8203a994f83', 'name': 'Zhengyin Du', 'hidden': False}, {'_id': '679080298ad1d8203a994f84', 'user': {'_id': '66df70fe5a0c5910d663160d', 'avatarUrl': '/avatars/980ca32bd0049ef5bbf002e7dc9f911c.svg', 'isPro': False, 'fullname': 'jiecao.chen', 'user': 'xmerge123', 'type': 'user'}, 'name': 'Jiecao Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:04:37.918Z', 'hidden': False}], 'publishedAt': '2025-01-20T11:46:04.000Z', 'title': 'Agent-R: Training Language Model Agents to Reflect via Iterative\\n  Self-Training', 'summary': \"Large Language Models (LLMs) agents are increasingly pivotal for addressing\\ncomplex tasks in interactive environments. Existing work mainly focuses on\\nenhancing performance through behavior cloning from stronger experts, yet such\\napproaches often falter in real-world applications, mainly due to the inability\\nto recover from errors. However, step-level critique data is difficult and\\nexpensive to collect. Automating and dynamically constructing self-critique\\ndatasets is thus crucial to empowering models with intelligent agent\\ncapabilities. In this work, we propose an iterative self-training framework,\\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\\nmethods that reward or penalize actions based on correctness, Agent-R leverages\\nMCTS to construct training data that recover correct trajectories from\\nerroneous ones. A key challenge of agent reflection lies in the necessity for\\ntimely revision rather than waiting until the end of a rollout. To address\\nthis, we introduce a model-guided critique construction mechanism: the actor\\nmodel identifies the first error step (within its current capability) in a\\nfailed trajectory. Starting from it, we splice it with the adjacent correct\\npath, which shares the same parent node in the tree. This strategy enables the\\nmodel to learn reflection based on its current policy, therefore yielding\\nbetter learning efficiency. To further explore the scalability of this\\nself-improvement paradigm, we investigate iterative refinement of both error\\ncorrection capabilities and dataset construction. Our findings demonstrate that\\nAgent-R continuously improves the model's ability to recover from errors and\\nenables timely error correction. Experiments on three interactive environments\\nshow that Agent-R effectively equips agents to correct erroneous actions while\\navoiding loops, achieving superior performance compared to baseline methods\\n(+5.59%).\", 'upvotes': 50, 'discussionId': '6790802b8ad1d8203a994fc7'}, 'publishedAt': '2025-01-22T00:20:57.292Z', 'title': 'Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11425.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12380', 'authors': [{'_id': '67906f432565fc5140d72dc3', 'name': 'Yilun Zhao', 'hidden': False}, {'_id': '67906f432565fc5140d72dc4', 'user': {'_id': '64ffd83b96ec8a52185dfb54', 'avatarUrl': '/avatars/a4fadc7e2f5c1125d5d455de4d5c9b8e.svg', 'isPro': False, 'fullname': 'Lujing Xie', 'user': 'leeroylucas', 'type': 'user'}, 'name': 'Lujing Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:06:45.496Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dc5', 'user': {'_id': '637169557a5e5d8efdc3e58e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg', 'isPro': False, 'fullname': 'Haowei Zhang', 'user': 'freesky', 'type': 'user'}, 'name': 'Haowei Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:59:50.465Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dc6', 'name': 'Guo Gan', 'hidden': False}, {'_id': '67906f432565fc5140d72dc7', 'name': 'Yitao Long', 'hidden': False}, {'_id': '67906f432565fc5140d72dc8', 'user': {'_id': '65e9945f6bcbcae600b7e64f', 'avatarUrl': '/avatars/8aa986a6c0e35c55d5d1461d1dc11ac3.svg', 'isPro': False, 'fullname': 'Zhiyuan Hu', 'user': 'zhiyhu', 'type': 'user'}, 'name': 'Zhiyuan Hu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:07:09.079Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dc9', 'user': {'_id': '66e83ec5deb449d8d856e78d', 'avatarUrl': '/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg', 'isPro': False, 'fullname': 'Tongyan Hu', 'user': 'entropyhu', 'type': 'user'}, 'name': 'Tongyan Hu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:59:48.493Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dca', 'user': {'_id': '6652e84bff6ccc0ef5ac7055', 'avatarUrl': '/avatars/70a8b6a0bae5f9e954fa15f56ab2ddc3.svg', 'isPro': False, 'fullname': 'Weiyuan Chen', 'user': 'RaidonShogun', 'type': 'user'}, 'name': 'Weiyuan Chen', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T14:04:09.444Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dcb', 'user': {'_id': '65415f1d5168c4f3487a2103', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65415f1d5168c4f3487a2103/qJuzDpOGSDL4E1-L2lGWW.jpeg', 'isPro': False, 'fullname': 'Chuhan Li', 'user': 'ChuhanLi', 'type': 'user'}, 'name': 'Chuhan Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:07:16.582Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dcc', 'name': 'Junyang Song', 'hidden': False}, {'_id': '67906f432565fc5140d72dcd', 'name': 'Zhijian Xu', 'hidden': False}, {'_id': '67906f432565fc5140d72dce', 'name': 'Chengye Wang', 'hidden': False}, {'_id': '67906f432565fc5140d72dcf', 'user': {'_id': '67907fdb6146e0f96241dcc3', 'avatarUrl': '/avatars/ba16cb87dce1e3ccbbe8091a3fe553fc.svg', 'isPro': False, 'fullname': 'Wf Pan', 'user': 'Phil-01', 'type': 'user'}, 'name': 'Weifeng Pan', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:18.785Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dd0', 'user': {'_id': '65dea56779f827c045b1df96', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Jtx4JcM5CNgoVyzxTC93S.jpeg', 'isPro': False, 'fullname': 'Ziyao Shangguan', 'user': 'ziyaosg', 'type': 'user'}, 'name': 'Ziyao Shangguan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:06:39.157Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dd1', 'user': {'_id': '63357c608adfa81faf2ac180', 'avatarUrl': '/avatars/ae0314c644f882251baf59b9134fd36f.svg', 'isPro': False, 'fullname': 'Xiangru Tang', 'user': 'RTT1', 'type': 'user'}, 'name': 'Xiangru Tang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:07:56.034Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dd2', 'user': {'_id': '62ffa3f8311cad266f9af236', 'avatarUrl': '/avatars/4c88cb518e000a475f8381573f21aa7f.svg', 'isPro': False, 'fullname': 'Zhenwen Liang', 'user': 'invokerliang', 'type': 'user'}, 'name': 'Zhenwen Liang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:08:01.981Z', 'hidden': False}, {'_id': '67906f432565fc5140d72dd3', 'name': 'Yixin Liu', 'hidden': False}, {'_id': '67906f432565fc5140d72dd4', 'name': 'Chen Zhao', 'hidden': False}, {'_id': '67906f432565fc5140d72dd5', 'user': {'_id': '5f5ba21188f57f65f951f255', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png', 'isPro': False, 'fullname': 'Arman Cohan', 'user': 'armanc', 'type': 'user'}, 'name': 'Arman Cohan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-22T14:09:26.876Z', 'hidden': False}], 'publishedAt': '2025-01-21T18:56:18.000Z', 'title': 'MMVU: Measuring Expert-Level Multi-Discipline Video Understanding', 'summary': 'We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark\\nfor evaluating foundation models in video understanding. MMVU includes 3,000\\nexpert-annotated questions spanning 27 subjects across four core disciplines:\\nScience, Healthcare, Humanities & Social Sciences, and Engineering. Compared to\\nprior benchmarks, MMVU features three key advancements. First, it challenges\\nmodels to apply domain-specific knowledge and perform expert-level reasoning to\\nanalyze specialized-domain videos, moving beyond the basic visual perception\\ntypically assessed in current video benchmarks. Second, each example is\\nannotated by human experts from scratch. We implement strict data quality\\ncontrols to ensure the high quality of the dataset. Finally, each example is\\nenriched with expert-annotated reasoning rationals and relevant domain\\nknowledge, facilitating in-depth analysis. We conduct an extensive evaluation\\nof 32 frontier multimodal foundation models on MMVU. The latest\\nSystem-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest\\nperformance among the tested models. However, they still fall short of matching\\nhuman expertise. Through in-depth error analyses and case studies, we offer\\nactionable insights for future advancements in expert-level,\\nknowledge-intensive video understanding for specialized domains.', 'upvotes': 43, 'discussionId': '67906f442565fc5140d72e4a'}, 'publishedAt': '2025-01-21T23:19:52.256Z', 'title': 'MMVU: Measuring Expert-Level Multi-Discipline Video Understanding', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12380.png', 'numComments': 1, 'submittedBy': {'_id': '62f662bcc58915315c4eccea', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg', 'fullname': 'Yilun', 'name': 'yilunzhao', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 7}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.11873', 'authors': [{'_id': '679071da11a3f67d8f498649', 'name': 'Zihan Qiu', 'hidden': False}, {'_id': '679071da11a3f67d8f49864a', 'name': 'Zeyu Huang', 'hidden': False}, {'_id': '679071da11a3f67d8f49864b', 'name': 'Bo Zheng', 'hidden': False}, {'_id': '679071da11a3f67d8f49864c', 'name': 'Kaiyue Wen', 'hidden': False}, {'_id': '679071da11a3f67d8f49864d', 'name': 'Zekun Wang', 'hidden': False}, {'_id': '679071da11a3f67d8f49864e', 'name': 'Rui Men', 'hidden': False}, {'_id': '679071da11a3f67d8f49864f', 'name': 'Ivan Titov', 'hidden': False}, {'_id': '679071da11a3f67d8f498650', 'user': {'_id': '6434d4989bd5a84b5dd0b0f5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg', 'isPro': False, 'fullname': 'Dayiheng Liu', 'user': 'Losin94', 'type': 'user'}, 'name': 'Dayiheng Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:15.815Z', 'hidden': False}, {'_id': '679071da11a3f67d8f498651', 'name': 'Jingren Zhou', 'hidden': False}, {'_id': '679071da11a3f67d8f498652', 'name': 'Junyang Lin', 'hidden': False}], 'publishedAt': '2025-01-21T04:04:39.000Z', 'title': 'Demons in the Detail: On Implementing Load Balancing Loss for Training\\n  Specialized Mixture-of-Expert Models', 'summary': 'This paper revisits the implementation of\\nLoad-balancing Loss (LBL) when training\\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E\\nsum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i\\nrepresents the frequency of expert i being selected, and p_i denotes the\\naverage gating score of the expert i. Existing MoE training frameworks\\nusually employ the parallel training strategy so that f_i and the LBL are\\ncalculated within a micro-batch and then averaged across parallel\\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\\nlevel, and the router is pushed to distribute the token evenly within each\\nsequence. Under this strict constraint, even tokens from a domain-specific\\nsequence (e.g., code) are uniformly routed to all experts, thereby\\ninhibiting expert specialization. In this work, we propose calculating LBL\\nusing a global-batch to loose this constraint. Because a\\nglobal-batch contains much more diverse sequences than a micro-batch, which\\nwill encourage load balance at the corpus level. Specifically, we introduce an\\nextra communication step to synchronize f_i across micro-batches and then use\\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\\n42.8B total parameters and 400B tokens), we surprisingly\\nfind that the global-batch LBL strategy yields excellent performance gains in\\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\\nthe global-batch LBL also greatly improves the domain specialization of MoE\\nexperts.', 'upvotes': 41, 'discussionId': '679071db11a3f67d8f498680'}, 'publishedAt': '2025-01-21T23:27:52.660Z', 'title': 'Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/ddUbQV_yVPwD6P0TSR5lu.png', 'https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/f7Q4QULppOygZlsYBUvY9.png', 'https://cdn-uploads.huggingface.co/production/uploads/647ccbd6e07cf9bb2d485244/9Jwx37bQkCjaWcccWbJ7b.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11873.png', 'numComments': 1, 'submittedBy': {'_id': '647ccbd6e07cf9bb2d485244', 'avatarUrl': '/avatars/e8915abaff04f6762247e196b7cf84df.svg', 'fullname': 'Zihan Qiu', 'name': 'QwQZh', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12224', 'authors': [{'_id': '6790db6588d8a90790d99ed4', 'name': 'Daniel Garibi', 'hidden': False}, {'_id': '6790db6588d8a90790d99ed5', 'name': 'Shahar Yadin', 'hidden': False}, {'_id': '6790db6588d8a90790d99ed6', 'name': 'Roni Paiss', 'hidden': False}, {'_id': '6790db6588d8a90790d99ed7', 'name': 'Omer Tov', 'hidden': False}, {'_id': '6790db6588d8a90790d99ed8', 'name': 'Shiran Zada', 'hidden': False}, {'_id': '6790db6588d8a90790d99ed9', 'name': 'Ariel Ephrat', 'hidden': False}, {'_id': '6790db6588d8a90790d99eda', 'name': 'Tomer Michaeli', 'hidden': False}, {'_id': '6790db6588d8a90790d99edb', 'name': 'Inbar Mosseri', 'hidden': False}, {'_id': '6790db6588d8a90790d99edc', 'name': 'Tali Dekel', 'hidden': False}], 'publishedAt': '2025-01-21T15:49:29.000Z', 'title': 'TokenVerse: Versatile Multi-concept Personalization in Token Modulation\\n  Space', 'summary': \"We present TokenVerse -- a method for multi-concept personalization,\\nleveraging a pre-trained text-to-image diffusion model. Our framework can\\ndisentangle complex visual elements and attributes from as little as a single\\nimage, while enabling seamless plug-and-play generation of combinations of\\nconcepts extracted from multiple images. As opposed to existing works,\\nTokenVerse can handle multiple images with multiple concepts each, and supports\\na wide-range of concepts, including objects, accessories, materials, pose, and\\nlighting. Our work exploits a DiT-based text-to-image model, in which the input\\ntext affects the generation through both attention and modulation (shift and\\nscale). We observe that the modulation space is semantic and enables localized\\ncontrol over complex concepts. Building on this insight, we devise an\\noptimization-based framework that takes as input an image and a text\\ndescription, and finds for each word a distinct direction in the modulation\\nspace. These directions can then be used to generate new images that combine\\nthe learned concepts in a desired configuration. We demonstrate the\\neffectiveness of TokenVerse in challenging personalization settings, and\\nshowcase its advantages over existing methods. project's webpage in\\nhttps://token-verse.github.io/\", 'upvotes': 24, 'discussionId': '6790db6c88d8a90790d9a0f7'}, 'publishedAt': '2025-01-22T06:50:59.785Z', 'title': 'TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/630551fd80bc5e03dad21ea6/iyhVz9LR0frnocT0Hc7sF.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12224.png', 'numComments': 1, 'submittedBy': {'_id': '630551fd80bc5e03dad21ea6', 'avatarUrl': '/avatars/849e41404df698cc89c68939de45ec9a.svg', 'fullname': 'Andrey Voynov', 'name': 'avoin', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12326', 'authors': [{'_id': '679078f902b4d94b0f2347c1', 'name': 'Yujia Qin', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c2', 'name': 'Yining Ye', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c3', 'name': 'Junjie Fang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c4', 'name': 'Haoming Wang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c5', 'name': 'Shihao Liang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c6', 'name': 'Shizuo Tian', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c7', 'name': 'Junda Zhang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c8', 'name': 'Jiahao Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347c9', 'name': 'Yunxin Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347ca', 'name': 'Shijue Huang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347cb', 'name': 'Wanjun Zhong', 'hidden': False}, {'_id': '679078f902b4d94b0f2347cc', 'name': 'Kuanye Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347cd', 'name': 'Jiale Yang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347ce', 'name': 'Yu Miao', 'hidden': False}, {'_id': '679078f902b4d94b0f2347cf', 'name': 'Woyu Lin', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d0', 'name': 'Longxiang Liu', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d1', 'name': 'Xu Jiang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d2', 'name': 'Qianli Ma', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d3', 'name': 'Jingyu Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d4', 'name': 'Xiaojun Xiao', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d5', 'name': 'Kai Cai', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d6', 'name': 'Chuang Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d7', 'name': 'Yaowei Zheng', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d8', 'name': 'Chaolin Jin', 'hidden': False}, {'_id': '679078f902b4d94b0f2347d9', 'name': 'Chen Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347da', 'name': 'Xiao Zhou', 'hidden': False}, {'_id': '679078f902b4d94b0f2347db', 'name': 'Minchao Wang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347dc', 'name': 'Haoli Chen', 'hidden': False}, {'_id': '679078f902b4d94b0f2347dd', 'name': 'Zhaojian Li', 'hidden': False}, {'_id': '679078f902b4d94b0f2347de', 'name': 'Haihua Yang', 'hidden': False}, {'_id': '679078f902b4d94b0f2347df', 'name': 'Haifeng Liu', 'hidden': False}, {'_id': '679078f902b4d94b0f2347e0', 'name': 'Feng Lin', 'hidden': False}, {'_id': '679078f902b4d94b0f2347e1', 'name': 'Tao Peng', 'hidden': False}, {'_id': '679078f902b4d94b0f2347e2', 'name': 'Xin Liu', 'hidden': False}, {'_id': '679078f902b4d94b0f2347e3', 'name': 'Guang Shi', 'hidden': False}], 'publishedAt': '2025-01-21T17:48:10.000Z', 'title': 'UI-TARS: Pioneering Automated GUI Interaction with Native Agents', 'summary': 'This paper introduces UI-TARS, a native GUI agent model that solely perceives\\nthe screenshots as input and performs human-like interactions (e.g., keyboard\\nand mouse operations). Unlike prevailing agent frameworks that depend on\\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\\nand workflows, UI-TARS is an end-to-end model that outperforms these\\nsophisticated frameworks. Experiments demonstrate its superior performance:\\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\\nperception, grounding, and GUI task execution. Notably, in the OSWorld\\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\\nGUI screenshots for context-aware understanding of UI elements and precise\\ncaptioning; (2) Unified Action Modeling, which standardizes actions into a\\nunified space across platforms and achieves precise grounding and interaction\\nthrough large-scale action traces; (3) System-2 Reasoning, which incorporates\\ndeliberate reasoning into multi-step decision making, involving multiple\\nreasoning patterns such as task decomposition, reflection thinking, milestone\\nrecognition, etc. (4) Iterative Training with Reflective Online Traces, which\\naddresses the data bottleneck by automatically collecting, filtering, and\\nreflectively refining new interaction traces on hundreds of virtual machines.\\nThrough iterative training and reflection tuning, UI-TARS continuously learns\\nfrom its mistakes and adapts to unforeseen situations with minimal human\\nintervention. We also analyze the evolution path of GUI agents to guide the\\nfurther development of this domain.', 'upvotes': 20, 'discussionId': '679078ff02b4d94b0f2348e0'}, 'publishedAt': '2025-01-21T23:51:53.248Z', 'title': 'UI-TARS: Pioneering Automated GUI Interaction with Native Agents', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12326.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12368', 'authors': [{'_id': '67907fd7d37463df976acaa7', 'name': 'Yuhang Zang', 'hidden': False}, {'_id': '67907fd7d37463df976acaa8', 'name': 'Xiaoyi Dong', 'hidden': False}, {'_id': '67907fd7d37463df976acaa9', 'name': 'Pan Zhang', 'hidden': False}, {'_id': '67907fd7d37463df976acaaa', 'name': 'Yuhang Cao', 'hidden': False}, {'_id': '67907fd7d37463df976acaab', 'name': 'Ziyu Liu', 'hidden': False}, {'_id': '67907fd7d37463df976acaac', 'name': 'Shengyuan Ding', 'hidden': False}, {'_id': '67907fd7d37463df976acaad', 'name': 'Shenxi Wu', 'hidden': False}, {'_id': '67907fd7d37463df976acaae', 'name': 'Yubo Ma', 'hidden': False}, {'_id': '67907fd7d37463df976acaaf', 'name': 'Haodong Duan', 'hidden': False}, {'_id': '67907fd7d37463df976acab0', 'name': 'Wenwei Zhang', 'hidden': False}, {'_id': '67907fd7d37463df976acab1', 'name': 'Kai Chen', 'hidden': False}, {'_id': '67907fd7d37463df976acab2', 'name': 'Dahua Lin', 'hidden': False}, {'_id': '67907fd7d37463df976acab3', 'user': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'isPro': True, 'fullname': 'Jiaqi Wang', 'user': 'myownskyW7', 'type': 'user'}, 'name': 'Jiaqi Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T14:04:07.315Z', 'hidden': False}], 'publishedAt': '2025-01-21T18:47:32.000Z', 'title': 'InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward\\n  Model', 'summary': 'Despite the promising performance of Large Vision Language Models (LVLMs) in\\nvisual understanding, they occasionally generate incorrect outputs. While\\nreward models (RMs) with reinforcement learning or test-time scaling offer the\\npotential for improving generation quality, a critical gap remains: publicly\\navailable multi-modal RMs for LVLMs are scarce, and the implementation details\\nof proprietary models are often unclear. We bridge this gap with\\nInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective\\nmulti-modal reward model that aligns LVLMs with human preferences. To ensure\\nthe robustness and versatility of IXC-2.5-Reward, we set up a high-quality\\nmulti-modal preference corpus spanning text, image, and video inputs across\\ndiverse domains, such as instruction following, general understanding,\\ntext-rich documents, mathematical reasoning, and video understanding.\\nIXC-2.5-Reward achieves excellent results on the latest multi-modal reward\\nmodel benchmark and shows competitive performance on text-only reward model\\nbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:\\n(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward\\nwith Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows\\nconsistent improvements in instruction following and multi-modal open-ended\\ndialogue; (2) Selecting the best response from candidate responses for\\ntest-time scaling; and (3) Filtering outlier or noisy samples from existing\\nimage and video instruction tuning training data. To ensure reproducibility and\\nfacilitate further research, we have open-sourced all model weights and\\ntraining recipes at https://github.com/InternLM/InternLM-XComposer', 'upvotes': 17, 'discussionId': '67907fd9d37463df976acb24'}, 'publishedAt': '2025-01-22T07:05:11.749Z', 'title': 'InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12368.png', 'numComments': 1, 'submittedBy': {'_id': '64b4eec4faa3181a5eab9c46', 'avatarUrl': '/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg', 'fullname': 'Jiaqi Wang', 'name': 'myownskyW7', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.11733', 'authors': [{'_id': '6790791b203b95acf96ebf45', 'user': {'_id': '628d7265db4cd1d1717c884f', 'avatarUrl': '/avatars/dff2a3dd10d84b4a73fa486402de7219.svg', 'isPro': False, 'fullname': 'Zhenhailong Wang', 'user': 'mikewang', 'type': 'user'}, 'name': 'Zhenhailong Wang', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-22T04:50:40.468Z', 'hidden': False}, {'_id': '6790791b203b95acf96ebf46', 'user': {'_id': '645b10e80c73ea27d13f7aca', 'avatarUrl': '/avatars/95e565306472a15067440b5b43e07a6f.svg', 'isPro': False, 'fullname': 'xuhaiyang', 'user': 'xhyandwyy', 'type': 'user'}, 'name': 'Haiyang Xu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:13.360Z', 'hidden': False}, {'_id': '6790791b203b95acf96ebf47', 'name': 'Junyang Wang', 'hidden': False}, {'_id': '6790791b203b95acf96ebf48', 'name': 'Xi Zhang', 'hidden': False}, {'_id': '6790791b203b95acf96ebf49', 'name': 'Ming Yan', 'hidden': False}, {'_id': '6790791b203b95acf96ebf4a', 'name': 'Ji Zhang', 'hidden': False}, {'_id': '6790791b203b95acf96ebf4b', 'name': 'Fei Huang', 'hidden': False}, {'_id': '6790791b203b95acf96ebf4c', 'name': 'Heng Ji', 'hidden': False}], 'publishedAt': '2025-01-20T20:35:46.000Z', 'title': 'Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks', 'summary': 'Smartphones have become indispensable in modern life, yet navigating complex\\ntasks on mobile devices often remains frustrating. Recent advancements in large\\nmultimodal model (LMM)-based mobile agents have demonstrated the ability to\\nperceive and act in mobile environments. However, current approaches face\\nsignificant limitations: they fall short in addressing real-world human needs,\\nstruggle with reasoning-intensive and long-horizon tasks, and lack mechanisms\\nto learn and improve from prior experiences. To overcome these challenges, we\\nintroduce Mobile-Agent-E, a hierarchical multi-agent framework capable of\\nself-evolution through past experience. By hierarchical, we mean an explicit\\nseparation of high-level planning and low-level action execution. The framework\\ncomprises a Manager, responsible for devising overall plans by breaking down\\ncomplex tasks into subgoals, and four subordinate agents--Perceptor, Operator,\\nAction Reflector, and Notetaker--which handle fine-grained visual perception,\\nimmediate action execution, error verification, and information aggregation,\\nrespectively. Mobile-Agent-E also features a novel self-evolution module which\\nmaintains a persistent long-term memory comprising Tips and Shortcuts. Tips are\\ngeneral guidance and lessons learned from prior tasks on how to effectively\\ninteract with the environment. Shortcuts are reusable, executable sequences of\\natomic operations tailored for specific subroutines. The inclusion of Tips and\\nShortcuts facilitates continuous refinement in performance and efficiency.\\nAlongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring\\ncomplex mobile tasks requiring long-horizon, multi-app interactions. Empirical\\nresults show that Mobile-Agent-E achieves a 22% absolute improvement over\\nprevious state-of-the-art approaches across three foundation model backbones.\\nProject page: https://x-plug.github.io/MobileAgent.', 'upvotes': 16, 'discussionId': '67907920203b95acf96ec126'}, 'publishedAt': '2025-01-22T00:17:48.799Z', 'title': 'Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11733.png', 'numComments': 1, 'submittedBy': {'_id': '645b10e80c73ea27d13f7aca', 'avatarUrl': '/avatars/95e565306472a15067440b5b43e07a6f.svg', 'fullname': 'xuhaiyang', 'name': 'xhyandwyy', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.10893', 'authors': [{'_id': '67907dd5e1d8fc832b3e7b0f', 'name': 'Hongjin Su', 'hidden': False}, {'_id': '67907dd5e1d8fc832b3e7b10', 'name': 'Ruoxi Sun', 'hidden': False}, {'_id': '67907dd5e1d8fc832b3e7b11', 'name': 'Jinsung Yoon', 'hidden': False}, {'_id': '67907dd5e1d8fc832b3e7b12', 'name': 'Pengcheng Yin', 'hidden': False}, {'_id': '67907dd5e1d8fc832b3e7b13', 'name': 'Tao Yu', 'hidden': False}, {'_id': '67907dd5e1d8fc832b3e7b14', 'name': 'Sercan Ö. Arık', 'hidden': False}], 'publishedAt': '2025-01-18T22:34:41.000Z', 'title': 'Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in\\n  Realistic Environments', 'summary': 'Autonomous agents powered by large language models (LLMs) have the potential\\nto enhance human capabilities, assisting with digital tasks from sending emails\\nto performing data analysis. The abilities of existing LLMs at such tasks are\\noften hindered by the lack of high-quality agent data from the corresponding\\nenvironments they interact with. We propose Learn-by-interact, a data-centric\\nframework to adapt LLM agents to any given environments without human\\nannotations. Learn-by-interact synthesizes trajectories of agent-environment\\ninteractions based on documentations, and constructs instructions by\\nsummarizing or abstracting the interaction histories, a process called backward\\nconstruction. We assess the quality of our synthetic data by using them in both\\ntraining-based scenarios and training-free in-context learning (ICL), where we\\ncraft innovative retrieval approaches optimized for agents. Extensive\\nexperiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across\\nrealistic coding, web, and desktop environments show the effectiveness of\\nLearn-by-interact in various downstream agentic tasks -- baseline results are\\nimproved by up to 12.2\\\\% for ICL with Claude-3.5 and 19.5\\\\% for training with\\nCodestral-22B. We further demonstrate the critical role of backward\\nconstruction, which provides up to 14.0\\\\% improvement for training. Our\\nablation studies demonstrate the efficiency provided by our synthesized data in\\nICL and the superiority of our retrieval pipeline over alternative approaches\\nlike conventional retrieval-augmented generation (RAG). We expect that\\nLearn-by-interact will serve as a foundation for agent data synthesis as LLMs\\nare increasingly deployed at real-world environments.', 'upvotes': 13, 'discussionId': '67907dd9e1d8fc832b3e7c36'}, 'publishedAt': '2025-01-22T00:11:18.322Z', 'title': 'Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10893.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12375', 'authors': [{'_id': '679069d37b150be8ddef0657', 'name': 'Sili Chen', 'hidden': False}, {'_id': '679069d37b150be8ddef0658', 'name': 'Hengkai Guo', 'hidden': False}, {'_id': '679069d37b150be8ddef0659', 'name': 'Shengnan Zhu', 'hidden': False}, {'_id': '679069d37b150be8ddef065a', 'name': 'Feihu Zhang', 'hidden': False}, {'_id': '679069d37b150be8ddef065b', 'name': 'Zilong Huang', 'hidden': False}, {'_id': '679069d37b150be8ddef065c', 'name': 'Jiashi Feng', 'hidden': False}, {'_id': '679069d37b150be8ddef065d', 'name': 'Bingyi Kang', 'hidden': False}], 'publishedAt': '2025-01-21T18:53:30.000Z', 'title': 'Video Depth Anything: Consistent Depth Estimation for Super-Long Videos', 'summary': 'Depth Anything has achieved remarkable success in monocular depth estimation\\nwith strong generalization ability. However, it suffers from temporal\\ninconsistency in videos, hindering its practical applications. Various methods\\nhave been proposed to alleviate this issue by leveraging video generation\\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\\nthese methods are only applicable to short videos (< 10 seconds) and require a\\ntrade-off between quality and computational efficiency. We propose Video Depth\\nAnything for high-quality, consistent depth estimation in super-long videos\\n(over several minutes) without sacrificing efficiency. We base our model on\\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\\nWe design a straightforward yet effective temporal consistency loss by\\nconstraining the temporal depth gradient, eliminating the need for additional\\ngeometric priors. The model is trained on a joint dataset of video depth and\\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\\nkey-frame-based strategy is developed for long video inference. Experiments\\nshow that our model can be applied to arbitrarily long videos without\\ncompromising quality, consistency, or generalization ability. Comprehensive\\nevaluations on multiple video benchmarks demonstrate that our approach sets a\\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\\ndifferent scales to support a range of scenarios, with our smallest model\\ncapable of real-time performance at 30 FPS.', 'upvotes': 12, 'discussionId': '679069d57b150be8ddef06ef'}, 'publishedAt': '2025-01-22T00:57:27.738Z', 'title': 'Video Depth Anything: Consistent Depth Estimation for Super-Long Videos', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12375.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.11223', 'authors': [{'_id': '6790772b8d7df822f1fb4405', 'name': 'Maciej Besta', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4406', 'name': 'Julia Barth', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4407', 'name': 'Eric Schreiber', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4408', 'name': 'Ales Kubicek', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4409', 'name': 'Afonso Catarino', 'hidden': False}, {'_id': '6790772b8d7df822f1fb440a', 'name': 'Robert Gerstenberger', 'hidden': False}, {'_id': '6790772b8d7df822f1fb440b', 'name': 'Piotr Nyczyk', 'hidden': False}, {'_id': '6790772b8d7df822f1fb440c', 'name': 'Patrick Iff', 'hidden': False}, {'_id': '6790772b8d7df822f1fb440d', 'name': 'Yueling Li', 'hidden': False}, {'_id': '6790772b8d7df822f1fb440e', 'name': 'Sam Houliston', 'hidden': False}, {'_id': '6790772b8d7df822f1fb440f', 'name': 'Tomasz Sternal', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4410', 'name': 'Marcin Copik', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4411', 'name': 'Grzegorz Kwaśniewski', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4412', 'name': 'Jürgen Müller', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4413', 'name': 'Łukasz Flis', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4414', 'name': 'Hannes Eberhard', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4415', 'name': 'Hubert Niewiadomski', 'hidden': False}, {'_id': '6790772b8d7df822f1fb4416', 'name': 'Torsten Hoefler', 'hidden': False}], 'publishedAt': '2025-01-20T02:16:19.000Z', 'title': 'Reasoning Language Models: A Blueprint', 'summary': 'Reasoning language models (RLMs), also known as Large Reasoning Models\\n(LRMs), such as OpenAI\\'s o1 and o3, DeepSeek-V3, and Alibaba\\'s QwQ, have\\nredefined AI\\'s problem-solving capabilities by extending large language models\\n(LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary\\nnature, and complex architectures - uniquely combining Reinforcement Learning\\n(RL), search heuristics, and LLMs - present accessibility and scalability\\nchallenges. To address these, we propose a comprehensive blueprint that\\norganizes RLM components into a modular framework, based on a survey and\\nanalysis of all RLM works. This blueprint incorporates diverse reasoning\\nstructures (chains, trees, graphs, and nested forms), reasoning strategies\\n(e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models\\nand others), and supervision schemes (Output-Based and Process-Based\\nSupervision). We also provide detailed mathematical formulations and\\nalgorithmic specifications to simplify RLM implementation. By showing how\\nschemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as\\nspecial cases, we demonstrate the blueprint\\'s versatility and unifying\\npotential. To illustrate its utility, we introduce x1, a modular implementation\\nfor rapid RLM prototyping and experimentation. Using x1 and a literature\\nreview, we provide key insights, such as multi-phase training for policy and\\nvalue models, and the importance of familiar training distributions. Finally,\\nwe outline how RLMs can integrate with a broader LLM ecosystem, including tools\\nand databases. Our work demystifies RLM construction, democratizes advanced\\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\\nexperimentation.', 'upvotes': 12, 'discussionId': '6790772d8d7df822f1fb4493'}, 'publishedAt': '2025-01-21T23:42:44.747Z', 'title': 'Reasoning Language Models: A Blueprint', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11223.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12202', 'authors': [{'_id': '67908409416b83605450716a', 'name': 'Zibo Zhao', 'hidden': False}, {'_id': '67908409416b83605450716b', 'name': 'Zeqiang Lai', 'hidden': False}, {'_id': '67908409416b83605450716c', 'name': 'Qingxiang Lin', 'hidden': False}, {'_id': '67908409416b83605450716d', 'name': 'Yunfei Zhao', 'hidden': False}, {'_id': '67908409416b83605450716e', 'name': 'Haolin Liu', 'hidden': False}, {'_id': '67908409416b83605450716f', 'name': 'Shuhui Yang', 'hidden': False}, {'_id': '67908409416b836054507170', 'name': 'Yifei Feng', 'hidden': False}, {'_id': '67908409416b836054507171', 'name': 'Mingxin Yang', 'hidden': False}, {'_id': '67908409416b836054507172', 'name': 'Sheng Zhang', 'hidden': False}, {'_id': '67908409416b836054507173', 'name': 'Xianghui Yang', 'hidden': False}, {'_id': '67908409416b836054507174', 'name': 'Huiwen Shi', 'hidden': False}, {'_id': '67908409416b836054507175', 'name': 'Sicong Liu', 'hidden': False}, {'_id': '67908409416b836054507176', 'name': 'Junta Wu', 'hidden': False}, {'_id': '67908409416b836054507177', 'name': 'Yihang Lian', 'hidden': False}, {'_id': '67908409416b836054507178', 'name': 'Fan Yang', 'hidden': False}, {'_id': '67908409416b836054507179', 'name': 'Ruining Tang', 'hidden': False}, {'_id': '67908409416b83605450717a', 'name': 'Zebin He', 'hidden': False}, {'_id': '67908409416b83605450717b', 'name': 'Xinzhou Wang', 'hidden': False}, {'_id': '67908409416b83605450717c', 'name': 'Jian Liu', 'hidden': False}, {'_id': '67908409416b83605450717d', 'name': 'Xuhui Zuo', 'hidden': False}, {'_id': '67908409416b83605450717e', 'name': 'Zhuo Chen', 'hidden': False}, {'_id': '67908409416b83605450717f', 'name': 'Biwen Lei', 'hidden': False}, {'_id': '67908409416b836054507180', 'name': 'Haohan Weng', 'hidden': False}, {'_id': '67908409416b836054507181', 'name': 'Jing Xu', 'hidden': False}, {'_id': '67908409416b836054507182', 'name': 'Yiling Zhu', 'hidden': False}, {'_id': '67908409416b836054507183', 'name': 'Xinhai Liu', 'hidden': False}, {'_id': '67908409416b836054507184', 'name': 'Lixin Xu', 'hidden': False}, {'_id': '67908409416b836054507185', 'name': 'Changrong Hu', 'hidden': False}, {'_id': '67908409416b836054507186', 'name': 'Tianyu Huang', 'hidden': False}, {'_id': '67908409416b836054507187', 'name': 'Lifu Wang', 'hidden': False}, {'_id': '67908409416b836054507188', 'name': 'Jihong Zhang', 'hidden': False}, {'_id': '67908409416b836054507189', 'name': 'Meng Chen', 'hidden': False}, {'_id': '67908409416b83605450718a', 'name': 'Liang Dong', 'hidden': False}, {'_id': '67908409416b83605450718b', 'name': 'Yiwen Jia', 'hidden': False}, {'_id': '67908409416b83605450718c', 'name': 'Yulin Cai', 'hidden': False}, {'_id': '67908409416b83605450718d', 'name': 'Jiaao Yu', 'hidden': False}, {'_id': '67908409416b83605450718e', 'name': 'Yixuan Tang', 'hidden': False}, {'_id': '67908409416b83605450718f', 'name': 'Hao Zhang', 'hidden': False}, {'_id': '67908409416b836054507190', 'name': 'Zheng Ye', 'hidden': False}, {'_id': '67908409416b836054507191', 'name': 'Peng He', 'hidden': False}, {'_id': '67908409416b836054507192', 'name': 'Runzhou Wu', 'hidden': False}, {'_id': '67908409416b836054507193', 'name': 'Chao Zhang', 'hidden': False}, {'_id': '67908409416b836054507194', 'name': 'Yonghao Tan', 'hidden': False}, {'_id': '67908409416b836054507195', 'name': 'Jie Xiao', 'hidden': False}, {'_id': '67908409416b836054507196', 'name': 'Yangyu Tao', 'hidden': False}, {'_id': '67908409416b836054507197', 'name': 'Jianchen Zhu', 'hidden': False}, {'_id': '67908409416b836054507198', 'name': 'Jinbao Xue', 'hidden': False}, {'_id': '67908409416b836054507199', 'name': 'Kai Liu', 'hidden': False}, {'_id': '67908409416b83605450719a', 'name': 'Chongqing Zhao', 'hidden': False}, {'_id': '67908409416b83605450719b', 'name': 'Xinming Wu', 'hidden': False}, {'_id': '67908409416b83605450719c', 'name': 'Zhichao Hu', 'hidden': False}, {'_id': '67908409416b83605450719d', 'name': 'Lei Qin', 'hidden': False}, {'_id': '67908409416b83605450719e', 'name': 'Jianbing Peng', 'hidden': False}, {'_id': '67908409416b83605450719f', 'name': 'Zhan Li', 'hidden': False}, {'_id': '67908409416b8360545071a0', 'name': 'Minghui Chen', 'hidden': False}, {'_id': '67908409416b8360545071a1', 'name': 'Xipeng Zhang', 'hidden': False}, {'_id': '67908409416b8360545071a2', 'name': 'Lin Niu', 'hidden': False}, {'_id': '67908409416b8360545071a3', 'name': 'Paige Wang', 'hidden': False}, {'_id': '67908409416b8360545071a4', 'name': 'Yingkai Wang', 'hidden': False}, {'_id': '67908409416b8360545071a5', 'name': 'Haozhao Kuang', 'hidden': False}, {'_id': '67908409416b8360545071a6', 'name': 'Zhongyi Fan', 'hidden': False}, {'_id': '67908409416b8360545071a7', 'name': 'Xu Zheng', 'hidden': False}, {'_id': '67908409416b8360545071a8', 'name': 'Weihao Zhuang', 'hidden': False}, {'_id': '67908409416b8360545071a9', 'name': 'YingPing He', 'hidden': False}, {'_id': '67908409416b8360545071aa', 'name': 'Tian Liu', 'hidden': False}, {'_id': '67908409416b8360545071ab', 'name': 'Yong Yang', 'hidden': False}, {'_id': '67908409416b8360545071ac', 'name': 'Di Wang', 'hidden': False}, {'_id': '67908409416b8360545071ad', 'name': 'Yuhong Liu', 'hidden': False}, {'_id': '67908409416b8360545071ae', 'name': 'Jie Jiang', 'hidden': False}, {'_id': '67908409416b8360545071af', 'name': 'Jingwei Huang', 'hidden': False}, {'_id': '67908409416b8360545071b0', 'name': 'Chunchao Guo', 'hidden': False}], 'publishedAt': '2025-01-21T15:16:54.000Z', 'title': 'Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D\\n  Assets Generation', 'summary': 'We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for\\ngenerating high-resolution textured 3D assets. This system includes two\\nfoundation components: a large-scale shape generation model -- Hunyuan3D-DiT,\\nand a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape\\ngenerative model, built on a scalable flow-based diffusion transformer, aims to\\ncreate geometry that properly aligns with a given condition image, laying a\\nsolid foundation for downstream applications. The texture synthesis model,\\nbenefiting from strong geometric and diffusion priors, produces high-resolution\\nand vibrant texture maps for either generated or hand-crafted meshes.\\nFurthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production\\nplatform that simplifies the re-creation process of 3D assets. It allows both\\nprofessional and amateur users to manipulate or even animate their meshes\\nefficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0\\noutperforms previous state-of-the-art models, including the open-source models\\nand closed-source models in geometry details, condition alignment, texture\\nquality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps\\nin the open-source 3D community for large-scale foundation generative models.\\nThe code and pre-trained weights of our models are available at:\\nhttps://github.com/Tencent/Hunyuan3D-2', 'upvotes': 10, 'discussionId': '6790840d416b8360545072a7'}, 'publishedAt': '2025-01-22T00:37:32.486Z', 'title': 'Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12202.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12273', 'authors': [{'_id': '67906c674932687e24e0cc08', 'name': 'Maosong Cao', 'hidden': False}, {'_id': '67906c674932687e24e0cc09', 'name': 'Taolin Zhang', 'hidden': False}, {'_id': '67906c674932687e24e0cc0a', 'name': 'Mo Li', 'hidden': False}, {'_id': '67906c674932687e24e0cc0b', 'name': 'Chuyu Zhang', 'hidden': False}, {'_id': '67906c674932687e24e0cc0c', 'name': 'Yunxin Liu', 'hidden': False}, {'_id': '67906c674932687e24e0cc0d', 'name': 'Haodong Duan', 'hidden': False}, {'_id': '67906c674932687e24e0cc0e', 'user': {'_id': '630716d11801ecc7d2595021', 'avatarUrl': '/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg', 'isPro': False, 'fullname': 'Songyang Zhang', 'user': 'zsytony', 'type': 'user'}, 'name': 'Songyang Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:23.652Z', 'hidden': False}, {'_id': '67906c674932687e24e0cc0f', 'name': 'Kai Chen', 'hidden': False}], 'publishedAt': '2025-01-21T16:44:12.000Z', 'title': 'Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\\n  Refinement', 'summary': 'The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\\nenhancing the conversational capabilities of Large Language Models (LLMs).\\nHowever, as LLMs become more advanced, the availability of high-quality\\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\\ngreater reliance on synthetic training data. In this work, we introduce Condor,\\na novel two-stage synthetic data generation framework that incorporates World\\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\\nat scale. Our experimental results demonstrate that a base model fine-tuned on\\nonly 20K Condor-generated samples achieves superior performance compared to\\ncounterparts. The additional refinement stage in Condor further enables\\niterative self-improvement for LLMs at various scales (up to 72B), validating\\nthe effectiveness of our approach. Furthermore, our investigation into the\\nscaling for synthetic data in post-training reveals substantial unexplored\\npotential for performance improvements, opening promising avenues for future\\nresearch.', 'upvotes': 10, 'discussionId': '67906c684932687e24e0cc61'}, 'publishedAt': '2025-01-21T22:56:36.701Z', 'title': 'Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12273.png', 'numComments': 1, 'submittedBy': {'_id': '630716d11801ecc7d2595021', 'avatarUrl': '/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg', 'fullname': 'Songyang Zhang', 'name': 'zsytony', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.08331', 'authors': [{'_id': '679086696d5aed184a333663', 'name': 'Ryan Burgert', 'hidden': False}, {'_id': '679086696d5aed184a333664', 'name': 'Yuancheng Xu', 'hidden': False}, {'_id': '679086696d5aed184a333665', 'name': 'Wenqi Xian', 'hidden': False}, {'_id': '679086696d5aed184a333666', 'name': 'Oliver Pilarski', 'hidden': False}, {'_id': '679086696d5aed184a333667', 'name': 'Pascal Clausen', 'hidden': False}, {'_id': '679086696d5aed184a333668', 'name': 'Mingming He', 'hidden': False}, {'_id': '679086696d5aed184a333669', 'name': 'Li Ma', 'hidden': False}, {'_id': '679086696d5aed184a33366a', 'name': 'Yitong Deng', 'hidden': False}, {'_id': '679086696d5aed184a33366b', 'user': {'_id': '66cb7dfcc5c61e4baa96469d', 'avatarUrl': '/avatars/6cab6e06164371778d15e4a1a3278eac.svg', 'isPro': False, 'fullname': 'Lingxiao Li', 'user': 'lingxiaol', 'type': 'user'}, 'name': 'Lingxiao Li', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-22T05:47:27.544Z', 'hidden': False}, {'_id': '679086696d5aed184a33366c', 'name': 'Mohsen Mousavi', 'hidden': False}, {'_id': '679086696d5aed184a33366d', 'name': 'Michael Ryoo', 'hidden': False}, {'_id': '679086696d5aed184a33366e', 'name': 'Paul Debevec', 'hidden': False}, {'_id': '679086696d5aed184a33366f', 'name': 'Ning Yu', 'hidden': False}], 'publishedAt': '2025-01-14T18:59:10.000Z', 'title': 'Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\\n  Real-Time Warped Noise', 'summary': 'Generative modeling aims to transform random noise into structured outputs.\\nIn this work, we enhance video diffusion models by allowing motion control via\\nstructured latent noise sampling. This is achieved by just a change in data: we\\npre-process training videos to yield structured noise. Consequently, our method\\nis agnostic to diffusion model design, requiring no changes to model\\narchitectures or training pipelines. Specifically, we propose a novel noise\\nwarping algorithm, fast enough to run in real time, that replaces random\\ntemporal Gaussianity with correlated warped noise derived from optical flow\\nfields, while preserving the spatial Gaussianity. The efficiency of our\\nalgorithm enables us to fine-tune modern video diffusion base models using\\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\\nrange of user-friendly motion control: local object motion control, global\\ncamera movement control, and motion transfer. The harmonization between\\ntemporal coherence and spatial Gaussianity in our warped noise leads to\\neffective motion control while maintaining per-frame pixel quality. Extensive\\nexperiments and user studies demonstrate the advantages of our method, making\\nit a robust and scalable approach for controlling motion in video diffusion\\nmodels. Video results are available on our webpage:\\nhttps://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code\\nand model checkpoints are available on GitHub:\\nhttps://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.', 'upvotes': 9, 'discussionId': '6790866f6d5aed184a333805'}, 'publishedAt': '2025-01-22T01:03:06.008Z', 'title': 'Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08331.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5736}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.10687', 'authors': [{'_id': '6790856e3b0a6384a4117d0e', 'name': 'Linrui Tian', 'hidden': False}, {'_id': '6790856e3b0a6384a4117d0f', 'name': 'Siqi Hu', 'hidden': False}, {'_id': '6790856e3b0a6384a4117d10', 'name': 'Qi Wang', 'hidden': False}, {'_id': '6790856e3b0a6384a4117d11', 'name': 'Bang Zhang', 'hidden': False}, {'_id': '6790856e3b0a6384a4117d12', 'name': 'Liefeng Bo', 'hidden': False}], 'publishedAt': '2025-01-18T07:51:29.000Z', 'title': 'EMO2: End-Effector Guided Audio-Driven Avatar Video Generation', 'summary': 'In this paper, we propose a novel audio-driven talking head method capable of\\nsimultaneously generating highly expressive facial expressions and hand\\ngestures. Unlike existing methods that focus on generating full-body or\\nhalf-body poses, we investigate the challenges of co-speech gesture generation\\nand identify the weak correspondence between audio features and full-body\\ngestures as a key limitation. To address this, we redefine the task as a\\ntwo-stage process. In the first stage, we generate hand poses directly from\\naudio input, leveraging the strong correlation between audio signals and hand\\nmovements. In the second stage, we employ a diffusion model to synthesize video\\nframes, incorporating the hand poses generated in the first stage to produce\\nrealistic facial expressions and body movements. Our experimental results\\ndemonstrate that the proposed method outperforms state-of-the-art approaches,\\nsuch as CyberHost and Vlogger, in terms of both visual quality and\\nsynchronization accuracy. This work provides a new perspective on audio-driven\\ngesture generation and a robust framework for creating expressive and natural\\ntalking head animations.', 'upvotes': 9, 'discussionId': '679085813b0a6384a41183f1'}, 'publishedAt': '2025-01-22T00:49:10.316Z', 'title': 'EMO2: End-Effector Guided Audio-Driven Avatar Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10687.png', 'numComments': 1, 'submittedBy': {'_id': '65df1f1ee98700500d4c289c', 'avatarUrl': '/avatars/be11bf61465df29ac997cc0fedad1cb9.svg', 'fullname': 'qi wang', 'name': 'lucaskingjade', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12390', 'authors': [{'_id': '67906d622ae55818ddfd0d93', 'user': {'_id': '645ab0b7c266796265baefa4', 'avatarUrl': '/avatars/bdac661996b63c4b2a56881707afa01f.svg', 'isPro': False, 'fullname': 'Chao Feng', 'user': 'chfeng', 'type': 'user'}, 'name': 'Chao Feng', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:21.429Z', 'hidden': False}, {'_id': '67906d622ae55818ddfd0d94', 'name': 'Ziyang Chen', 'hidden': False}, {'_id': '67906d622ae55818ddfd0d95', 'name': 'Aleksander Holynski', 'hidden': False}, {'_id': '67906d622ae55818ddfd0d96', 'name': 'Alexei A. Efros', 'hidden': False}, {'_id': '67906d622ae55818ddfd0d97', 'name': 'Andrew Owens', 'hidden': False}], 'publishedAt': '2025-01-21T18:59:46.000Z', 'title': 'GPS as a Control Signal for Image Generation', 'summary': 'We show that the GPS tags contained in photo metadata provide a useful\\ncontrol signal for image generation. We train GPS-to-image models and use them\\nfor tasks that require a fine-grained understanding of how images vary within a\\ncity. In particular, we train a diffusion model to generate images conditioned\\non both GPS and text. The learned model generates images that capture the\\ndistinctive appearance of different neighborhoods, parks, and landmarks. We\\nalso extract 3D models from 2D GPS-to-image models through score distillation\\nsampling, using GPS conditioning to constrain the appearance of the\\nreconstruction from each viewpoint. Our evaluations suggest that our\\nGPS-conditioned models successfully learn to generate images that vary based on\\nlocation, and that GPS conditioning improves estimated 3D structure.', 'upvotes': 8, 'discussionId': '67906d682ae55818ddfd0f53'}, 'publishedAt': '2025-01-21T23:41:48.239Z', 'title': 'GPS as a Control Signal for Image Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12390.png', 'numComments': 1, 'submittedBy': {'_id': '645ab0b7c266796265baefa4', 'avatarUrl': '/avatars/bdac661996b63c4b2a56881707afa01f.svg', 'fullname': 'Chao Feng', 'name': 'chfeng', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.10057', 'authors': [{'_id': '678e6e149abc7b5b42df8807', 'name': 'Paul Röttger', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8808', 'name': 'Giuseppe Attanasio', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8809', 'name': 'Felix Friedrich', 'hidden': False}, {'_id': '678e6e149abc7b5b42df880a', 'name': 'Janis Goldzycher', 'hidden': False}, {'_id': '678e6e149abc7b5b42df880b', 'name': 'Alicia Parrish', 'hidden': False}, {'_id': '678e6e149abc7b5b42df880c', 'name': 'Rishabh Bhardwaj', 'hidden': False}, {'_id': '678e6e149abc7b5b42df880d', 'name': 'Chiara Di Bonaventura', 'hidden': False}, {'_id': '678e6e149abc7b5b42df880e', 'name': 'Roman Eng', 'hidden': False}, {'_id': '678e6e149abc7b5b42df880f', 'name': 'Gaia El Khoury Geagea', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8810', 'name': 'Sujata Goswami', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8811', 'name': 'Jieun Han', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8812', 'name': 'Dirk Hovy', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8813', 'name': 'Seogyeong Jeong', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8814', 'name': 'Paloma Jeretič', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8815', 'name': 'Flor Miriam Plaza-del-Arco', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8816', 'name': 'Donya Rooein', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8817', 'name': 'Patrick Schramowski', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8818', 'name': 'Anastassia Shaitarova', 'hidden': False}, {'_id': '678e6e149abc7b5b42df8819', 'name': 'Xudong Shen', 'hidden': False}, {'_id': '678e6e149abc7b5b42df881a', 'name': 'Richard Willats', 'hidden': False}, {'_id': '678e6e149abc7b5b42df881b', 'name': 'Andrea Zugarini', 'hidden': False}, {'_id': '678e6e149abc7b5b42df881c', 'name': 'Bertie Vidgen', 'hidden': False}], 'publishedAt': '2025-01-17T09:22:35.000Z', 'title': 'MSTS: A Multimodal Safety Test Suite for Vision-Language Models', 'summary': 'Vision-language models (VLMs), which process image and text inputs, are\\nincreasingly integrated into chat assistants and other consumer AI\\napplications. Without proper safeguards, however, VLMs may give harmful advice\\n(e.g. how to self-harm) or encourage unsafe behaviours (e.g. to consume drugs).\\nDespite these clear hazards, little work so far has evaluated VLM safety and\\nthe novel risks created by multimodal inputs. To address this gap, we introduce\\nMSTS, a Multimodal Safety Test Suite for VLMs. MSTS comprises 400 test prompts\\nacross 40 fine-grained hazard categories. Each test prompt consists of a text\\nand an image that only in combination reveal their full unsafe meaning. With\\nMSTS, we find clear safety issues in several open VLMs. We also find some VLMs\\nto be safe by accident, meaning that they are safe because they fail to\\nunderstand even simple test prompts. We translate MSTS into ten languages,\\nshowing non-English prompts to increase the rate of unsafe model responses. We\\nalso show models to be safer when tested with text only rather than multimodal\\nprompts. Finally, we explore the automation of VLM safety assessments, finding\\neven the best safety classifiers to be lacking.', 'upvotes': 4, 'discussionId': '678e6e159abc7b5b42df8896'}, 'publishedAt': '2025-01-22T04:22:56.830Z', 'title': 'MSTS: A Multimodal Safety Test Suite for Vision-Language Models', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/K3gr9O2a011NTtNgwzCEt.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10057.png', 'numComments': 1, 'submittedBy': {'_id': '62e7dd4036a8e8a82700041c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg', 'fullname': 'Felix Friedrich', 'name': 'felfri', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.11900', 'authors': [{'_id': '679079a3a78e61ae7817a5f4', 'user': {'_id': '63a15c583c8841cfe2dbc5c6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671519286473-noauth.jpeg', 'isPro': False, 'fullname': 'Lian Junhong', 'user': 'THEATLAS', 'type': 'user'}, 'name': 'Junhong Lian', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-22T10:08:10.850Z', 'hidden': False}, {'_id': '679079a3a78e61ae7817a5f5', 'name': 'Xiang Ao', 'hidden': False}, {'_id': '679079a3a78e61ae7817a5f6', 'name': 'Xinyu Liu', 'hidden': False}, {'_id': '679079a3a78e61ae7817a5f7', 'name': 'Yang Liu', 'hidden': False}, {'_id': '679079a3a78e61ae7817a5f8', 'name': 'Qing He', 'hidden': False}], 'publishedAt': '2025-01-21T05:30:20.000Z', 'title': 'Panoramic Interests: Stylistic-Content Aware Personalized Headline\\n  Generation', 'summary': \"Personalized news headline generation aims to provide users with\\nattention-grabbing headlines that are tailored to their preferences. Prevailing\\nmethods focus on user-oriented content preferences, but most of them overlook\\nthe fact that diverse stylistic preferences are integral to users' panoramic\\ninterests, leading to suboptimal personalization. In view of this, we propose a\\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\\nframework. SCAPE extracts both content and stylistic features from headlines\\nwith the aid of large language model (LLM) collaboration. It further adaptively\\nintegrates users' long- and short-term interests through a contrastive\\nlearning-based hierarchical fusion network. By incorporating the panoramic\\ninterests into the headline generator, SCAPE reflects users' stylistic-content\\npreferences during the generation process. Extensive experiments on the\\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines.\", 'upvotes': 3, 'discussionId': '679079a4a78e61ae7817a648'}, 'publishedAt': '2025-01-22T08:10:31.176Z', 'title': 'Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.11900.png', 'numComments': 1, 'submittedBy': {'_id': '63a15c583c8841cfe2dbc5c6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671519286473-noauth.jpeg', 'fullname': 'Lian Junhong', 'name': 'THEATLAS', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.10573', 'authors': [{'_id': '6790ffb0561e5b824100c29a', 'user': {'_id': '6632140cf69134729c68e65f', 'avatarUrl': '/avatars/f68426b505752606223bb78fd82a1af9.svg', 'isPro': False, 'fullname': 'Karthik Viswanathan', 'user': 'vkarthik095', 'type': 'user'}, 'name': 'Karthik Viswanathan', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-22T14:24:50.240Z', 'hidden': False}, {'_id': '6790ffb0561e5b824100c29b', 'name': 'Yuri Gardinazzi', 'hidden': False}, {'_id': '6790ffb0561e5b824100c29c', 'name': 'Giada Panerai', 'hidden': False}, {'_id': '6790ffb0561e5b824100c29d', 'name': 'Alberto Cazzaniga', 'hidden': False}, {'_id': '6790ffb0561e5b824100c29e', 'name': 'Matteo Biagetti', 'hidden': False}], 'publishedAt': '2025-01-17T22:02:17.000Z', 'title': 'The Geometry of Tokens in Internal Representations of Large Language\\n  Models', 'summary': 'We investigate the relationship between the geometry of token embeddings and\\ntheir role in the next token prediction within transformer models. An important\\naspect of this connection uses the notion of empirical measure, which encodes\\nthe distribution of token point clouds across transformer layers and drives the\\nevolution of token representations in the mean-field interacting picture. We\\nuse metrics such as intrinsic dimension, neighborhood overlap, and cosine\\nsimilarity to observationally probe these empirical measures across layers. To\\nvalidate our approach, we compare these metrics to a dataset where the tokens\\nare shuffled, which disrupts the syntactic and semantic structure. Our findings\\nreveal a correlation between the geometric properties of token embeddings and\\nthe cross-entropy loss of next token predictions, implying that prompts with\\nhigher loss values have tokens represented in higher-dimensional spaces.', 'upvotes': 2, 'discussionId': '6790ffb2561e5b824100c308'}, 'publishedAt': '2025-01-22T09:28:05.021Z', 'title': 'The Geometry of Tokens in Internal Representations of Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.10573.png', 'numComments': 1, 'submittedBy': {'_id': '6632140cf69134729c68e65f', 'avatarUrl': '/avatars/f68426b505752606223bb78fd82a1af9.svg', 'fullname': 'Karthik Viswanathan', 'name': 'vkarthik095', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]