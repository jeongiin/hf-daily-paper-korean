[
    "{'paper': {'id': '2501.15368', 'authors': [{'_id': '67986c6822990ae89bb71fb9', 'user': {'_id': '6797cc0ff386b10d1609e3ff', 'avatarUrl': '/avatars/3ec1020e974ed01f60a46150501171da.svg', 'isPro': False, 'fullname': 'Yadong Li', 'user': 'AdamLee1', 'type': 'user'}, 'name': 'Yadong Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T10:29:02.659Z', 'hidden': False}, {'_id': '67986c6822990ae89bb71fba', 'name': 'Jun Liu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fbb', 'name': 'Tao Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fbc', 'name': 'Tao Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fbd', 'name': 'Song Chen', 'hidden': False}, {'_id': '67986c6822990ae89bb71fbe', 'name': 'Tianpeng Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fbf', 'name': 'Zehuan Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc0', 'name': 'Lijun Liu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc1', 'name': 'Lingfeng Ming', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc2', 'name': 'Guosheng Dong', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc3', 'name': 'Da Pan', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc4', 'name': 'Chong Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc5', 'name': 'Yuanbo Fang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc6', 'name': 'Dongdong Kuang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc7', 'user': {'_id': '65e88bfa7b458aa68925ea89', 'avatarUrl': '/avatars/6cfd5c3fceda7d3f2cb52639fc6b1597.svg', 'isPro': False, 'fullname': 'Wang Ming Rui', 'user': 'reiiichan', 'type': 'user'}, 'name': 'Mingrui Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:55:18.208Z', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc8', 'name': 'Chenglin Zhu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fc9', 'name': 'Youwei Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fca', 'name': 'Hongyu Guo', 'hidden': False}, {'_id': '67986c6822990ae89bb71fcb', 'name': 'Fengyu Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fcc', 'user': {'_id': '65e71ef39cf349af2940b317', 'avatarUrl': '/avatars/fc1cd8d3510946fc947d67b16b51834b.svg', 'isPro': False, 'fullname': 'Yuran Wang', 'user': 'Ryann829', 'type': 'user'}, 'name': 'Yuran Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-28T13:52:53.610Z', 'hidden': False}, {'_id': '67986c6822990ae89bb71fcd', 'name': 'Bowen Ding', 'hidden': False}, {'_id': '67986c6822990ae89bb71fce', 'name': 'Wei Song', 'hidden': False}, {'_id': '67986c6822990ae89bb71fcf', 'name': 'Xu Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd0', 'name': 'Yuqi Huo', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd1', 'name': 'Zheng Liang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd2', 'name': 'Shusen Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd3', 'name': 'Xin Wu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd4', 'name': 'Shuai Zhao', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd5', 'name': 'Linchu Xiong', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd6', 'name': 'Yozhen Wu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd7', 'name': 'Jiahui Ye', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd8', 'name': 'Wenhao Lu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fd9', 'name': 'Bowen Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fda', 'name': 'Yan Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fdb', 'name': 'Yaqi Zhou', 'hidden': False}, {'_id': '67986c6822990ae89bb71fdc', 'name': 'Xin Chen', 'hidden': False}, {'_id': '67986c6822990ae89bb71fdd', 'name': 'Lei Su', 'hidden': False}, {'_id': '67986c6822990ae89bb71fde', 'name': 'Hongda Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fdf', 'name': 'Fuzhong Chen', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe0', 'name': 'Xuezhen Dong', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe1', 'name': 'Na Nie', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe2', 'name': 'Zhiying Wu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe3', 'name': 'Bin Xiao', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe4', 'name': 'Ting Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe5', 'name': 'Shunya Dang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe6', 'name': 'Ping Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe7', 'name': 'Yijia Sun', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe8', 'name': 'Jincheng Wu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fe9', 'name': 'Jinjie Yang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fea', 'name': 'Xionghai Lin', 'hidden': False}, {'_id': '67986c6822990ae89bb71feb', 'name': 'Zhi Ma', 'hidden': False}, {'_id': '67986c6822990ae89bb71fec', 'name': 'Kegeng Wu', 'hidden': False}, {'_id': '67986c6822990ae89bb71fed', 'name': 'Jia li', 'hidden': False}, {'_id': '67986c6822990ae89bb71fee', 'name': 'Aiyuan Yang', 'hidden': False}, {'_id': '67986c6822990ae89bb71fef', 'name': 'Hui Liu', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff0', 'name': 'Jianqiang Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff1', 'name': 'Xiaoxi Chen', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff2', 'name': 'Guangwei Ai', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff3', 'name': 'Wentao Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff4', 'name': 'Yicong Chen', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff5', 'name': 'Xiaoqin Huang', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff6', 'name': 'Kun Li', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff7', 'name': 'Wenjing Luo', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff8', 'name': 'Yifei Duan', 'hidden': False}, {'_id': '67986c6822990ae89bb71ff9', 'name': 'Lingling Zhu', 'hidden': False}, {'_id': '67986c6822990ae89bb71ffa', 'name': 'Ran Xiao', 'hidden': False}, {'_id': '67986c6822990ae89bb71ffb', 'name': 'Zhe Su', 'hidden': False}, {'_id': '67986c6822990ae89bb71ffc', 'name': 'Jiani Pu', 'hidden': False}, {'_id': '67986c6822990ae89bb71ffd', 'name': 'Dian Wang', 'hidden': False}, {'_id': '67986c6822990ae89bb71ffe', 'name': 'Xu Jia', 'hidden': False}, {'_id': '67986c6822990ae89bb71fff', 'name': 'Tianyu Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb72000', 'name': 'Mengyu Ai', 'hidden': False}, {'_id': '67986c6822990ae89bb72001', 'name': 'Mang Wang', 'hidden': False}, {'_id': '67986c6822990ae89bb72002', 'name': 'Yujing Qiao', 'hidden': False}, {'_id': '67986c6822990ae89bb72003', 'name': 'Lei Zhang', 'hidden': False}, {'_id': '67986c6822990ae89bb72004', 'name': 'Yanjun Shen', 'hidden': False}, {'_id': '67986c6822990ae89bb72005', 'name': 'Fan Yang', 'hidden': False}, {'_id': '67986c6822990ae89bb72006', 'name': 'Miao Zhen', 'hidden': False}, {'_id': '67986c6822990ae89bb72007', 'name': 'Yijie Zhou', 'hidden': False}, {'_id': '67986c6822990ae89bb72008', 'name': 'Mingyang Chen', 'hidden': False}, {'_id': '67986c6822990ae89bb72009', 'name': 'Fei Li', 'hidden': False}, {'_id': '67986c6822990ae89bb7200a', 'name': 'Chenzheng Zhu', 'hidden': False}, {'_id': '67986c6822990ae89bb7200b', 'name': 'Keer Lu', 'hidden': False}, {'_id': '67986c6822990ae89bb7200c', 'name': 'Yaqi Zhao', 'hidden': False}, {'_id': '67986c6822990ae89bb7200d', 'name': 'Hao Liang', 'hidden': False}, {'_id': '67986c6822990ae89bb7200e', 'name': 'Youquan Li', 'hidden': False}, {'_id': '67986c6822990ae89bb7200f', 'name': 'Yanzhao Qin', 'hidden': False}, {'_id': '67986c6822990ae89bb72010', 'name': 'Linzhuang Sun', 'hidden': False}, {'_id': '67986c6822990ae89bb72011', 'name': 'Jianhua Xu', 'hidden': False}, {'_id': '67986c6822990ae89bb72012', 'name': 'Haoze Sun', 'hidden': False}, {'_id': '67986c6822990ae89bb72013', 'name': 'Mingan Lin', 'hidden': False}, {'_id': '67986c6822990ae89bb72014', 'name': 'Zenan Zhou', 'hidden': False}, {'_id': '67986c6822990ae89bb72015', 'user': {'_id': '6501587887b370a56ad2608e', 'avatarUrl': '/avatars/6779baaa8ed9032de55a2f78e1f52e20.svg', 'isPro': False, 'fullname': 'Wei-Peng Chen', 'user': 'whenfra', 'type': 'user'}, 'name': 'Weipeng Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:54:48.451Z', 'hidden': False}], 'publishedAt': '2025-01-26T02:19:03.000Z', 'title': 'Baichuan-Omni-1.5 Technical Report', 'summary': 'We introduce Baichuan-Omni-1.5, an omni-modal model that not only has\\nomni-modal understanding capabilities but also provides end-to-end audio\\ngeneration capabilities. To achieve fluent and high-quality interaction across\\nmodalities without compromising the capabilities of any modality, we\\nprioritized optimizing three key aspects. First, we establish a comprehensive\\ndata cleaning and synthesis pipeline for multimodal data, obtaining about 500B\\nhigh-quality data (text, audio, and vision). Second, an audio-tokenizer\\n(Baichuan-Audio-Tokenizer) has been designed to capture both semantic and\\nacoustic information from audio, enabling seamless integration and enhanced\\ncompatibility with MLLM. Lastly, we designed a multi-stage training strategy\\nthat progressively integrates multimodal alignment and multitask fine-tuning,\\nensuring effective synergy across all modalities. Baichuan-Omni-1.5 leads\\ncontemporary models (including GPT4o-mini and MiniCPM-o 2.6) in terms of\\ncomprehensive omni-modal capabilities. Notably, it achieves results comparable\\nto leading models such as Qwen2-VL-72B across various multimodal medical\\nbenchmarks.', 'upvotes': 28, 'discussionId': '67986c6b22990ae89bb720aa'}, 'publishedAt': '2025-01-28T00:34:49.721Z', 'title': 'Baichuan-Omni-1.5 Technical Report', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15368.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5834}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.15383', 'authors': [{'_id': '67986c83b5e71350993d28eb', 'name': 'An Yang', 'hidden': False}, {'_id': '67986c83b5e71350993d28ec', 'user': {'_id': '6583ab7983a9e1460c67d876', 'avatarUrl': '/avatars/74400bc448c3f07e23a4cd53d68a6af7.svg', 'isPro': False, 'fullname': 'bowen', 'user': 'bowenYu', 'type': 'user'}, 'name': 'Bowen Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:56:10.598Z', 'hidden': False}, {'_id': '67986c83b5e71350993d28ed', 'name': 'Chengyuan Li', 'hidden': False}, {'_id': '67986c83b5e71350993d28ee', 'user': {'_id': '6434d4989bd5a84b5dd0b0f5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6434d4989bd5a84b5dd0b0f5/0Elf9qbfG9Hkgypm9pTGm.jpeg', 'isPro': False, 'fullname': 'Dayiheng Liu', 'user': 'Losin94', 'type': 'user'}, 'name': 'Dayiheng Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:56:44.491Z', 'hidden': False}, {'_id': '67986c83b5e71350993d28ef', 'name': 'Fei Huang', 'hidden': False}, {'_id': '67986c83b5e71350993d28f0', 'name': 'Haoyan Huang', 'hidden': False}, {'_id': '67986c83b5e71350993d28f1', 'name': 'Jiandong Jiang', 'hidden': False}, {'_id': '67986c83b5e71350993d28f2', 'user': {'_id': '654bead777401b47e6424f88', 'avatarUrl': '/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg', 'isPro': False, 'fullname': 'Jianhong Tu', 'user': 'ToviTu', 'type': 'user'}, 'name': 'Jianhong Tu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:58:35.581Z', 'hidden': False}, {'_id': '67986c83b5e71350993d28f3', 'name': 'Jianwei Zhang', 'hidden': False}, {'_id': '67986c83b5e71350993d28f4', 'name': 'Jingren Zhou', 'hidden': False}, {'_id': '67986c83b5e71350993d28f5', 'user': {'_id': '620760a26e3b7210c2ff1943', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg', 'isPro': False, 'fullname': 'Junyang Lin', 'user': 'JustinLin610', 'type': 'user'}, 'name': 'Junyang Lin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:57:31.261Z', 'hidden': False}, {'_id': '67986c83b5e71350993d28f6', 'name': 'Kai Dang', 'hidden': False}, {'_id': '67986c83b5e71350993d28f7', 'user': {'_id': '65b0b3957e5d5a4ecc750de0', 'avatarUrl': '/avatars/e0d79d3265ca4ad5c5411feb01043fb4.svg', 'isPro': False, 'fullname': 'Kexin Yang', 'user': 'dawn0929', 'type': 'user'}, 'name': 'Kexin Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:57:56.435Z', 'hidden': False}, {'_id': '67986c83b5e71350993d28f8', 'name': 'Le Yu', 'hidden': False}, {'_id': '67986c83b5e71350993d28f9', 'name': 'Mei Li', 'hidden': False}, {'_id': '67986c83b5e71350993d28fa', 'user': {'_id': '636a390037d9329b4a007009', 'avatarUrl': '/avatars/a3c9117e104d4667e39e20ec83dc5cd6.svg', 'isPro': False, 'fullname': 'Minmin Sun', 'user': 'minminsun', 'type': 'user'}, 'name': 'Minmin Sun', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:58:27.795Z', 'hidden': False}, {'_id': '67986c83b5e71350993d28fb', 'name': 'Qin Zhu', 'hidden': False}, {'_id': '67986c83b5e71350993d28fc', 'name': 'Rui Men', 'hidden': False}, {'_id': '67986c83b5e71350993d28fd', 'name': 'Tao He', 'hidden': False}, {'_id': '67986c83b5e71350993d28fe', 'name': 'Weijia Xu', 'hidden': False}, {'_id': '67986c83b5e71350993d28ff', 'name': 'Wenbiao Yin', 'hidden': False}, {'_id': '67986c83b5e71350993d2900', 'user': {'_id': '63f4c99721eb234ab73dd112', 'avatarUrl': '/avatars/162e92d7aeb7de1c6ebf4d6e2bff33f5.svg', 'isPro': False, 'fullname': 'yu wenyuan', 'user': 'liuxinyijian', 'type': 'user'}, 'name': 'Wenyuan Yu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T13:58:11.021Z', 'hidden': False}, {'_id': '67986c83b5e71350993d2901', 'name': 'Xiafei Qiu', 'hidden': False}, {'_id': '67986c83b5e71350993d2902', 'name': 'Xingzhang Ren', 'hidden': False}, {'_id': '67986c83b5e71350993d2903', 'name': 'Xinlong Yang', 'hidden': False}, {'_id': '67986c83b5e71350993d2904', 'name': 'Yong Li', 'hidden': False}, {'_id': '67986c83b5e71350993d2905', 'name': 'Zhiying Xu', 'hidden': False}, {'_id': '67986c83b5e71350993d2906', 'name': 'Zipeng Zhang', 'hidden': False}], 'publishedAt': '2025-01-26T03:47:25.000Z', 'title': 'Qwen2.5-1M Technical Report', 'summary': 'We introduce Qwen2.5-1M, a series of models that extend the context length to\\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\\nhave significantly enhanced long-context capabilities through long-context\\npre-training and post-training. Key techniques such as long data synthesis,\\nprogressive pre-training, and multi-stage supervised fine-tuning are employed\\nto effectively enhance long-context performance while reducing training costs.\\n  To promote the use of long-context models among a broader user base, we\\npresent and open-source our inference framework. This framework includes a\\nlength extrapolation method that can expand the model context lengths by at\\nleast four times, or even more, without additional training. To reduce\\ninference costs, we implement a sparse attention method along with chunked\\nprefill optimization for deployment scenarios and a sparsity refinement method\\nto improve precision. Additionally, we detail our optimizations in the\\ninference engine, including kernel optimization, pipeline parallelism, and\\nscheduling optimization, which significantly enhance overall inference\\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\\nachieve a remarkable 3x to 7x prefill speedup in scenarios with 1 million\\ntokens of context. This framework provides an efficient and powerful solution\\nfor developing applications that require long-context processing using\\nopen-source models.\\n  The Qwen2.5-1M series currently includes the open-source models\\nQwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, as well as the API-accessed\\nmodel Qwen2.5-Turbo. Evaluations show that Qwen2.5-1M models have been greatly\\nimproved in long-context tasks without compromising performance in\\nshort-context scenarios. Specifically, the Qwen2.5-14B-Instruct-1M model\\nsignificantly outperforms GPT-4o-mini in long-context tasks and supports\\ncontexts eight times longer.', 'upvotes': 15, 'discussionId': '67986c84b5e71350993d2974'}, 'publishedAt': '2025-01-28T00:35:46.871Z', 'title': 'Qwen2.5-1M Technical Report', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15383.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5834}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.16142', 'authors': [{'_id': '67986cbc7dbf69e4e38539b7', 'name': 'Scott Fujimoto', 'hidden': False}, {'_id': '67986cbc7dbf69e4e38539b8', 'user': {'_id': '64b6df54dce8f1fbb8ac9ed7', 'avatarUrl': '/avatars/82ca21cb9c8bacde071769bf4a888375.svg', 'isPro': False, 'fullname': \"Pierluca D'Oro\", 'user': 'pierluca', 'type': 'user'}, 'name': \"Pierluca D'Oro\", 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:09:37.496Z', 'hidden': False}, {'_id': '67986cbc7dbf69e4e38539b9', 'name': 'Amy Zhang', 'hidden': False}, {'_id': '67986cbc7dbf69e4e38539ba', 'user': {'_id': '6344cf73ee1504dbcd5bdfe7', 'avatarUrl': '/avatars/6dd2bf1f9c5679e5c8c85d62c9836aac.svg', 'isPro': False, 'fullname': 'Yuandong Tian', 'user': 'tydsh', 'type': 'user'}, 'name': 'Yuandong Tian', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:10:10.676Z', 'hidden': False}, {'_id': '67986cbc7dbf69e4e38539bb', 'name': 'Michael Rabbat', 'hidden': False}], 'publishedAt': '2025-01-27T15:36:37.000Z', 'title': 'Towards General-Purpose Model-Free Reinforcement Learning', 'summary': 'Reinforcement learning (RL) promises a framework for near-universal\\nproblem-solving. In practice however, RL algorithms are often tailored to\\nspecific benchmarks, relying on carefully tuned hyperparameters and algorithmic\\nchoices. Recently, powerful model-based RL methods have shown impressive\\ngeneral results across benchmarks but come at the cost of increased complexity\\nand slow run times, limiting their broader applicability. In this paper, we\\nattempt to find a unifying model-free deep RL algorithm that can address a\\ndiverse class of domains and problem settings. To achieve this, we leverage\\nmodel-based representations that approximately linearize the value function,\\ntaking advantage of the denser task objectives used by model-based RL while\\navoiding the costs associated with planning or simulated trajectories. We\\nevaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a\\nsingle set of hyperparameters and show a competitive performance against\\ndomain-specific and general baselines, providing a concrete step towards\\nbuilding general-purpose model-free deep RL algorithms.', 'upvotes': 10, 'discussionId': '67986cbf7dbf69e4e3853a89'}, 'publishedAt': '2025-01-28T00:36:09.186Z', 'title': 'Towards General-Purpose Model-Free Reinforcement Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16142.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5834}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.15570', 'authors': [{'_id': '679843ae7d7b7f8196c61ab7', 'user': {'_id': '63a00aa29f1f2baab2034cf8', 'avatarUrl': '/avatars/818d104f45cbce2c47d443756fa806c8.svg', 'isPro': False, 'fullname': 'Yueyu Lin', 'user': 'yueyulin', 'type': 'user'}, 'name': 'Lin Yueyu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:10:34.376Z', 'hidden': False}, {'_id': '679843ae7d7b7f8196c61ab8', 'name': 'Li Zhiyuan', 'hidden': False}, {'_id': '679843ae7d7b7f8196c61ab9', 'user': {'_id': '64087a0992033c15073afb8c', 'avatarUrl': '/avatars/9c590ab5c6526edce5084169ec7bde2e.svg', 'isPro': False, 'fullname': 'peteryue', 'user': 'peteryue', 'type': 'user'}, 'name': 'Peter Yue', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:11:02.358Z', 'hidden': False}, {'_id': '679843ae7d7b7f8196c61aba', 'user': {'_id': '6176b32847ee6431f632981e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg', 'isPro': False, 'fullname': 'IvanD', 'user': 'xiaol', 'type': 'user'}, 'name': 'Liu Xiao', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-28T02:44:02.658Z', 'hidden': False}], 'publishedAt': '2025-01-26T15:56:56.000Z', 'title': 'ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\\n  Model Born from Transformer', 'summary': \"As is known, hybrid quadratic and subquadratic attention models in multi-head\\narchitectures have surpassed both Transformer and Linear RNN models , with\\nthese works primarily focusing on reducing KV complexity and improving\\nefficiency. For further research on expressiveness, we introduce our series of\\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\\naims to make RNN more expressive and demonstrates state tracking ability beyond\\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\\napproach that reduces the entire knowledge processing time to just 8 hours\\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\\nexplain the detailed process and share our insights on building more powerful\\nfoundation models. Please note that this is an ongoing work that will be\\nupdated continuously. The model checkpoints and source code are available at\\nhttps://github.com/yynil/RWKVInside{https://github.com/yynil/RWKVInside},\\nhttps://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.\", 'upvotes': 7, 'discussionId': '679843af7d7b7f8196c61b21'}, 'publishedAt': '2025-01-28T03:02:56.062Z', 'title': 'ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15570.png', 'numComments': 1, 'submittedBy': {'_id': '6176b32847ee6431f632981e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6176b32847ee6431f632981e/02rZ_oLAI0Ll6Y6be7Q9F.jpeg', 'fullname': 'IvanD', 'name': 'xiaol', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 81}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.15907', 'authors': [{'_id': '6798a917a8b0d165e39e17f5', 'user': {'_id': '61a7569eaf0333e76eb428a8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61a7569eaf0333e76eb428a8/zwseNheR4Hx0DtCmf_v5H.jpeg', 'isPro': False, 'fullname': 'HarryHe11', 'user': 'HarryHe', 'type': 'user'}, 'name': 'Haorui He', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-28T13:52:52.095Z', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17f6', 'user': {'_id': '64b77dd308e2452d18ddd279', 'avatarUrl': '/avatars/258f21fa20a3a187050d80c6088a1f50.svg', 'isPro': False, 'fullname': 'shangzengqiang', 'user': 'clatter-1', 'type': 'user'}, 'name': 'Zengqiang Shang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:12:12.565Z', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17f7', 'name': 'Chaoren Wang', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17f8', 'name': 'Xuyuan Li', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17f9', 'user': {'_id': '66b5f38a080d890d1727a2a4', 'avatarUrl': '/avatars/4d73017ce888437225d994d8ba370e5d.svg', 'isPro': False, 'fullname': 'guyicheng', 'user': 'guyicheng', 'type': 'user'}, 'name': 'Yicheng Gu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:12:57.932Z', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17fa', 'name': 'Hua Hua', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17fb', 'name': 'Liwei Liu', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17fc', 'name': 'Chen Yang', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17fd', 'user': {'_id': '6635a711a5243c9638f5e4df', 'avatarUrl': '/avatars/08651622fc1fd5089551b510be8c4530.svg', 'isPro': False, 'fullname': 'Jiaqi Li', 'user': 'jiaqili3', 'type': 'user'}, 'name': 'Jiaqi Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:14:30.597Z', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17fe', 'name': 'Peiyang Shi', 'hidden': False}, {'_id': '6798a917a8b0d165e39e17ff', 'user': {'_id': '63072d60cd148dbc5e49f4dd', 'avatarUrl': '/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg', 'isPro': False, 'fullname': 'Yuancheng Wang', 'user': 'Hecheng0625', 'type': 'user'}, 'name': 'Yuancheng Wang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:14:50.950Z', 'hidden': False}, {'_id': '6798a917a8b0d165e39e1800', 'name': 'Kai Chen', 'hidden': False}, {'_id': '6798a917a8b0d165e39e1801', 'user': {'_id': '65fbe8eb030389a29b87446f', 'avatarUrl': '/avatars/6d3ba153c41945e566b7c2c2d6af6da6.svg', 'isPro': False, 'fullname': 'pengyuan zhang', 'user': 'pengyuan2024', 'type': 'user'}, 'name': 'Pengyuan Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:14:57.414Z', 'hidden': False}, {'_id': '6798a917a8b0d165e39e1802', 'name': 'Zhizheng Wu', 'hidden': False}], 'publishedAt': '2025-01-27T09:59:20.000Z', 'title': 'Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\\n  Speech Generation', 'summary': 'Recent advancements in speech generation have been driven by the large-scale\\ntraining datasets. However, current models fall short of capturing the\\nspontaneity and variability inherent in real-world human speech, due to their\\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\\npipeline to extract high-quality training data from valuable yet underexplored\\nin-the-wild data that capture spontaneous human speech in real-world contexts.\\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\\nover 101k hours of speech across six languages: English, Chinese, German,\\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\\ndataset exceeding 216k hours, making it the largest open-source speech\\ngeneration dataset available. Extensive experiments demonstrate that Emilia\\nsignificantly outperforms traditional audiobook datasets in generating\\nspontaneous and human-like speech, showcasing superior performance in capturing\\ndiverse speaker timbre and speaking styles of real-world human speech.\\nFurthermore, this work underscores the importance of scaling dataset size to\\nadvance speech generation research and validates the effectiveness of Emilia\\nfor both multilingual and crosslingual speech generation.', 'upvotes': 6, 'discussionId': '6798a919a8b0d165e39e187d'}, 'publishedAt': '2025-01-28T05:40:25.750Z', 'title': 'Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15907.png', 'numComments': 1, 'submittedBy': {'_id': '61a7569eaf0333e76eb428a8', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61a7569eaf0333e76eb428a8/zwseNheR4Hx0DtCmf_v5H.jpeg', 'fullname': 'HarryHe11', 'name': 'HarryHe', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 10}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.15369', 'authors': [{'_id': '6798706dabdc35456a92212d', 'user': {'_id': '65019cc870367843160fbb33', 'avatarUrl': '/avatars/f5455482fe9efbdeeea1bd3a3c119f02.svg', 'isPro': False, 'fullname': 'ZhengChuanyang', 'user': 'BillionZheng', 'type': 'user'}, 'name': 'Chuanyang Zheng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:16:05.976Z', 'hidden': False}], 'publishedAt': '2025-01-26T02:34:58.000Z', 'title': 'iFormer: Integrating ConvNet and Transformer for Mobile Application', 'summary': 'We present a new family of mobile hybrid vision networks, called iFormer,\\nwith a focus on optimizing latency and accuracy on mobile applications. iFormer\\neffectively integrates the fast local representation capacity of convolution\\nwith the efficient global modeling ability of self-attention. The local\\ninteractions are derived from transforming a standard convolutional network,\\ni.e., ConvNeXt, to design a more lightweight mobile network. Our newly\\nintroduced mobile modulation attention removes memory-intensive operations in\\nMHA and employs an efficient modulation mechanism to boost dynamic global\\nrepresentational capacity. We conduct comprehensive experiments demonstrating\\nthat iFormer outperforms existing lightweight networks across various tasks.\\nNotably, iFormer achieves an impressive Top-1 accuracy of 80.4\\\\% on ImageNet-1k\\nwith a latency of only 1.10 ms on an iPhone 13, surpassing the recently\\nproposed MobileNetV4 under similar latency constraints. Additionally, our\\nmethod shows significant improvements in downstream tasks, including COCO\\nobject detection, instance segmentation, and ADE20k semantic segmentation,\\nwhile still maintaining low latency on mobile devices for high-resolution\\ninputs in these scenarios.', 'upvotes': 4, 'discussionId': '6798706eabdc35456a92215a'}, 'publishedAt': '2025-01-28T00:51:51.263Z', 'title': 'iFormer: Integrating ConvNet and Transformer for Mobile Application', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.15369.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5834}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.12370', 'authors': [{'_id': '6798d09d208ffebef5bcfa47', 'name': 'Samira Abnar', 'hidden': False}, {'_id': '6798d09d208ffebef5bcfa48', 'user': {'_id': '64b1a4f64dd3e24895daa236', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b1a4f64dd3e24895daa236/lzlZ4DBOw-YVspNttAEY3.jpeg', 'isPro': False, 'fullname': 'Harshay Shah', 'user': 'harshay', 'type': 'user'}, 'name': 'Harshay Shah', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:16:56.095Z', 'hidden': False}, {'_id': '6798d09d208ffebef5bcfa49', 'user': {'_id': '64c3726f2a5eaefd000cdedd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64c3726f2a5eaefd000cdedd/iwFifH1sWQy7agW3eTmNQ.png', 'isPro': False, 'fullname': 'Dan Busbridge', 'user': 'dbusbridge', 'type': 'user'}, 'name': 'Dan Busbridge', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:17:03.153Z', 'hidden': False}, {'_id': '6798d09d208ffebef5bcfa4a', 'name': 'Alaaeldin Mohamed Elnouby Ali', 'hidden': False}, {'_id': '6798d09d208ffebef5bcfa4b', 'name': 'Josh Susskind', 'hidden': False}, {'_id': '6798d09d208ffebef5bcfa4c', 'user': {'_id': '6737e918edaf7e05e4b35791', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uk--P2mE2Jeg1QoxmXSTS.png', 'isPro': False, 'fullname': 'Vimal Thilak', 'user': 'vimalthilak', 'type': 'user'}, 'name': 'Vimal Thilak', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:17:43.503Z', 'hidden': False}], 'publishedAt': '2025-01-21T18:51:15.000Z', 'title': 'Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for\\n  Mixture-of-Experts Language Models', 'summary': \"Scaling the capacity of language models has consistently proven to be a\\nreliable approach for improving performance and unlocking new capabilities.\\nCapacity can be primarily defined by two dimensions: the number of model\\nparameters and the compute per example. While scaling typically involves\\nincreasing both, the precise interplay between these factors and their combined\\ncontribution to overall capacity remains not fully understood. We explore this\\nrelationship in the context of sparse Mixture-of-Experts (MoEs), which allow\\nscaling the number of parameters without proportionally increasing the FLOPs\\nper example. We investigate how varying the sparsity level, i.e., the fraction\\nof inactive parameters, impacts model's performance during pretraining and\\ndownstream few-shot evaluation. We find that under different constraints (e.g.,\\nparameter size and total training compute), there is an optimal level of\\nsparsity that improves both training efficiency and model performance. These\\nresults provide a better understanding of the impact of sparsity in scaling\\nlaws for MoEs and complement existing works in this area, offering insights for\\ndesigning more efficient architectures.\", 'upvotes': 2, 'discussionId': '6798d09e208ffebef5bcfa9c'}, 'publishedAt': '2025-01-28T07:42:14.777Z', 'title': 'Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.12370.png', 'numComments': 1, 'submittedBy': {'_id': '651e96991b97c9f33d26bde6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg', 'fullname': 'Elie Bakouch', 'name': 'eliebak', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 76}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.16295', 'authors': [{'_id': '67986cd6bdc99911a989b0a5', 'name': 'Weixin Liang', 'hidden': False}, {'_id': '67986cd6bdc99911a989b0a6', 'user': {'_id': '6532e347b66f4bf689cf269a', 'avatarUrl': '/avatars/76b5dddf80a24d3ef5c68b702280da82.svg', 'isPro': False, 'fullname': 'Junhong Shen', 'user': 'sjunhongs', 'type': 'user'}, 'name': 'Junhong Shen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:18:15.019Z', 'hidden': False}, {'_id': '67986cd6bdc99911a989b0a7', 'user': {'_id': '65a76ff1e504d9738d636217', 'avatarUrl': '/avatars/26bf5e3f19057835ee95d72c24904d77.svg', 'isPro': False, 'fullname': 'Genghan Zhang', 'user': 'Genghan', 'type': 'user'}, 'name': 'Genghan Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-28T14:18:21.072Z', 'hidden': False}, {'_id': '67986cd6bdc99911a989b0a8', 'name': 'Ning Dong', 'hidden': False}, {'_id': '67986cd6bdc99911a989b0a9', 'name': 'Luke Zettlemoyer', 'hidden': False}, {'_id': '67986cd6bdc99911a989b0aa', 'name': 'Lili Yu', 'hidden': False}], 'publishedAt': '2025-01-27T18:35:05.000Z', 'title': 'Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with\\n  Modality-Aware Sparsity', 'summary': 'State Space Models (SSMs) have emerged as efficient alternatives to\\nTransformers for sequential modeling, but their inability to leverage\\nmodality-specific features limits their performance in multi-modal pretraining.\\nHere, we propose Mixture-of-Mamba, a novel SSM architecture that introduces\\nmodality-aware sparsity through modality-specific parameterization of the Mamba\\nblock. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996;\\n2024), we extend the benefits of modality-aware sparsity to SSMs while\\npreserving their computational efficiency. We evaluate Mixture-of-Mamba across\\nthree multi-modal pretraining settings: Transfusion (interleaved text and\\ncontinuous image tokens with diffusion loss), Chameleon (interleaved text and\\ndiscrete image tokens), and an extended three-modality framework incorporating\\nspeech. Mixture-of-Mamba consistently reaches the same loss values at earlier\\ntraining steps with significantly reduced computational costs. In the\\nTransfusion setting, Mixture-of-Mamba achieves equivalent image loss using only\\n34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting,\\nMixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at\\nthe 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the\\nthree-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the\\n1.4B scale. Our ablation study highlights the synergistic effects of decoupling\\nprojection components, where joint decoupling yields greater gains than\\nindividual modifications. These results establish modality-aware sparsity as a\\nversatile and effective design principle, extending its impact from\\nTransformers to SSMs and setting new benchmarks in multi-modal pretraining. Our\\ncode can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba', 'upvotes': 2, 'discussionId': '67986cd7bdc99911a989b0ea'}, 'publishedAt': '2025-01-28T00:36:31.841Z', 'title': 'Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.16295.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5834}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.14912', 'authors': [{'_id': '67986d764fccd4b95149db0b', 'name': 'Juan Ramirez', 'hidden': False}, {'_id': '67986d764fccd4b95149db0c', 'name': 'Ignacio Hounie', 'hidden': False}, {'_id': '67986d764fccd4b95149db0d', 'name': 'Juan Elenter', 'hidden': False}, {'_id': '67986d764fccd4b95149db0e', 'name': 'Jose Gallego-Posada', 'hidden': False}, {'_id': '67986d764fccd4b95149db0f', 'name': 'Meraj Hashemizadeh', 'hidden': False}, {'_id': '67986d764fccd4b95149db10', 'name': 'Alejandro Ribeiro', 'hidden': False}, {'_id': '67986d764fccd4b95149db11', 'name': 'Simon Lacoste-Julien', 'hidden': False}], 'publishedAt': '2025-01-24T20:39:38.000Z', 'title': 'Feasible Learning', 'summary': 'We introduce Feasible Learning (FL), a sample-centric learning paradigm where\\nmodels are trained by solving a feasibility problem that bounds the loss for\\neach training sample. In contrast to the ubiquitous Empirical Risk Minimization\\n(ERM) framework, which optimizes for average performance, FL demands\\nsatisfactory performance on every individual data point. Since any model that\\nmeets the prescribed performance threshold is a valid FL solution, the choice\\nof optimization algorithm and its dynamics play a crucial role in shaping the\\nproperties of the resulting solutions. In particular, we study a primal-dual\\napproach which dynamically re-weights the importance of each sample during\\ntraining. To address the challenge of setting a meaningful threshold in\\npractice, we introduce a relaxation of FL that incorporates slack variables of\\nminimal norm. Our empirical analysis, spanning image classification, age\\nregression, and preference optimization in large language models, demonstrates\\nthat models trained via FL can learn from data while displaying improved tail\\nbehavior compared to ERM, with only a marginal impact on average performance.', 'upvotes': 1, 'discussionId': '67986d784fccd4b95149db6b'}, 'publishedAt': '2025-01-28T00:39:11.423Z', 'title': 'Feasible Learning', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.14912.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5834}, 'isAuthorParticipating': False}"
]