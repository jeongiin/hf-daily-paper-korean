[
    "{'paper': {'id': '2501.09751', 'authors': [{'_id': '6789f776766dd160379b89fb', 'user': {'_id': '647229256facfb01d8ae7b89', 'avatarUrl': '/avatars/2fc34d2739b28c1089b20e7a7fa40f0e.svg', 'isPro': False, 'fullname': 'Xi Ze Kun', 'user': 'ZekunXi', 'type': 'user'}, 'name': 'Zekun Xi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:29:57.432Z', 'hidden': False}, {'_id': '6789f776766dd160379b89fc', 'name': 'Wenbiao Yin', 'hidden': False}, {'_id': '6789f776766dd160379b89fd', 'user': {'_id': '669663472d25bd04e9af1d66', 'avatarUrl': '/avatars/8b11d5d79d1b8b205baa498a942f573c.svg', 'isPro': False, 'fullname': 'Jizhan Fang', 'user': 'JizhanFang', 'type': 'user'}, 'name': 'Jizhan Fang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:30:13.133Z', 'hidden': False}, {'_id': '6789f776766dd160379b89fe', 'user': {'_id': '644a4fbc2166258fccc664bc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg', 'isPro': False, 'fullname': 'Jialong Wu', 'user': 'callanwu', 'type': 'user'}, 'name': 'Jialong Wu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:31:40.494Z', 'hidden': False}, {'_id': '6789f776766dd160379b89ff', 'user': {'_id': '63d32cd7b734eaa4d4fa410b', 'avatarUrl': '/avatars/68acb80f62bc6493e1ad26506999b6c4.svg', 'isPro': False, 'fullname': 'Runnan Fang', 'user': 'Runnaning', 'type': 'user'}, 'name': 'Runnan Fang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:31:49.555Z', 'hidden': False}, {'_id': '6789f776766dd160379b8a00', 'user': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'name': 'Ningyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-17T10:25:07.769Z', 'hidden': False}, {'_id': '6789f776766dd160379b8a01', 'name': 'Jiang Yong', 'hidden': False}, {'_id': '6789f776766dd160379b8a02', 'user': {'_id': '63a091e42fabbbb89991f5ce', 'avatarUrl': '/avatars/d55485b06461764c36c9edf9d6e8892c.svg', 'isPro': False, 'fullname': 'pengjun xie', 'user': 'xpjandy', 'type': 'user'}, 'name': 'Pengjun Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:32:07.000Z', 'hidden': False}, {'_id': '6789f776766dd160379b8a03', 'user': {'_id': '635b8b6a37c6a2c12e2cce00', 'avatarUrl': '/avatars/229fb72180529141515d1df797b33709.svg', 'isPro': False, 'fullname': 'Fei Huang', 'user': 'hzhwcmhf', 'type': 'user'}, 'name': 'Fei Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:32:22.107Z', 'hidden': False}, {'_id': '6789f776766dd160379b8a04', 'user': {'_id': '64931296137833d7ec7689cd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64931296137833d7ec7689cd/TBihNdp1ZwIWjhfAWjRr6.jpeg', 'isPro': False, 'fullname': 'Huajun Chen', 'user': 'huajunsir', 'type': 'user'}, 'name': 'Huajun Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:32:36.118Z', 'hidden': False}], 'publishedAt': '2025-01-16T18:58:06.000Z', 'title': 'OmniThink: Expanding Knowledge Boundaries in Machine Writing through\\n  Thinking', 'summary': \"Machine writing with large language models often relies on\\nretrieval-augmented generation. However, these approaches remain confined\\nwithin the boundaries of the model's predefined scope, limiting the generation\\nof content with rich information. Specifically, vanilla-retrieved information\\ntends to lack depth, utility, and suffers from redundancy, which negatively\\nimpacts the quality of generated articles, leading to shallow, repetitive, and\\nunoriginal outputs. To address these issues, we propose OmniThink, a machine\\nwriting framework that emulates the human-like process of iterative expansion\\nand reflection. The core idea behind OmniThink is to simulate the cognitive\\nbehavior of learners as they progressively deepen their knowledge of the\\ntopics. Experimental results demonstrate that OmniThink improves the knowledge\\ndensity of generated articles without compromising metrics such as coherence\\nand depth. Human evaluations and expert feedback further highlight the\\npotential of OmniThink to address real-world challenges in the generation of\\nlong-form articles.\", 'upvotes': 26, 'discussionId': '6789f777766dd160379b8a39'}, 'publishedAt': '2025-01-17T01:24:01.494Z', 'title': 'OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09751.png', 'numComments': 1, 'submittedBy': {'_id': '645dbaa6f5760d1530d7580d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg', 'fullname': 'Simeon Emanuilov', 'name': 's-emanuilov', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 15}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09732', 'authors': [{'_id': '6789e1dfaa9f64e4af498482', 'user': {'_id': '66c398fc5f4422f886b71a00', 'avatarUrl': '/avatars/9cd690d7857de1b926ddcdc2bccbfdfa.svg', 'isPro': False, 'fullname': 'Nanye Ma', 'user': 'willllis', 'type': 'user'}, 'name': 'Nanye Ma', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:32:46.832Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498483', 'user': {'_id': '64d39e0f5ceebf9c30359082', 'avatarUrl': '/avatars/7c21f18874498a793dd2275277d4dafb.svg', 'isPro': False, 'fullname': 'Shangyuan Tong', 'user': 'S8T', 'type': 'user'}, 'name': 'Shangyuan Tong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:32:52.717Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498484', 'name': 'Haolin Jia', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498485', 'user': {'_id': '643441ccd55dea2d0ec2c309', 'avatarUrl': '/avatars/82e99d445e2b513ad7270fa852adbcbb.svg', 'isPro': False, 'fullname': 'Hexiang Hu', 'user': 'hexianghu', 'type': 'user'}, 'name': 'Hexiang Hu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:33:08.220Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498486', 'user': {'_id': '6729508649696b4e066b0506', 'avatarUrl': '/avatars/246e13b236edf039f2ca27e4f4051be8.svg', 'isPro': False, 'fullname': 'Yu-Chuan Su', 'user': 'ycsu', 'type': 'user'}, 'name': 'Yu-Chuan Su', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:33:14.078Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498487', 'user': {'_id': '65676b1711b2bbd6c2ab093a', 'avatarUrl': '/avatars/bdd4364057c6b9e54d7ec451ad1ffb64.svg', 'isPro': False, 'fullname': 'mingdazhang', 'user': 'mingdazhang', 'type': 'user'}, 'name': 'Mingda Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:33:35.040Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498488', 'name': 'Xuan Yang', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af498489', 'user': {'_id': '6338914220fc636fd8b27fb8', 'avatarUrl': '/avatars/4c5a0c925c0f0296e02aa498218f339d.svg', 'isPro': False, 'fullname': 'li', 'user': 'yandong', 'type': 'user'}, 'name': 'Yandong Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:33:47.975Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af49848a', 'name': 'Tommi Jaakkola', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af49848b', 'user': {'_id': '634ce7ce05f736dff37aae5f', 'avatarUrl': '/avatars/1405c76a38f7ea497e4439f0e4e786a8.svg', 'isPro': False, 'fullname': 'Xuhui Jia', 'user': 'Jxh-cuit', 'type': 'user'}, 'name': 'Xuhui Jia', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:33:58.031Z', 'hidden': False}, {'_id': '6789e1dfaa9f64e4af49848c', 'user': {'_id': '6596422646624a86ff3b3bda', 'avatarUrl': '/avatars/216e12b77e45ac5f1fa20932f5745411.svg', 'isPro': False, 'fullname': 'Saining Xie', 'user': 'sainx', 'type': 'user'}, 'name': 'Saining Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:34:03.297Z', 'hidden': False}], 'publishedAt': '2025-01-16T18:30:37.000Z', 'title': 'Inference-Time Scaling for Diffusion Models beyond Scaling Denoising\\n  Steps', 'summary': 'Generative models have made significant impacts across various domains,\\nlargely due to their ability to scale during training by increasing data,\\ncomputational resources, and model size, a phenomenon characterized by the\\nscaling laws. Recent research has begun to explore inference-time scaling\\nbehavior in Large Language Models (LLMs), revealing how performance can further\\nimprove with additional computation during inference. Unlike LLMs, diffusion\\nmodels inherently possess the flexibility to adjust inference-time computation\\nvia the number of denoising steps, although the performance gains typically\\nflatten after a few dozen. In this work, we explore the inference-time scaling\\nbehavior of diffusion models beyond increasing denoising steps and investigate\\nhow the generation performance can further improve with increased computation.\\nSpecifically, we consider a search problem aimed at identifying better noises\\nfor the diffusion sampling process. We structure the design space along two\\naxes: the verifiers used to provide feedback, and the algorithms used to find\\nbetter noise candidates. Through extensive experiments on class-conditioned and\\ntext-conditioned image generation benchmarks, our findings reveal that\\nincreasing inference-time compute leads to substantial improvements in the\\nquality of samples generated by diffusion models, and with the complicated\\nnature of images, combinations of the components in the framework can be\\nspecifically chosen to conform with different application scenario.', 'upvotes': 22, 'discussionId': '6789e1e4aa9f64e4af498679'}, 'publishedAt': '2025-01-16T23:52:15.279Z', 'title': 'Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09732.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09484', 'authors': [{'_id': '6789f2795a84f1087bc9274a', 'user': {'_id': '633e570be7d5ce7bfe037a53', 'avatarUrl': '/avatars/f0997831ad5ccdaa29d070fed294e2f6.svg', 'isPro': False, 'fullname': 'Zhaocheng Liu', 'user': 'zhaocheng', 'type': 'user'}, 'name': 'Zhaocheng Liu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-17T10:25:21.951Z', 'hidden': False}, {'_id': '6789f2795a84f1087bc9274b', 'name': 'Quan Tu', 'hidden': False}, {'_id': '6789f2795a84f1087bc9274c', 'name': 'Wen Ye', 'hidden': False}, {'_id': '6789f2795a84f1087bc9274d', 'name': 'Yu Xiao', 'hidden': False}, {'_id': '6789f2795a84f1087bc9274e', 'name': 'Zhishou Zhang', 'hidden': False}, {'_id': '6789f2795a84f1087bc9274f', 'name': 'Hengfu Cui', 'hidden': False}, {'_id': '6789f2795a84f1087bc92750', 'name': 'Yalun Zhu', 'hidden': False}, {'_id': '6789f2795a84f1087bc92751', 'user': {'_id': '62dcdb86d36b2070f928a51e', 'avatarUrl': '/avatars/a341e4305217f8abd14cff97201a24aa.svg', 'isPro': False, 'fullname': 'sdujq', 'user': 'sdujq', 'type': 'user'}, 'name': 'Qiang Ju', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-17T10:25:20.100Z', 'hidden': False}, {'_id': '6789f2795a84f1087bc92752', 'user': {'_id': '64ab92c362b769f936bba203', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ab92c362b769f936bba203/Kq3Nlnq3DTPwungx8r9G5.jpeg', 'isPro': False, 'fullname': 'Shizheng Li', 'user': 'ShizhengLi', 'type': 'user'}, 'name': 'Shizheng Li', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:34:35.155Z', 'hidden': False}, {'_id': '6789f2795a84f1087bc92753', 'user': {'_id': '62d65139667051e0a29bffe7', 'avatarUrl': '/avatars/0252aa2bcd4cf1c8e4b87e5f164b6da5.svg', 'isPro': False, 'fullname': 'Jian Xie', 'user': 'hsaest', 'type': 'user'}, 'name': 'Jian Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:34:48.855Z', 'hidden': False}], 'publishedAt': '2025-01-16T11:41:14.000Z', 'title': 'Exploring the Inquiry-Diagnosis Relationship with Advanced Patient\\n  Simulators', 'summary': 'Online medical consultation (OMC) restricts doctors to gathering patient\\ninformation solely through inquiries, making the already complex sequential\\ndecision-making process of diagnosis even more challenging. Recently, the rapid\\nadvancement of large language models has demonstrated a significant potential\\nto transform OMC. However, most studies have primarily focused on improving\\ndiagnostic accuracy under conditions of relatively sufficient information,\\nwhile paying limited attention to the \"inquiry\" phase of the consultation\\nprocess. This lack of focus has left the relationship between \"inquiry\" and\\n\"diagnosis\" insufficiently explored. In this paper, we first extract real\\npatient interaction strategies from authentic doctor-patient conversations and\\nuse these strategies to guide the training of a patient simulator that closely\\nmirrors real-world behavior. By inputting medical records into our patient\\nsimulator to simulate patient responses, we conduct extensive experiments to\\nexplore the relationship between \"inquiry\" and \"diagnosis\" in the consultation\\nprocess. Experimental results demonstrate that inquiry and diagnosis adhere to\\nthe Liebig\\'s law: poor inquiry quality limits the effectiveness of diagnosis,\\nregardless of diagnostic capability, and vice versa. Furthermore, the\\nexperiments reveal significant differences in the inquiry performance of\\nvarious models. To investigate this phenomenon, we categorize the inquiry\\nprocess into four types: (1) chief complaint inquiry; (2) specification of\\nknown symptoms; (3) inquiry about accompanying symptoms; and (4) gathering\\nfamily or medical history. We analyze the distribution of inquiries across the\\nfour types for different models to explore the reasons behind their significant\\nperformance differences. We plan to open-source the weights and related code of\\nour patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.', 'upvotes': 14, 'discussionId': '6789f27a5a84f1087bc9279e'}, 'publishedAt': '2025-01-17T01:05:31.701Z', 'title': 'Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09484.png', 'numComments': 3, 'submittedBy': {'_id': '60ec4a33375c1280fb422704', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1633664276374-60ec4a33375c1280fb422704.png', 'fullname': 'Wang Yulong', 'name': 'wangyulong', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 5}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09756', 'authors': [{'_id': '6789e9b78a27185f5084533a', 'name': 'Sumit Chaturvedi', 'hidden': False}, {'_id': '6789e9b78a27185f5084533b', 'user': {'_id': '63b48ed5a50cfcefda9dbe67', 'avatarUrl': '/avatars/98a2f07ea6a7ce3792f250cf9fecf402.svg', 'isPro': False, 'fullname': 'Mengwei Ren', 'user': 'mengweir', 'type': 'user'}, 'name': 'Mengwei Ren', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:35:43.879Z', 'hidden': False}, {'_id': '6789e9b78a27185f5084533c', 'name': 'Yannick Hold-Geoffroy', 'hidden': False}, {'_id': '6789e9b78a27185f5084533d', 'name': 'Jingyuan Liu', 'hidden': False}, {'_id': '6789e9b78a27185f5084533e', 'name': 'Julie Dorsey', 'hidden': False}, {'_id': '6789e9b78a27185f5084533f', 'user': {'_id': '62a8efd508a7ea93ff18785a', 'avatarUrl': '/avatars/ff259233d437833a304329bb973a5a04.svg', 'isPro': False, 'fullname': 'Zhixin Shu', 'user': 'zhixinshu', 'type': 'user'}, 'name': 'Zhixin Shu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:36:05.684Z', 'hidden': False}], 'publishedAt': '2025-01-16T18:59:48.000Z', 'title': 'SynthLight: Portrait Relighting with Diffusion Model by Learning to\\n  Re-render Synthetic Faces', 'summary': \"We introduce SynthLight, a diffusion model for portrait relighting. Our\\napproach frames image relighting as a re-rendering problem, where pixels are\\ntransformed in response to changes in environmental lighting conditions. Using\\na physically-based rendering engine, we synthesize a dataset to simulate this\\nlighting-conditioned transformation with 3D head assets under varying lighting.\\nWe propose two training and inference strategies to bridge the gap between the\\nsynthetic and real image domains: (1) multi-task training that takes advantage\\nof real human portraits without lighting labels; (2) an inference time\\ndiffusion sampling procedure based on classifier-free guidance that leverages\\nthe input portrait to better preserve details. Our method generalizes to\\ndiverse real photographs and produces realistic illumination effects, including\\nspecular highlights and cast shadows, while preserving the subject's identity.\\nOur quantitative experiments on Light Stage data demonstrate results comparable\\nto state-of-the-art relighting methods. Our qualitative results on in-the-wild\\nimages showcase rich and unprecedented illumination effects. Project Page:\\nhttps://vrroom.github.io/synthlight/\", 'upvotes': 11, 'discussionId': '6789e9bd8a27185f50845514'}, 'publishedAt': '2025-01-17T00:25:22.582Z', 'title': 'SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09756.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09747', 'authors': [{'_id': '6789e918e1b3fda757de947f', 'user': {'_id': '65d4c1ff29b4ac81c265e6e6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65d4c1ff29b4ac81c265e6e6/GXgs28okxGfpBdqhxov9-.png', 'isPro': False, 'fullname': 'Karl Pertsch', 'user': 'KarlP', 'type': 'user'}, 'name': 'Karl Pertsch', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:36:46.947Z', 'hidden': False}, {'_id': '6789e918e1b3fda757de9480', 'user': {'_id': '6307eabda670ed10f9d2571f', 'avatarUrl': '/avatars/b0811b25ed4fb7e48dd380898049c764.svg', 'isPro': False, 'fullname': 'Kyle Stachowicz', 'user': 'kylestach', 'type': 'user'}, 'name': 'Kyle Stachowicz', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:36:59.830Z', 'hidden': False}, {'_id': '6789e918e1b3fda757de9481', 'user': {'_id': '633a26ab474cfeb1a864dc56', 'avatarUrl': '/avatars/cd40accd894fc78810fc0d5108f413e9.svg', 'isPro': False, 'fullname': 'Brian I', 'user': 'brianichter', 'type': 'user'}, 'name': 'Brian Ichter', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:37:05.229Z', 'hidden': False}, {'_id': '6789e918e1b3fda757de9482', 'user': {'_id': '67225875b46c703941fa7967', 'avatarUrl': '/avatars/7c89fbdd9a135210209bcd0cbfe7988a.svg', 'isPro': False, 'fullname': 'Danny Driess', 'user': 'dannydriess', 'type': 'user'}, 'name': 'Danny Driess', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:37:10.829Z', 'hidden': False}, {'_id': '6789e918e1b3fda757de9483', 'name': 'Suraj Nair', 'hidden': False}, {'_id': '6789e918e1b3fda757de9484', 'name': 'Quan Vuong', 'hidden': False}, {'_id': '6789e918e1b3fda757de9485', 'user': {'_id': '663a7190d9dee283e3f56150', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/663a7190d9dee283e3f56150/zoYBdFIQPGWTS0R7bpg07.jpeg', 'isPro': False, 'fullname': 'Oier Mees', 'user': 'oier-mees', 'type': 'user'}, 'name': 'Oier Mees', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:37:30.435Z', 'hidden': False}, {'_id': '6789e918e1b3fda757de9486', 'user': {'_id': '64ac22a9193f0a807deb673d', 'avatarUrl': '/avatars/fcac4912678ad3cb6e817d40bdee9aea.svg', 'isPro': False, 'fullname': 'Chelsea Finn', 'user': 'cbfinn', 'type': 'user'}, 'name': 'Chelsea Finn', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:37:36.079Z', 'hidden': False}, {'_id': '6789e918e1b3fda757de9487', 'user': {'_id': '665ce54120a307a3754849dd', 'avatarUrl': '/avatars/e698726e9be61dd50ce2efe372ed5dac.svg', 'isPro': False, 'fullname': 'Sergey Levine', 'user': 'svlevine', 'type': 'user'}, 'name': 'Sergey Levine', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:37:41.213Z', 'hidden': False}], 'publishedAt': '2025-01-16T18:57:04.000Z', 'title': 'FAST: Efficient Action Tokenization for Vision-Language-Action Models', 'summary': 'Autoregressive sequence models, such as Transformer-based vision-language\\naction (VLA) policies, can be tremendously effective for capturing complex and\\ngeneralizable robotic behaviors. However, such models require us to choose a\\ntokenization of our continuous action signals, which determines how the\\ndiscrete symbols predicted by the model map to continuous robot actions. We\\nfind that current approaches for robot action tokenization, based on simple\\nper-dimension, per-timestep binning schemes, typically perform poorly when\\nlearning dexterous skills from high-frequency robot data. To address this\\nchallenge, we propose a new compression-based tokenization scheme for robot\\nactions, based on the discrete cosine transform. Our tokenization approach,\\nFrequency-space Action Sequence Tokenization (FAST), enables us to train\\nautoregressive VLAs for highly dexterous and high-frequency tasks where\\nstandard discretization methods fail completely. Based on FAST, we release\\nFAST+, a universal robot action tokenizer, trained on 1M real robot action\\ntrajectories. It can be used as a black-box tokenizer for a wide range of robot\\naction sequences, with diverse action spaces and control frequencies. Finally,\\nwe show that, when combined with the pi0 VLA, our method can scale to training\\non 10k hours of robot data and match the performance of diffusion VLAs, while\\nreducing training time by up to 5x.', 'upvotes': 9, 'discussionId': '6789e91ae1b3fda757de94d3'}, 'publishedAt': '2025-01-17T00:22:48.005Z', 'title': 'FAST: Efficient Action Tokenization for Vision-Language-Action Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09747.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09755', 'authors': [{'_id': '6789e275810f471d6aa3d2fa', 'name': 'Philippe Hansen-Estruch', 'hidden': False}, {'_id': '6789e275810f471d6aa3d2fb', 'name': 'David Yan', 'hidden': False}, {'_id': '6789e275810f471d6aa3d2fc', 'name': 'Ching-Yao Chung', 'hidden': False}, {'_id': '6789e275810f471d6aa3d2fd', 'user': {'_id': '648c9605565e3a44f3c9bb7b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/648c9605565e3a44f3c9bb7b/W5chvk17Zol6-2QSWkFVR.jpeg', 'isPro': True, 'fullname': 'Orr Zohar', 'user': 'orrzohar', 'type': 'user'}, 'name': 'Orr Zohar', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:38:14.289Z', 'hidden': False}, {'_id': '6789e275810f471d6aa3d2fe', 'name': 'Jialiang Wang', 'hidden': False}, {'_id': '6789e275810f471d6aa3d2ff', 'user': {'_id': '655846d7ed8df83128f5826a', 'avatarUrl': '/avatars/d7ce174d7d1b8614d5f6f071225c0057.svg', 'isPro': False, 'fullname': 'Hou', 'user': 'Tingbo', 'type': 'user'}, 'name': 'Tingbo Hou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:38:38.454Z', 'hidden': False}, {'_id': '6789e275810f471d6aa3d300', 'name': 'Tao Xu', 'hidden': False}, {'_id': '6789e275810f471d6aa3d301', 'name': 'Sriram Vishwanath', 'hidden': False}, {'_id': '6789e275810f471d6aa3d302', 'name': 'Peter Vajda', 'hidden': False}, {'_id': '6789e275810f471d6aa3d303', 'user': {'_id': '63e58e3a006a775275e59e41', 'avatarUrl': '/avatars/75262a35b27a2ae1939df9118120d99e.svg', 'isPro': False, 'fullname': 'Xinlei Chen', 'user': 'endernewton', 'type': 'user'}, 'name': 'Xinlei Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:38:56.604Z', 'hidden': False}], 'publishedAt': '2025-01-16T18:59:04.000Z', 'title': 'Learnings from Scaling Visual Tokenizers for Reconstruction and\\n  Generation', 'summary': \"Visual tokenization via auto-encoding empowers state-of-the-art image and\\nvideo generative models by compressing pixels into a latent space. Although\\nscaling Transformer-based generators has been central to recent advances, the\\ntokenizer component itself is rarely scaled, leaving open questions about how\\nauto-encoder design choices influence both its objective of reconstruction and\\ndownstream generative performance. Our work aims to conduct an exploration of\\nscaling in auto-encoders to fill in this blank. To facilitate this exploration,\\nwe replace the typical convolutional backbone with an enhanced Vision\\nTransformer architecture for Tokenization (ViTok). We train ViTok on\\nlarge-scale image and video datasets far exceeding ImageNet-1K, removing data\\nconstraints on tokenizer scaling. We first study how scaling the auto-encoder\\nbottleneck affects both reconstruction and generation -- and find that while it\\nis highly correlated with reconstruction, its relationship with generation is\\nmore complex. We next explored the effect of separately scaling the\\nauto-encoders' encoder and decoder on reconstruction and generation\\nperformance. Crucially, we find that scaling the encoder yields minimal gains\\nfor either reconstruction or generation, while scaling the decoder boosts\\nreconstruction but the benefits for generation are mixed. Building on our\\nexploration, we design ViTok as a lightweight auto-encoder that achieves\\ncompetitive performance with state-of-the-art auto-encoders on ImageNet-1K and\\nCOCO reconstruction tasks (256p and 512p) while outperforming existing\\nauto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x\\nfewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates\\ncompetitive performance on image generation for ImageNet-1K and sets new\\nstate-of-the-art benchmarks for class-conditional video generation on UCF-101.\", 'upvotes': 9, 'discussionId': '6789e27a810f471d6aa3d4e2'}, 'publishedAt': '2025-01-16T23:54:42.823Z', 'title': 'Learnings from Scaling Visual Tokenizers for Reconstruction and Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09755.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09686', 'authors': [{'_id': '6789e04652739943c940af38', 'name': 'Fengli Xu', 'hidden': False}, {'_id': '6789e04652739943c940af39', 'user': {'_id': '6583e2b283a9e1460c6fb1e0', 'avatarUrl': '/avatars/a949165b1cec5e1d1d55f3af98182156.svg', 'isPro': False, 'fullname': 'Qianyue Hao', 'user': 'haohao11', 'type': 'user'}, 'name': 'Qianyue Hao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:42:15.441Z', 'hidden': False}, {'_id': '6789e04652739943c940af3a', 'user': {'_id': '64feba7efa64465422ce3003', 'avatarUrl': '/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg', 'isPro': False, 'fullname': 'zongzefang', 'user': 'zzfoutofspace', 'type': 'user'}, 'name': 'Zefang Zong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:42:26.065Z', 'hidden': False}, {'_id': '6789e04652739943c940af3b', 'name': 'Jingwei Wang', 'hidden': False}, {'_id': '6789e04652739943c940af3c', 'user': {'_id': '64071cbc2e309e65451f87b6', 'avatarUrl': '/avatars/2201244380d221b9db5661f20510d853.svg', 'isPro': False, 'fullname': 'yunke zhang', 'user': 'berserkerko', 'type': 'user'}, 'name': 'Yunke Zhang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:43:05.430Z', 'hidden': False}, {'_id': '6789e04652739943c940af3d', 'name': 'Jingyi Wang', 'hidden': False}, {'_id': '6789e04652739943c940af3e', 'name': 'Xiaochong Lan', 'hidden': False}, {'_id': '6789e04652739943c940af3f', 'user': {'_id': '6509a040d95f30b9dcdbf789', 'avatarUrl': '/avatars/4e4b441e22a1f2c3387e5b981ba6bbbe.svg', 'isPro': False, 'fullname': 'Jiahui Gong', 'user': 'zhazhahui7', 'type': 'user'}, 'name': 'Jiahui Gong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:43:44.415Z', 'hidden': False}, {'_id': '6789e04652739943c940af40', 'user': {'_id': '6566fcb5118497d0af91dc3b', 'avatarUrl': '/avatars/e081d7d1b1657245cd818e5417cdcb2e.svg', 'isPro': False, 'fullname': 'TIANJIAN OUYANG', 'user': 'Ouyangtj', 'type': 'user'}, 'name': 'Tianjian Ouyang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:43:50.599Z', 'hidden': False}, {'_id': '6789e04652739943c940af41', 'name': 'Fanjin Meng', 'hidden': False}, {'_id': '6789e04652739943c940af42', 'user': {'_id': '654dd731671d2c1ced0539e1', 'avatarUrl': '/avatars/7eaa5eee16aac9105cd2af7ee84841c3.svg', 'isPro': False, 'fullname': 'shaochenyang', 'user': 'l-1-l', 'type': 'user'}, 'name': 'Chenyang Shao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:44:14.869Z', 'hidden': False}, {'_id': '6789e04652739943c940af43', 'user': {'_id': '668e965959d9ffef7e8b035a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tqjvWWxXgTkNhYeNhoPJQ.jpeg', 'isPro': False, 'fullname': 'Yuwei Yan', 'user': 'PinkGranite', 'type': 'user'}, 'name': 'Yuwei Yan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:44:24.670Z', 'hidden': False}, {'_id': '6789e04652739943c940af44', 'user': {'_id': '663ef727f9c2d3c9da3b8d4b', 'avatarUrl': '/avatars/4bb431e410078f7ba2487597d1beb589.svg', 'isPro': False, 'fullname': 'qinglong yang', 'user': 'm912218831', 'type': 'user'}, 'name': 'Qinglong Yang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:44:31.561Z', 'hidden': False}, {'_id': '6789e04652739943c940af45', 'user': {'_id': '6517326e335b4c728b3406a4', 'avatarUrl': '/avatars/cde4a672778fc69c172e6bdeee2d540a.svg', 'isPro': False, 'fullname': 'Yiwen Song', 'user': 'yiwen-song', 'type': 'user'}, 'name': 'Yiwen Song', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:44:57.348Z', 'hidden': False}, {'_id': '6789e04652739943c940af46', 'name': 'Sijian Ren', 'hidden': False}, {'_id': '6789e04652739943c940af47', 'name': 'Xinyuan Hu', 'hidden': False}, {'_id': '6789e04652739943c940af48', 'name': 'Yu Li', 'hidden': False}, {'_id': '6789e04652739943c940af49', 'name': 'Jie Feng', 'hidden': False}, {'_id': '6789e04652739943c940af4a', 'user': {'_id': '65049f41c6ae3df8f28cfe96', 'avatarUrl': '/avatars/a653ed4a215c4bd34710a1cee9f4d9cd.svg', 'isPro': False, 'fullname': 'Chen Gao', 'user': 'gaochen315', 'type': 'user'}, 'name': 'Chen Gao', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:45:59.888Z', 'hidden': False}, {'_id': '6789e04652739943c940af4b', 'name': 'Yong Li', 'hidden': False}], 'publishedAt': '2025-01-16T17:37:58.000Z', 'title': 'Towards Large Reasoning Models: A Survey of Reinforced Reasoning with\\n  Large Language Models', 'summary': 'Language has long been conceived as an essential tool for human reasoning.\\nThe breakthrough of Large Language Models (LLMs) has sparked significant\\nresearch interest in leveraging these models to tackle complex reasoning tasks.\\nResearchers have moved beyond simple autoregressive token generation by\\nintroducing the concept of \"thought\" -- a sequence of tokens representing\\nintermediate steps in the reasoning process. This innovative paradigm enables\\nLLMs\\' to mimic complex human reasoning processes, such as tree search and\\nreflective thinking. Recently, an emerging trend of learning to reason has\\napplied reinforcement learning (RL) to train LLMs to master reasoning\\nprocesses. This approach enables the automatic generation of high-quality\\nreasoning trajectories through trial-and-error search algorithms, significantly\\nexpanding LLMs\\' reasoning capacity by providing substantially more training\\ndata. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\"\\nwith more tokens during test-time inference can further significantly boost\\nreasoning accuracy. Therefore, the train-time and test-time scaling combined to\\nshow a new research frontier -- a path toward Large Reasoning Model. The\\nintroduction of OpenAI\\'s o1 series marks a significant milestone in this\\nresearch direction. In this survey, we present a comprehensive review of recent\\nprogress in LLM reasoning. We begin by introducing the foundational background\\nof LLMs and then explore the key technical components driving the development\\nof large reasoning models, with a focus on automated data construction,\\nlearning-to-reason techniques, and test-time scaling. We also analyze popular\\nopen-source projects at building large reasoning models, and conclude with open\\nchallenges and future research directions.', 'upvotes': 7, 'discussionId': '6789e04752739943c940afa5'}, 'publishedAt': '2025-01-16T23:45:20.697Z', 'title': 'Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09686.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08617', 'authors': [{'_id': '6789d842f16a4dec461a2040', 'user': {'_id': '650237b5d1b0b0db4f29ae8a', 'avatarUrl': '/avatars/8f92cf8f3f1ddb45c2c58c4a59ce4633.svg', 'isPro': False, 'fullname': 'KAIQU LIANG', 'user': 'kaiquliang', 'type': 'user'}, 'name': 'Kaiqu Liang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:39:14.928Z', 'hidden': False}, {'_id': '6789d842f16a4dec461a2041', 'name': 'Haimin Hu', 'hidden': False}, {'_id': '6789d842f16a4dec461a2042', 'name': 'Ryan Liu', 'hidden': False}, {'_id': '6789d842f16a4dec461a2043', 'name': 'Thomas L. Griffiths', 'hidden': False}, {'_id': '6789d842f16a4dec461a2044', 'name': 'Jaime Fernández Fisac', 'hidden': False}], 'publishedAt': '2025-01-15T06:33:15.000Z', 'title': 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation', 'summary': \"Generative AI systems like foundation models (FMs) must align well with human\\nvalues to ensure their behavior is helpful and trustworthy. While Reinforcement\\nLearning from Human Feedback (RLHF) has shown promise for optimizing model\\nperformance using human judgments, existing RLHF pipelines predominantly rely\\non immediate feedback, which can fail to accurately reflect the downstream\\nimpact of an interaction on users' utility. We demonstrate that feedback based\\non evaluators' foresight estimates of downstream consequences systematically\\ninduces Goodhart's Law dynamics, incentivizing misaligned behaviors like\\nsycophancy and deception and ultimately degrading user outcomes. To alleviate\\nthis, we propose decoupling evaluation from prediction by refocusing RLHF on\\nhindsight feedback. Our theoretical analysis reveals that conditioning\\nevaluator feedback on downstream observations mitigates misalignment and\\nimproves expected human utility, even when these observations are simulated by\\nthe AI system itself. To leverage this insight in a practical alignment\\nalgorithm, we introduce Reinforcement Learning from Hindsight Simulation\\n(RLHS), which first simulates plausible consequences and then elicits feedback\\nto assess what behaviors were genuinely beneficial in hindsight. We apply RLHS\\nto two widely-employed online and offline preference optimization methods --\\nProximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) --\\nand show empirically that misalignment is significantly reduced with both\\nmethods. Through an online human user study, we show that RLHS consistently\\noutperforms RLHF in helping users achieve their goals and earns higher\\nsatisfaction ratings, despite being trained solely with simulated hindsight\\nfeedback. These results underscore the importance of focusing on long-term\\nconsequences, even simulated ones, to mitigate misalignment in RLHF.\", 'upvotes': 7, 'discussionId': '6789d844f16a4dec461a20dc'}, 'publishedAt': '2025-01-16T23:16:25.298Z', 'title': 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/650237b5d1b0b0db4f29ae8a/TdAsH0rQE1qFCphiura5C.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08617.png', 'numComments': 1, 'submittedBy': {'_id': '650237b5d1b0b0db4f29ae8a', 'avatarUrl': '/avatars/8f92cf8f3f1ddb45c2c58c4a59ce4633.svg', 'fullname': 'KAIQU LIANG', 'name': 'kaiquliang', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.09503', 'authors': [{'_id': '6789e8178a14e7d6ea183dd3', 'name': 'Junjie He', 'hidden': False}, {'_id': '6789e8178a14e7d6ea183dd4', 'user': {'_id': '64130f4014c9a170ae873bd8', 'avatarUrl': '/avatars/97f4609bbfe81660b28a12f3b135cdc3.svg', 'isPro': False, 'fullname': 'tuoyuxiang', 'user': 'tuoyuxiang', 'type': 'user'}, 'name': 'Yuxiang Tuo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:47:02.023Z', 'hidden': False}, {'_id': '6789e8178a14e7d6ea183dd5', 'user': {'_id': '63b66df8889aa6707f167f5d', 'avatarUrl': '/avatars/62248a70698a95f74ce7267ca42cacef.svg', 'isPro': False, 'fullname': 'ashui', 'user': 'ashui', 'type': 'user'}, 'name': 'Binghui Chen', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-17T05:18:19.329Z', 'hidden': False}, {'_id': '6789e8178a14e7d6ea183dd6', 'name': 'Chongyang Zhong', 'hidden': False}, {'_id': '6789e8178a14e7d6ea183dd7', 'user': {'_id': '659caf6b0030e8faffb00c41', 'avatarUrl': '/avatars/dde52bcf694120e59307b4ab8a7eeb33.svg', 'isPro': False, 'fullname': 'gengyifeng', 'user': 'gengyifeng', 'type': 'user'}, 'name': 'Yifeng Geng', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:47:21.325Z', 'hidden': False}, {'_id': '6789e8178a14e7d6ea183dd8', 'user': {'_id': '63d0cc736b985b0f25d0412c', 'avatarUrl': '/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg', 'isPro': False, 'fullname': 'Bo', 'user': 'Liefeng', 'type': 'user'}, 'name': 'Liefeng Bo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:47:29.725Z', 'hidden': False}], 'publishedAt': '2025-01-16T12:28:39.000Z', 'title': 'AnyStory: Towards Unified Single and Multiple Subject Personalization in\\n  Text-to-Image Generation', 'summary': 'Recently, large-scale generative models have demonstrated outstanding\\ntext-to-image generation capabilities. However, generating high-fidelity\\npersonalized images with specific subjects still presents challenges,\\nespecially in cases involving multiple subjects. In this paper, we propose\\nAnyStory, a unified approach for personalized subject generation. AnyStory not\\nonly achieves high-fidelity personalization for single subjects, but also for\\nmultiple subjects, without sacrificing subject fidelity. Specifically, AnyStory\\nmodels the subject personalization problem in an \"encode-then-route\" manner. In\\nthe encoding step, AnyStory utilizes a universal and powerful image encoder,\\ni.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve\\nhigh-fidelity encoding of subject features. In the routing step, AnyStory\\nutilizes a decoupled instance-aware subject router to accurately perceive and\\npredict the potential location of the corresponding subject in the latent\\nspace, and guide the injection of subject conditions. Detailed experimental\\nresults demonstrate the excellent performance of our method in retaining\\nsubject details, aligning text descriptions, and personalizing for multiple\\nsubjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .', 'upvotes': 5, 'discussionId': '6789e81b8a14e7d6ea183f16'}, 'publishedAt': '2025-01-17T00:18:47.834Z', 'title': 'AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09503.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09433', 'authors': [{'_id': '6789e73d7a7ce2b07045dd3e', 'name': 'Hwan Heo', 'hidden': False}, {'_id': '6789e73d7a7ce2b07045dd3f', 'user': {'_id': '634c1f9bb6628cbe2861dcc2', 'avatarUrl': '/avatars/dd48dff0b639123c605b5c0ee10577d7.svg', 'isPro': False, 'fullname': 'Jangyeong.Kim', 'user': 'longshiine', 'type': 'user'}, 'name': 'Jangyeong Kim', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:48:03.141Z', 'hidden': False}, {'_id': '6789e73d7a7ce2b07045dd40', 'name': 'Seongyeong Lee', 'hidden': False}, {'_id': '6789e73d7a7ce2b07045dd41', 'name': 'Jeong A Wi', 'hidden': False}, {'_id': '6789e73d7a7ce2b07045dd42', 'name': 'Junyoung Choi', 'hidden': False}, {'_id': '6789e73d7a7ce2b07045dd43', 'name': 'Sangjun Ahn', 'hidden': False}], 'publishedAt': '2025-01-16T10:03:15.000Z', 'title': 'CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation', 'summary': 'The synthesis of high-quality 3D assets from textual or visual inputs has\\nbecome a central objective in modern generative modeling. Despite the\\nproliferation of 3D generation algorithms, they frequently grapple with\\nchallenges such as multi-view inconsistency, slow generation times, low\\nfidelity, and surface reconstruction problems. While some studies have\\naddressed some of these issues, a comprehensive solution remains elusive. In\\nthis paper, we introduce CaPa, a carve-and-paint framework that\\ngenerates high-fidelity 3D assets efficiently. CaPa employs a two-stage\\nprocess, decoupling geometry generation from texture synthesis. Initially, a 3D\\nlatent diffusion model generates geometry guided by multi-view inputs, ensuring\\nstructural consistency across perspectives. Subsequently, leveraging a novel,\\nmodel-agnostic Spatially Decoupled Attention, the framework synthesizes\\nhigh-resolution textures (up to 4K) for a given geometry. Furthermore, we\\npropose a 3D-aware occlusion inpainting algorithm that fills untextured\\nregions, resulting in cohesive results across the entire model. This pipeline\\ngenerates high-quality 3D assets in less than 30 seconds, providing\\nready-to-use outputs for commercial applications. Experimental results\\ndemonstrate that CaPa excels in both texture fidelity and geometric stability,\\nestablishing a new standard for practical, scalable 3D asset generation.', 'upvotes': 5, 'discussionId': '6789e7437a7ce2b07045df3b'}, 'publishedAt': '2025-01-17T00:14:48.500Z', 'title': 'CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09433.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5687}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09653', 'authors': [{'_id': '678a632dd03f325c5a5ad954', 'name': 'Jonathan Katzy', 'hidden': False}, {'_id': '678a632dd03f325c5a5ad955', 'name': 'Razvan Mihai Popescu', 'hidden': False}, {'_id': '678a632dd03f325c5a5ad956', 'name': 'Arie van Deursen', 'hidden': False}, {'_id': '678a632dd03f325c5a5ad957', 'user': {'_id': '63dd4b2af37111482526e0e9', 'avatarUrl': '/avatars/ffd2a07c61ea6272009df6184cb9dcef.svg', 'isPro': False, 'fullname': 'Maliheh Izadi', 'user': 'MalihehIzadi', 'type': 'user'}, 'name': 'Maliheh Izadi', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T14:25:21.390Z', 'hidden': False}], 'publishedAt': '2025-01-16T16:48:41.000Z', 'title': 'The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating\\n  Large Language Models', 'summary': 'The recent rise in the popularity of large language models has spurred the\\ndevelopment of extensive code datasets needed to train them. This has left\\nlimited code available for collection and use in the downstream investigation\\nof specific behaviors, or evaluation of large language models without suffering\\nfrom data contamination. To address this problem, we release The Heap, a large\\nmultilingual dataset covering 57 programming languages that has been\\ndeduplicated with respect to other open datasets of code, enabling researchers\\nto conduct fair evaluations of large language models without significant data\\ncleaning overhead.', 'upvotes': 3, 'discussionId': '678a632dd03f325c5a5ad997'}, 'publishedAt': '2025-01-17T09:21:33.913Z', 'title': 'The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09653.png', 'numComments': 1, 'submittedBy': {'_id': '60107b385ac3e86b3ea4fc34', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1627505688463-60107b385ac3e86b3ea4fc34.jpeg', 'fullname': 'Daniel van Strien', 'name': 'davanstrien', 'type': 'user', 'isPro': True, 'isHf': True, 'isMod': False, 'followerCount': 500}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09038', 'authors': [{'_id': '678a20d55a84f1087bd61c82', 'user': {'_id': '6475c37b04c82116f9bb2356', 'avatarUrl': '/avatars/6ec34eb3cfd091a38454ac3de72aaddc.svg', 'isPro': False, 'fullname': 'saman motamed', 'user': 'sam-motamed', 'type': 'user'}, 'name': 'Saman Motamed', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:51:43.421Z', 'hidden': False}, {'_id': '678a20d55a84f1087bd61c83', 'name': 'Laura Culp', 'hidden': False}, {'_id': '678a20d55a84f1087bd61c84', 'user': {'_id': '630e6ef664f1f8d0c771b758', 'avatarUrl': '/avatars/1df2bfadb2b6fdf8307189936efc6ef0.svg', 'isPro': False, 'fullname': 'Kevin Swersky', 'user': 'kswersky', 'type': 'user'}, 'name': 'Kevin Swersky', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:51:57.327Z', 'hidden': False}, {'_id': '678a20d55a84f1087bd61c85', 'name': 'Priyank Jaini', 'hidden': False}, {'_id': '678a20d55a84f1087bd61c86', 'user': {'_id': '673bbe0d7dfcdedd52619ec2', 'avatarUrl': '/avatars/531a44f05d0c738bbe3e028c76c2e948.svg', 'isPro': False, 'fullname': 'Robert Geirhos', 'user': 'rgeirhos', 'type': 'user'}, 'name': 'Robert Geirhos', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-17T10:52:10.657Z', 'hidden': False}], 'publishedAt': '2025-01-14T20:59:37.000Z', 'title': 'Do generative video models learn physical principles from watching\\n  videos?', 'summary': \"AI video generation is undergoing a revolution, with quality and realism\\nadvancing rapidly. These advances have led to a passionate scientific debate:\\nDo video models learn ``world models'' that discover laws of physics -- or,\\nalternatively, are they merely sophisticated pixel predictors that achieve\\nvisual realism without understanding the physical principles of reality? We\\naddress this question by developing Physics-IQ, a comprehensive benchmark\\ndataset that can only be solved by acquiring a deep understanding of various\\nphysical principles, like fluid dynamics, optics, solid mechanics, magnetism\\nand thermodynamics. We find that across a range of current models (Sora,\\nRunway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical\\nunderstanding is severely limited, and unrelated to visual realism. At the same\\ntime, some test cases can already be successfully solved. This indicates that\\nacquiring certain physical principles from observation alone may be possible,\\nbut significant challenges remain. While we expect rapid advances ahead, our\\nwork demonstrates that visual realism does not imply physical understanding.\\nOur project page is at https://physics-iq.github.io; code at\\nhttps://github.com/google-deepmind/physics-IQ-benchmark.\", 'upvotes': 1, 'discussionId': '678a20d95a84f1087bd61d65'}, 'publishedAt': '2025-01-17T04:30:36.847Z', 'title': 'Do generative video models learn physical principles from watching videos?', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/6475c37b04c82116f9bb2356/V3qzZlvvmsm_9IssX9aVF.mp4'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09038.png', 'numComments': 1, 'submittedBy': {'_id': '6475c37b04c82116f9bb2356', 'avatarUrl': '/avatars/6ec34eb3cfd091a38454ac3de72aaddc.svg', 'fullname': 'saman motamed', 'name': 'sam-motamed', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}"
]