[
    "{'paper': {'id': '2501.08313', 'authors': [{'_id': '67871e6ef492fb2235af8978', 'name': 'MiniMax', 'hidden': False}, {'_id': '67871e6ef492fb2235af8979', 'name': 'Aonian Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af897a', 'name': 'Bangwei Gong', 'hidden': False}, {'_id': '67871e6ef492fb2235af897b', 'name': 'Bo Yang', 'hidden': False}, {'_id': '67871e6ef492fb2235af897c', 'name': 'Boji Shan', 'hidden': False}, {'_id': '67871e6ef492fb2235af897d', 'name': 'Chang Liu', 'hidden': False}, {'_id': '67871e6ef492fb2235af897e', 'name': 'Cheng Zhu', 'hidden': False}, {'_id': '67871e6ef492fb2235af897f', 'user': {'_id': '642662fa22bddcea3d289f0a', 'avatarUrl': '/avatars/9b28e1325d866a24d33fdfafcaa85c4b.svg', 'isPro': False, 'fullname': 'Enoch Zhang', 'user': 'enochzhang', 'type': 'user'}, 'name': 'Chunhao Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:51.874Z', 'hidden': False}, {'_id': '67871e6ef492fb2235af8980', 'name': 'Congchao Guo', 'hidden': False}, {'_id': '67871e6ef492fb2235af8981', 'name': 'Da Chen', 'hidden': False}, {'_id': '67871e6ef492fb2235af8982', 'name': 'Dong Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af8983', 'name': 'Enwei Jiao', 'hidden': False}, {'_id': '67871e6ef492fb2235af8984', 'name': 'Gengxin Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af8985', 'name': 'Guojun Zhang', 'hidden': False}, {'_id': '67871e6ef492fb2235af8986', 'name': 'Haohai Sun', 'hidden': False}, {'_id': '67871e6ef492fb2235af8987', 'name': 'Houze Dong', 'hidden': False}, {'_id': '67871e6ef492fb2235af8988', 'name': 'Jiadai Zhu', 'hidden': False}, {'_id': '67871e6ef492fb2235af8989', 'name': 'Jiaqi Zhuang', 'hidden': False}, {'_id': '67871e6ef492fb2235af898a', 'name': 'Jiayuan Song', 'hidden': False}, {'_id': '67871e6ef492fb2235af898b', 'name': 'Jin Zhu', 'hidden': False}, {'_id': '67871e6ef492fb2235af898c', 'name': 'Jingtao Han', 'hidden': False}, {'_id': '67871e6ef492fb2235af898d', 'name': 'Jingyang Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af898e', 'name': 'Junbin Xie', 'hidden': False}, {'_id': '67871e6ef492fb2235af898f', 'name': 'Junhao Xu', 'hidden': False}, {'_id': '67871e6ef492fb2235af8990', 'name': 'Junjie Yan', 'hidden': False}, {'_id': '67871e6ef492fb2235af8991', 'name': 'Kaishun Zhang', 'hidden': False}, {'_id': '67871e6ef492fb2235af8992', 'name': 'Kecheng Xiao', 'hidden': False}, {'_id': '67871e6ef492fb2235af8993', 'name': 'Kexi Kang', 'hidden': False}, {'_id': '67871e6ef492fb2235af8994', 'name': 'Le Han', 'hidden': False}, {'_id': '67871e6ef492fb2235af8995', 'name': 'Leyang Wang', 'hidden': False}, {'_id': '67871e6ef492fb2235af8996', 'name': 'Lianfei Yu', 'hidden': False}, {'_id': '67871e6ef492fb2235af8997', 'name': 'Liheng Feng', 'hidden': False}, {'_id': '67871e6ef492fb2235af8998', 'name': 'Lin Zheng', 'hidden': False}, {'_id': '67871e6ef492fb2235af8999', 'name': 'Linbo Chai', 'hidden': False}, {'_id': '67871e6ef492fb2235af899a', 'name': 'Long Xing', 'hidden': False}, {'_id': '67871e6ef492fb2235af899b', 'name': 'Meizhi Ju', 'hidden': False}, {'_id': '67871e6ef492fb2235af899c', 'name': 'Mingyuan Chi', 'hidden': False}, {'_id': '67871e6ef492fb2235af899d', 'name': 'Mozhi Zhang', 'hidden': False}, {'_id': '67871e6ef492fb2235af899e', 'name': 'Peikai Huang', 'hidden': False}, {'_id': '67871e6ef492fb2235af899f', 'name': 'Pengcheng Niu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a0', 'name': 'Pengfei Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a1', 'name': 'Pengyu Zhao', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a2', 'name': 'Qi Yang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a3', 'name': 'Qidi Xu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a4', 'name': 'Qiexiang Wang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a5', 'name': 'Qin Wang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a6', 'name': 'Qiuhui Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a7', 'name': 'Ruitao Leng', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a8', 'name': 'Shengmin Shi', 'hidden': False}, {'_id': '67871e6ef492fb2235af89a9', 'name': 'Shuqi Yu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89aa', 'name': 'Sichen Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ab', 'name': 'Songquan Zhu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ac', 'name': 'Tao Huang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ad', 'name': 'Tianrun Liang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ae', 'name': 'Weigao Sun', 'hidden': False}, {'_id': '67871e6ef492fb2235af89af', 'name': 'Weixuan Sun', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b0', 'name': 'Weiyu Cheng', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b1', 'name': 'Wenkai Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b2', 'name': 'Xiangjun Song', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b3', 'name': 'Xiao Su', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b4', 'user': {'_id': '64638bddc615cbc12447f9f1', 'avatarUrl': '/avatars/52910d1717451ff983f745322e5850dd.svg', 'isPro': False, 'fullname': 'Xiaodong Han', 'user': 'Hannnnnxd', 'type': 'user'}, 'name': 'Xiaodong Han', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:47.808Z', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b5', 'name': 'Xinjie Zhang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b6', 'name': 'Xinzhu Hou', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b7', 'name': 'Xu Min', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b8', 'name': 'Xun Zou', 'hidden': False}, {'_id': '67871e6ef492fb2235af89b9', 'name': 'Xuyang Shen', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ba', 'name': 'Yan Gong', 'hidden': False}, {'_id': '67871e6ef492fb2235af89bb', 'name': 'Yingjie Zhu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89bc', 'name': 'Yipeng Zhou', 'hidden': False}, {'_id': '67871e6ef492fb2235af89bd', 'name': 'Yiran Zhong', 'hidden': False}, {'_id': '67871e6ef492fb2235af89be', 'name': 'Yongyi Hu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89bf', 'name': 'Yuanxiang Fan', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c0', 'name': 'Yue Yu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c1', 'name': 'Yufeng Yang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c2', 'name': 'Yuhao Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c3', 'name': 'Yunan Huang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c4', 'name': 'Yunji Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c5', 'name': 'Yunpeng Huang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c6', 'name': 'Yunzhi Xu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c7', 'name': 'Yuxin Mao', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c8', 'name': 'Zehan Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89c9', 'name': 'Zekang Li', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ca', 'name': 'Zewei Tao', 'hidden': False}, {'_id': '67871e6ef492fb2235af89cb', 'name': 'Zewen Ying', 'hidden': False}, {'_id': '67871e6ef492fb2235af89cc', 'name': 'Zhaoyang Cong', 'hidden': False}, {'_id': '67871e6ef492fb2235af89cd', 'name': 'Zhen Qin', 'hidden': False}, {'_id': '67871e6ef492fb2235af89ce', 'name': 'Zhenhua Fan', 'hidden': False}, {'_id': '67871e6ef492fb2235af89cf', 'name': 'Zhihang Yu', 'hidden': False}, {'_id': '67871e6ef492fb2235af89d0', 'name': 'Zhuo Jiang', 'hidden': False}, {'_id': '67871e6ef492fb2235af89d1', 'name': 'Zijia Wu', 'hidden': False}], 'publishedAt': '2025-01-14T18:50:05.000Z', 'title': 'MiniMax-01: Scaling Foundation Models with Lightning Attention', 'summary': 'We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01,\\nwhich are comparable to top-tier models while offering superior capabilities in\\nprocessing longer contexts. The core lies in lightning attention and its\\nefficient scaling. To maximize computational capacity, we integrate it with\\nMixture of Experts (MoE), creating a model with 32 experts and 456 billion\\ntotal parameters, of which 45.9 billion are activated for each token. We\\ndevelop an optimized parallel strategy and highly efficient\\ncomputation-communication overlap techniques for MoE and lightning attention.\\nThis approach enables us to conduct efficient training and inference on models\\nwith hundreds of billions of parameters across contexts spanning millions of\\ntokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens\\nduring training and extrapolate to 4 million tokens during inference at an\\naffordable cost. Our vision-language model, MiniMax-VL-01 is built through\\ncontinued training with 512 billion vision-language tokens. Experiments on both\\nstandard and in-house benchmarks show that our models match the performance of\\nstate-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32\\ntimes longer context window. We publicly release MiniMax-01 at\\nhttps://github.com/MiniMax-AI.', 'upvotes': 178, 'discussionId': '67871e6ff492fb2235af8a6a'}, 'publishedAt': '2025-01-14T21:48:31.318Z', 'title': 'MiniMax-01: Scaling Foundation Models with Lightning Attention', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08313.png', 'numComments': 1, 'submittedBy': {'_id': '642e4d4d6748dd4f8eeb7732', 'avatarUrl': '/avatars/fd911e9143d1a7aedd21a7d611543fcc.svg', 'fullname': 'Xuyang Shen', 'name': 'Ryan1122', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.06751', 'authors': [{'_id': '67876c0783ec692b4570e0af', 'name': 'Michael Toker', 'hidden': False}, {'_id': '67876c0783ec692b4570e0b0', 'name': 'Ido Galil', 'hidden': False}, {'_id': '67876c0783ec692b4570e0b1', 'name': 'Hadas Orgad', 'hidden': False}, {'_id': '67876c0783ec692b4570e0b2', 'name': 'Rinon Gal', 'hidden': False}, {'_id': '67876c0783ec692b4570e0b3', 'name': 'Yoad Tewel', 'hidden': False}, {'_id': '67876c0783ec692b4570e0b4', 'name': 'Gal Chechik', 'hidden': False}, {'_id': '67876c0783ec692b4570e0b5', 'name': 'Yonatan Belinkov', 'hidden': False}], 'publishedAt': '2025-01-12T08:36:38.000Z', 'title': 'Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models', 'summary': \"Text-to-image (T2I) diffusion models rely on encoded prompts to guide the\\nimage generation process. Typically, these prompts are extended to a fixed\\nlength by adding padding tokens before text encoding. Despite being a default\\npractice, the influence of padding tokens on the image generation process has\\nnot been investigated. In this work, we conduct the first in-depth analysis of\\nthe role padding tokens play in T2I models. We develop two causal techniques to\\nanalyze how information is encoded in the representation of tokens across\\ndifferent components of the T2I pipeline. Using these techniques, we\\ninvestigate when and how padding tokens impact the image generation process.\\nOur findings reveal three distinct scenarios: padding tokens may affect the\\nmodel's output during text encoding, during the diffusion process, or be\\neffectively ignored. Moreover, we identify key relationships between these\\nscenarios and the model's architecture (cross or self-attention) and its\\ntraining process (frozen or trained text encoder). These insights contribute to\\na deeper understanding of the mechanisms of padding tokens, potentially\\ninforming future model design and training practices in T2I systems.\", 'upvotes': 25, 'discussionId': '67876c0d83ec692b4570e1ec'}, 'publishedAt': '2025-01-15T03:05:10.948Z', 'title': 'Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.06751.png', 'numComments': 1, 'submittedBy': {'_id': '61c865e3d3702a3bbf50bc04', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61c865e3d3702a3bbf50bc04/eGgcyeUnOHz0hyap5vhvr.jpeg', 'fullname': 'Michael Toker', 'name': 'tokeron', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08332', 'authors': [{'_id': '678727aadd2e5dbecdf08fe3', 'name': 'Zhiheng Liu', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fe4', 'name': 'Ka Leong Cheng', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fe5', 'name': 'Xi Chen', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fe6', 'name': 'Jie Xiao', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fe7', 'name': 'Hao Ouyang', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fe8', 'name': 'Kai Zhu', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fe9', 'name': 'Yu Liu', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fea', 'name': 'Yujun Shen', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08feb', 'name': 'Qifeng Chen', 'hidden': False}, {'_id': '678727aadd2e5dbecdf08fec', 'name': 'Ping Luo', 'hidden': False}], 'publishedAt': '2025-01-14T18:59:55.000Z', 'title': 'MangaNinja: Line Art Colorization with Precise Reference Following', 'summary': 'Derived from diffusion models, MangaNinjia specializes in the task of\\nreference-guided line art colorization. We incorporate two thoughtful designs\\nto ensure precise character detail transcription, including a patch shuffling\\nmodule to facilitate correspondence learning between the reference color image\\nand the target line art, and a point-driven control scheme to enable\\nfine-grained color matching. Experiments on a self-collected benchmark\\ndemonstrate the superiority of our model over current solutions in terms of\\nprecise colorization. We further showcase the potential of the proposed\\ninteractive point control in handling challenging cases, cross-character\\ncolorization, multi-reference harmonization, beyond the reach of existing\\nalgorithms.', 'upvotes': 25, 'discussionId': '678727acdd2e5dbecdf09097'}, 'publishedAt': '2025-01-14T22:13:07.257Z', 'title': 'MangaNinja: Line Art Colorization with Precise Reference Following', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08332.png', 'numComments': 1, 'submittedBy': {'_id': '6479925ab77e18dbf640bd67', 'avatarUrl': '/avatars/bb52ecd22ca4b49157f8668be35409e7.svg', 'fullname': 'Zhiheng Liu', 'name': 'Johanan0528', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 6}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08187', 'authors': [{'_id': '67871cb6ce7f3eb12692b222', 'user': {'_id': '63d660ae44f1d8fbe585d463', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674993743489-noauth.jpeg', 'isPro': False, 'fullname': 'Yin Fang', 'user': 'Fangyinfff', 'type': 'user'}, 'name': 'Yin Fang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:58.033Z', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b223', 'name': 'Xinle Deng', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b224', 'name': 'Kangwei Liu', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b225', 'user': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'isPro': False, 'fullname': 'Ningyu Zhang', 'user': 'Ningyu', 'type': 'user'}, 'name': 'Ningyu Zhang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:54.355Z', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b226', 'name': 'Jingyang Qian', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b227', 'name': 'Penghui Yang', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b228', 'name': 'Xiaohui Fan', 'hidden': False}, {'_id': '67871cb6ce7f3eb12692b229', 'name': 'Huajun Chen', 'hidden': False}], 'publishedAt': '2025-01-14T15:12:19.000Z', 'title': 'A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\\n  Following', 'summary': 'Large language models excel at interpreting complex natural language\\ninstructions, enabling them to perform a wide range of tasks. In the life\\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\\nof cellular biology\", capturing intricate gene expression patterns at the\\nsingle-cell level. However, interacting with this \"language\" through\\nconventional tools is often inefficient and unintuitive, posing challenges for\\nresearchers. To address these limitations, we present InstructCell, a\\nmulti-modal AI copilot that leverages natural language as a medium for more\\ndirect and flexible single-cell analysis. We construct a comprehensive\\nmulti-modal instruction dataset that pairs text-based instructions with\\nscRNA-seq profiles from diverse tissues and species. Building on this, we\\ndevelop a multi-modal cell language architecture capable of simultaneously\\ninterpreting and processing both modalities. InstructCell empowers researchers\\nto accomplish critical tasks-such as cell type annotation, conditional\\npseudo-cell generation, and drug sensitivity prediction-using straightforward\\nnatural language commands. Extensive evaluations demonstrate that InstructCell\\nconsistently meets or exceeds the performance of existing single-cell\\nfoundation models, while adapting to diverse experimental conditions. More\\nimportantly, InstructCell provides an accessible and intuitive tool for\\nexploring complex single-cell data, lowering technical barriers and enabling\\ndeeper biological insights.', 'upvotes': 17, 'discussionId': '67871cbcce7f3eb12692b37e'}, 'publishedAt': '2025-01-14T21:58:03.573Z', 'title': 'A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/Y3Br4s8MMP0E14TrwYF7E.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08187.png', 'numComments': 1, 'submittedBy': {'_id': '620b3bbb0668e435407c8d0a', 'avatarUrl': '/avatars/e0fccbb2577d76088e09f054c35cffbc.svg', 'fullname': 'Ningyu Zhang', 'name': 'Ningyu', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 15}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.08316', 'authors': [{'_id': '678725b38c1e7b6c4a69f88a', 'user': {'_id': '645863f7dc18eb1a9b5d29df', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645863f7dc18eb1a9b5d29df/t49Nnyl4tbkUn7CmQqKZh.jpeg', 'isPro': False, 'fullname': 'Peter Lin', 'user': 'PeterL1n', 'type': 'user'}, 'name': 'Shanchuan Lin', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:34.462Z', 'hidden': False}, {'_id': '678725b38c1e7b6c4a69f88b', 'name': 'Xin Xia', 'hidden': False}, {'_id': '678725b38c1e7b6c4a69f88c', 'name': 'Yuxi Ren', 'hidden': False}, {'_id': '678725b38c1e7b6c4a69f88d', 'name': 'Ceyuan Yang', 'hidden': False}, {'_id': '678725b38c1e7b6c4a69f88e', 'name': 'Xuefeng Xiao', 'hidden': False}, {'_id': '678725b38c1e7b6c4a69f88f', 'name': 'Lu Jiang', 'hidden': False}], 'publishedAt': '2025-01-14T18:51:48.000Z', 'title': 'Diffusion Adversarial Post-Training for One-Step Video Generation', 'summary': 'The diffusion models are widely used for image and video generation, but\\ntheir iterative generation process is slow and expansive. While existing\\ndistillation approaches have demonstrated the potential for one-step generation\\nin the image domain, they still suffer from significant quality degradation. In\\nthis work, we propose Adversarial Post-Training (APT) against real data\\nfollowing diffusion pre-training for one-step video generation. To improve the\\ntraining stability and quality, we introduce several improvements to the model\\narchitecture and training procedures, along with an approximated R1\\nregularization objective. Empirically, our experiments show that our\\nadversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,\\n24fps videos in real time using a single forward evaluation step. Additionally,\\nour model is capable of generating 1024px images in a single step, achieving\\nquality comparable to state-of-the-art methods.', 'upvotes': 16, 'discussionId': '678725b68c1e7b6c4a69f911'}, 'publishedAt': '2025-01-14T22:04:35.473Z', 'title': 'Diffusion Adversarial Post-Training for One-Step Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08316.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5661}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08225', 'authors': [{'_id': '67872d3659aa2a284c52ad5f', 'name': 'Yabo Zhang', 'hidden': False}, {'_id': '67872d3659aa2a284c52ad60', 'name': 'Xinpeng Zhou', 'hidden': False}, {'_id': '67872d3659aa2a284c52ad61', 'name': 'Yihan Zeng', 'hidden': False}, {'_id': '67872d3659aa2a284c52ad62', 'name': 'Hang Xu', 'hidden': False}, {'_id': '67872d3659aa2a284c52ad63', 'name': 'Hui Li', 'hidden': False}, {'_id': '67872d3659aa2a284c52ad64', 'name': 'Wangmeng Zuo', 'hidden': False}], 'publishedAt': '2025-01-14T16:09:16.000Z', 'title': 'FramePainter: Endowing Interactive Image Editing with Video Diffusion\\n  Priors', 'summary': 'Interactive image editing allows users to modify images through visual\\ninteraction operations such as drawing, clicking, and dragging. Existing\\nmethods construct such supervision signals from videos, as they capture how\\nobjects change with various physical interactions. However, these models are\\nusually built upon text-to-image diffusion models, so necessitate (i) massive\\ntraining samples and (ii) an additional reference encoder to learn real-world\\ndynamics and visual consistency. In this paper, we reformulate this task as an\\nimage-to-video generation problem, so that inherit powerful video diffusion\\npriors to reduce training costs and ensure temporal consistency. Specifically,\\nwe introduce FramePainter as an efficient instantiation of this formulation.\\nInitialized with Stable Video Diffusion, it only uses a lightweight sparse\\ncontrol encoder to inject editing signals. Considering the limitations of\\ntemporal attention in handling large motion between two frames, we further\\npropose matching attention to enlarge the receptive field while encouraging\\ndense correspondence between edited and source image tokens. We highlight the\\neffectiveness and efficiency of FramePainter across various of editing signals:\\nit domainantly outperforms previous state-of-the-art methods with far less\\ntraining data, achieving highly seamless and coherent editing of images, \\\\eg,\\nautomatically adjust the reflection of the cup. Moreover, FramePainter also\\nexhibits exceptional generalization in scenarios not present in real-world\\nvideos, \\\\eg, transform the clownfish into shark-like shape. Our code will be\\navailable at https://github.com/YBYBZhang/FramePainter.', 'upvotes': 10, 'discussionId': '67872d3a59aa2a284c52aed0'}, 'publishedAt': '2025-01-14T22:36:58.221Z', 'title': 'FramePainter: Endowing Interactive Image Editing with Video Diffusion Priors', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08225.png', 'numComments': 1, 'submittedBy': {'_id': '62cd752ba9be5c19555c2b4c', 'avatarUrl': '/avatars/ed72684f0f2139516ccde24cd467cea6.svg', 'fullname': 'YaboZhang', 'name': 'Yabo', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 4}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08328', 'authors': [{'_id': '6787367e42691fc7dc545292', 'name': 'Richard Zhuang', 'hidden': False}, {'_id': '6787367e42691fc7dc545293', 'user': {'_id': '64e8f4a24f3f7b0b84834315', 'avatarUrl': '/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg', 'isPro': False, 'fullname': 'Akshat Gupta', 'user': 'akshat57', 'type': 'user'}, 'name': 'Akshat Gupta', 'status': 'extracted_confirmed', 'statusLastChangedAt': '2025-01-15T04:19:21.989Z', 'hidden': False}, {'_id': '6787367e42691fc7dc545294', 'name': 'Richard Yang', 'hidden': False}, {'_id': '6787367e42691fc7dc545295', 'name': 'Aniket Rahane', 'hidden': False}, {'_id': '6787367e42691fc7dc545296', 'name': 'Zhengyu Li', 'hidden': False}, {'_id': '6787367e42691fc7dc545297', 'name': 'Gopala Anumanchipalli', 'hidden': False}], 'publishedAt': '2025-01-14T18:59:03.000Z', 'title': 'PokerBench: Training Large Language Models to become Professional Poker\\n  Players', 'summary': 'We introduce PokerBench - a benchmark for evaluating the poker-playing\\nabilities of large language models (LLMs). As LLMs excel in traditional NLP\\ntasks, their application to complex, strategic games like poker poses a new\\nchallenge. Poker, an incomplete information game, demands a multitude of skills\\nsuch as mathematics, reasoning, planning, strategy, and a deep understanding of\\ngame theory and human psychology. This makes Poker the ideal next frontier for\\nlarge language models. PokerBench consists of a comprehensive compilation of\\n11,000 most important scenarios, split between pre-flop and post-flop play,\\ndeveloped in collaboration with trained poker players. We evaluate prominent\\nmodels including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models,\\nfinding that all state-of-the-art LLMs underperform in playing optimal poker.\\nHowever, after fine-tuning, these models show marked improvements. We validate\\nPokerBench by having models with different scores compete with each other,\\ndemonstrating that higher scores on PokerBench lead to higher win rates in\\nactual poker games. Through gameplay between our fine-tuned model and GPT-4, we\\nalso identify limitations of simple supervised fine-tuning for learning optimal\\nplaying strategy, suggesting the need for more advanced methodologies for\\neffectively training language models to excel in games. PokerBench thus\\npresents a unique benchmark for a quick and reliable evaluation of the\\npoker-playing ability of LLMs as well as a comprehensive benchmark to study the\\nprogress of LLMs in complex game-playing scenarios. The dataset and code will\\nbe made available at: https://github.com/pokerllm/pokerbench.', 'upvotes': 8, 'discussionId': '6787368042691fc7dc5452dd'}, 'publishedAt': '2025-01-14T23:17:00.915Z', 'title': 'PokerBench: Training Large Language Models to become Professional Poker Players', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08328.png', 'numComments': 1, 'submittedBy': {'_id': '64e8f4a24f3f7b0b84834315', 'avatarUrl': '/avatars/242bb68c7ccffe5061c2d1c229ea3b0b.svg', 'fullname': 'Akshat Gupta', 'name': 'akshat57', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.07730', 'authors': [{'_id': '678734168c1e7b6c4a6e5ff9', 'name': 'Dongwon Kim', 'hidden': False}, {'_id': '678734168c1e7b6c4a6e5ffa', 'user': {'_id': '661c9059bcd78151e5c06ea1', 'avatarUrl': '/avatars/308745629823adedc6dfcade934ae143.svg', 'isPro': False, 'fullname': 'Ju He', 'user': 'turkeyju', 'type': 'user'}, 'name': 'Ju He', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:25.249Z', 'hidden': False}, {'_id': '678734168c1e7b6c4a6e5ffb', 'name': 'Qihang Yu', 'hidden': False}, {'_id': '678734168c1e7b6c4a6e5ffc', 'name': 'Chenglin Yang', 'hidden': False}, {'_id': '678734168c1e7b6c4a6e5ffd', 'name': 'Xiaohui Shen', 'hidden': False}, {'_id': '678734168c1e7b6c4a6e5ffe', 'name': 'Suha Kwak', 'hidden': False}, {'_id': '678734168c1e7b6c4a6e5fff', 'name': 'Liang-Chieh Chen', 'hidden': False}], 'publishedAt': '2025-01-13T22:37:17.000Z', 'title': 'Democratizing Text-to-Image Masked Generative Models with Compact\\n  Text-Aware One-Dimensional Tokens', 'summary': 'Image tokenizers form the foundation of modern text-to-image generative\\nmodels but are notoriously difficult to train. Furthermore, most existing\\ntext-to-image models rely on large-scale, high-quality private datasets, making\\nthem challenging to replicate. In this work, we introduce Text-Aware\\nTransformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful\\nimage tokenizer that can utilize either discrete or continuous 1-dimensional\\ntokens. TA-TiTok uniquely integrates textual information during the tokenizer\\ndecoding stage (i.e., de-tokenization), accelerating convergence and enhancing\\nperformance. TA-TiTok also benefits from a simplified, yet effective, one-stage\\ntraining process, eliminating the need for the complex two-stage distillation\\nused in previous 1-dimensional tokenizers. This design allows for seamless\\nscalability to large datasets. Building on this, we introduce a family of\\ntext-to-image Masked Generative Models (MaskGen), trained exclusively on open\\ndata while achieving comparable performance to models trained on private data.\\nWe aim to release both the efficient, strong TA-TiTok tokenizers and the\\nopen-data, open-weight MaskGen models to promote broader access and democratize\\nthe field of text-to-image masked generative models.', 'upvotes': 6, 'discussionId': '678734178c1e7b6c4a6e6071'}, 'publishedAt': '2025-01-14T23:11:25.137Z', 'title': 'Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/661c9059bcd78151e5c06ea1/-7KLvUVPYbrrljt6Nx6iS.png', 'https://cdn-uploads.huggingface.co/production/uploads/661c9059bcd78151e5c06ea1/SH9x-3yrSYBXH9Nkn9uNy.png', 'https://cdn-uploads.huggingface.co/production/uploads/661c9059bcd78151e5c06ea1/ITR8a3J2vHuYGmCWfX-xS.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07730.png', 'numComments': 1, 'submittedBy': {'_id': '661c9059bcd78151e5c06ea1', 'avatarUrl': '/avatars/308745629823adedc6dfcade934ae143.svg', 'fullname': 'Ju He', 'name': 'turkeyju', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.05131', 'authors': [{'_id': '678739a51f65db189f9e75c5', 'name': 'Dewei Zhou', 'hidden': False}, {'_id': '678739a51f65db189f9e75c6', 'user': {'_id': '64e99fc07e2ec711a7138262', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e99fc07e2ec711a7138262/FmP3F8_UXgh9K-0gwS99A.jpeg', 'isPro': False, 'fullname': '谢集', 'user': 'sanaka87', 'type': 'user'}, 'name': 'Ji Xie', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:48:22.289Z', 'hidden': False}, {'_id': '678739a51f65db189f9e75c7', 'name': 'Zongxin Yang', 'hidden': False}, {'_id': '678739a51f65db189f9e75c8', 'name': 'Yi Yang', 'hidden': False}], 'publishedAt': '2025-01-09T10:34:00.000Z', 'title': '3DIS-FLUX: simple and efficient multi-instance generation with DiT\\n  rendering', 'summary': \"The growing demand for controllable outputs in text-to-image generation has\\ndriven significant advancements in multi-instance generation (MIG), enabling\\nusers to define both instance layouts and attributes. Currently, the\\nstate-of-the-art methods in MIG are primarily adapter-based. However, these\\nmethods necessitate retraining a new adapter each time a more advanced model is\\nreleased, resulting in significant resource consumption. A methodology named\\nDepth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which\\ndecouples MIG into two distinct phases: 1) depth-based scene construction and\\n2) detail rendering with widely pre-trained depth control models. The 3DIS\\nmethod requires adapter training solely during the scene construction phase,\\nwhile enabling various models to perform training-free detail rendering.\\nInitially, 3DIS focused on rendering techniques utilizing U-Net architectures\\nsuch as SD1.5, SD2, and SDXL, without exploring the potential of recent\\nDiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension\\nof the 3DIS framework that integrates the FLUX model for enhanced rendering\\ncapabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map\\ncontrolled image generation and introduce a detail renderer that manipulates\\nthe Attention Mask in FLUX's Joint Attention mechanism based on layout\\ninformation. This approach allows for the precise rendering of fine-grained\\nattributes of each instance. Our experimental results indicate that 3DIS-FLUX,\\nleveraging the FLUX model, outperforms the original 3DIS method, which utilized\\nSD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in\\nterms of both performance and image quality. Project Page:\\nhttps://limuloo.github.io/3DIS/.\", 'upvotes': 5, 'discussionId': '678739a71f65db189f9e7629'}, 'publishedAt': '2025-01-15T09:50:03.046Z', 'title': '3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/64e99fc07e2ec711a7138262/qBkh68ldWX3LoBNZYW6NC.png', 'https://cdn-uploads.huggingface.co/production/uploads/64e99fc07e2ec711a7138262/-pwsPgiyVrqOVZlI4leB7.png', 'https://cdn-uploads.huggingface.co/production/uploads/64e99fc07e2ec711a7138262/d-54oe5r7u4dWtQ6_Swow.png'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.05131.png', 'numComments': 1, 'submittedBy': {'_id': '64e99fc07e2ec711a7138262', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e99fc07e2ec711a7138262/FmP3F8_UXgh9K-0gwS99A.jpeg', 'fullname': '谢集', 'name': 'sanaka87', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.08326', 'authors': [{'_id': '6787772516c02260b4e6592b', 'name': 'Miran Heo', 'hidden': False}, {'_id': '6787772516c02260b4e6592c', 'name': 'Min-Hung Chen', 'hidden': False}, {'_id': '6787772516c02260b4e6592d', 'name': 'De-An Huang', 'hidden': False}, {'_id': '6787772516c02260b4e6592e', 'name': 'Sifei Liu', 'hidden': False}, {'_id': '6787772516c02260b4e6592f', 'name': 'Subhashree Radhakrishnan', 'hidden': False}, {'_id': '6787772516c02260b4e65930', 'name': 'Seon Joo Kim', 'hidden': False}, {'_id': '6787772516c02260b4e65931', 'name': 'Yu-Chiang Frank Wang', 'hidden': False}, {'_id': '6787772516c02260b4e65932', 'name': 'Ryo Hachiuma', 'hidden': False}], 'publishedAt': '2025-01-14T18:58:04.000Z', 'title': 'Omni-RGPT: Unifying Image and Video Region-level Understanding via Token\\n  Marks', 'summary': 'We present Omni-RGPT, a multimodal large language model designed to\\nfacilitate region-level comprehension for both images and videos. To achieve\\nconsistent region representation across spatio-temporal dimensions, we\\nintroduce Token Mark, a set of tokens highlighting the target regions within\\nthe visual feature space. These tokens are directly embedded into spatial\\nregions using region prompts (e.g., boxes or masks) and simultaneously\\nincorporated into the text prompt to specify the target, establishing a direct\\nconnection between visual and text tokens. To further support robust video\\nunderstanding without requiring tracklets, we introduce an auxiliary task that\\nguides Token Mark by leveraging the consistency of the tokens, enabling stable\\nregion interpretation across the video. Additionally, we introduce a\\nlarge-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT\\nachieves state-of-the-art results on image and video-based commonsense\\nreasoning benchmarks while showing strong performance in captioning and\\nreferring expression comprehension tasks.', 'upvotes': 5, 'discussionId': '6787772816c02260b4e65a22'}, 'publishedAt': '2025-01-15T03:52:43.037Z', 'title': 'Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08326.png', 'numComments': 1, 'submittedBy': {'_id': '64ae22dd1aee69ece065cdcd', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png', 'fullname': 'Min-Hung Chen', 'name': 'cmhungsteve', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 3}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08319', 'authors': [{'_id': '67876067c377690c01ab4478', 'name': 'Yoav Gur-Arieh', 'hidden': False}, {'_id': '67876067c377690c01ab4479', 'name': 'Roy Mayan', 'hidden': False}, {'_id': '67876067c377690c01ab447a', 'name': 'Chen Agassy', 'hidden': False}, {'_id': '67876067c377690c01ab447b', 'user': {'_id': '627b2d0527dc4650b62eef42', 'avatarUrl': '/avatars/e70381850f5657b54e90f5539f3d74eb.svg', 'isPro': False, 'fullname': 'Atticus Geiger', 'user': 'atticusg', 'type': 'user'}, 'name': 'Atticus Geiger', 'status': 'extracted_pending', 'statusLastChangedAt': '2025-01-15T07:14:48.875Z', 'hidden': False}, {'_id': '67876067c377690c01ab447c', 'name': 'Mor Geva', 'hidden': False}], 'publishedAt': '2025-01-14T18:53:00.000Z', 'title': 'Enhancing Automated Interpretability with Output-Centric Feature\\n  Descriptions', 'summary': 'Automated interpretability pipelines generate natural language descriptions\\nfor the concepts represented by features in large language models (LLMs), such\\nas plants or the first word in a sentence. These descriptions are derived using\\ninputs that activate the feature, which may be a dimension or a direction in\\nthe model\\'s representation space. However, identifying activating inputs is\\ncostly, and the mechanistic role of a feature in model behavior is determined\\nboth by how inputs cause a feature to activate and by how feature activation\\naffects outputs. Using steering evaluations, we reveal that current pipelines\\nprovide descriptions that fail to capture the causal effect of the feature on\\noutputs. To fix this, we propose efficient, output-centric methods for\\nautomatically generating feature descriptions. These methods use the tokens\\nweighted higher after feature stimulation or the highest weight tokens after\\napplying the vocabulary \"unembedding\" head directly to the feature. Our\\noutput-centric descriptions better capture the causal effect of a feature on\\nmodel outputs than input-centric descriptions, but combining the two leads to\\nthe best performance on both input and output evaluations. Lastly, we show that\\noutput-centric descriptions can be used to find inputs that activate features\\npreviously thought to be \"dead\".', 'upvotes': 4, 'discussionId': '67876068c377690c01ab44cb'}, 'publishedAt': '2025-01-15T02:16:22.721Z', 'title': 'Enhancing Automated Interpretability with Output-Centric Feature Descriptions', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08319.png', 'numComments': 1, 'submittedBy': {'_id': '5e7749883d77a72421292d07', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1670231290373-5e7749883d77a72421292d07.jpeg', 'fullname': 'Gabriele Sarti', 'name': 'gsarti', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 209}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08197', 'authors': [{'_id': '67873d8945c53fa98281bb59', 'user': {'_id': '6374c494958cd71fa7ea0a9d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png', 'isPro': False, 'fullname': 'yuyijiong', 'user': 'yuyijiong', 'type': 'user'}, 'name': 'Yijiong Yu', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:47:14.751Z', 'hidden': False}, {'_id': '67873d8945c53fa98281bb5a', 'name': 'Ziyun Dai', 'hidden': False}, {'_id': '67873d8945c53fa98281bb5b', 'name': 'Zekun Wang', 'hidden': False}, {'_id': '67873d8945c53fa98281bb5c', 'name': 'Wei Wang', 'hidden': False}, {'_id': '67873d8945c53fa98281bb5d', 'name': 'Ran Chen', 'hidden': False}, {'_id': '67873d8945c53fa98281bb5e', 'name': 'Ji Pei', 'hidden': False}], 'publishedAt': '2025-01-14T15:22:47.000Z', 'title': 'OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for\\n  LLM Training', 'summary': 'Large language models (LLMs) have demonstrated remarkable capabilities, but\\ntheir success heavily relies on the quality of pretraining corpora. For Chinese\\nLLMs, the scarcity of high-quality Chinese datasets presents a significant\\nchallenge, often limiting their performance. To address this issue, we propose\\nthe OpenCSG Chinese Corpus, a series of high-quality datasets specifically\\ndesigned for LLM pretraining, post-training, and fine-tuning. This corpus\\nincludes Fineweb-edu-chinese, Fineweb-edu-chinese-v2, Cosmopedia-chinese, and\\nSmoltalk-chinese, each with distinct characteristics: Fineweb-edu datasets\\nfocus on filtered, high-quality content derived from diverse Chinese web\\nsources; Cosmopedia-chinese provides synthetic, textbook-style data for\\nknowledge-intensive training; and Smoltalk-chinese emphasizes stylistic and\\ndiverse chat-format data. The OpenCSG Chinese Corpus is characterized by its\\nhigh-quality text, diverse coverage across domains, and scalable, reproducible\\ndata curation processes. Additionally, we conducted extensive experimental\\nanalyses, including evaluations on smaller parameter models, which demonstrated\\nsignificant performance improvements in tasks such as C-Eval, showcasing the\\neffectiveness of the corpus for training Chinese LLMs.', 'upvotes': 4, 'discussionId': '67873d8a45c53fa98281bba1'}, 'publishedAt': '2025-01-15T00:57:59.401Z', 'title': 'OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08197.png', 'numComments': 1, 'submittedBy': {'_id': '6374c494958cd71fa7ea0a9d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6374c494958cd71fa7ea0a9d/2YCKv6tXCZXtsIOFIIXjs.png', 'fullname': 'yuyijiong', 'name': 'yuyijiong', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 41}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.07888', 'authors': [{'_id': '67873cdd1f65db189f9f64d7', 'name': 'Liping Yuan', 'hidden': False}, {'_id': '67873cdd1f65db189f9f64d8', 'name': 'Jiawei Wang', 'hidden': False}, {'_id': '67873cdd1f65db189f9f64d9', 'name': 'Haomiao Sun', 'hidden': False}, {'_id': '67873cdd1f65db189f9f64da', 'name': 'Yuchen Zhang', 'hidden': False}, {'_id': '67873cdd1f65db189f9f64db', 'name': 'Yuan Lin', 'hidden': False}], 'publishedAt': '2025-01-14T06:54:39.000Z', 'title': 'Tarsier2: Advancing Large Vision-Language Models from Detailed Video\\n  Description to Comprehensive Video Understanding', 'summary': 'We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)\\ndesigned for generating detailed and accurate video descriptions, while also\\nexhibiting superior general video understanding capabilities. Tarsier2 achieves\\nsignificant advancements through three key upgrades: (1) Scaling pre-training\\ndata from 11M to 40M video-text pairs, enriching both volume and diversity; (2)\\nPerforming fine-grained temporal alignment during supervised fine-tuning; (3)\\nUsing model-based sampling to automatically construct preference data and\\napplying DPO training for optimization. Extensive experiments show that\\nTarsier2-7B consistently outperforms leading proprietary models, including\\nGPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K\\nbenchmark, Tarsier2-7B improves F1 by 2.8\\\\% over GPT-4o and 5.8\\\\% over\\nGemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\\\\%\\nperformance advantage over GPT-4o and +24.9\\\\% over Gemini-1.5-Pro. Tarsier2-7B\\nalso sets new state-of-the-art results across 15 public benchmarks, spanning\\ntasks such as video question-answering, video grounding, hallucination test,\\nand embodied question-answering, demonstrating its versatility as a robust\\ngeneralist vision-language model.', 'upvotes': 4, 'discussionId': '67873ce11f65db189f9f6615'}, 'publishedAt': '2025-01-14T23:43:33.935Z', 'title': 'Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07888.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5661}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08292', 'authors': [{'_id': '678731bbe24feaaa753768b5', 'name': 'Abhilasha Ravichander', 'hidden': False}, {'_id': '678731bbe24feaaa753768b6', 'name': 'Shrusti Ghela', 'hidden': False}, {'_id': '678731bbe24feaaa753768b7', 'name': 'David Wadden', 'hidden': False}, {'_id': '678731bbe24feaaa753768b8', 'name': 'Yejin Choi', 'hidden': False}], 'publishedAt': '2025-01-14T18:13:08.000Z', 'title': 'HALoGEN: Fantastic LLM Hallucinations and Where to Find Them', 'summary': 'Despite their impressive ability to generate high-quality and fluent text,\\ngenerative large language models (LLMs) also produce hallucinations: statements\\nthat are misaligned with established world knowledge or provided input context.\\nHowever, measuring hallucination can be challenging, as having humans verify\\nmodel generations on-the-fly is both expensive and time-consuming. In this\\nwork, we release HALoGEN, a comprehensive hallucination benchmark consisting\\nof: (1) 10,923 prompts for generative models spanning nine domains including\\nprogramming, scientific attribution, and summarization, and (2) automatic\\nhigh-precision verifiers for each use case that decompose LLM generations into\\natomic units, and verify each unit against a high-quality knowledge source. We\\nuse this framework to evaluate ~150,000 generations from 14 language models,\\nfinding that even the best-performing models are riddled with hallucinations\\n(sometimes up to 86% of generated atomic facts depending on the domain). We\\nfurther define a novel error classification for LLM hallucinations based on\\nwhether they likely stem from incorrect recollection of training data (Type A\\nerrors), or incorrect knowledge in training data (Type B errors), or are\\nfabrication (Type C errors). We hope our framework provides a foundation to\\nenable the principled study of why generative models hallucinate, and advances\\nthe development of trustworthy large language models.', 'upvotes': 4, 'discussionId': '678731bde24feaaa7537695b'}, 'publishedAt': '2025-01-14T23:01:53.467Z', 'title': 'HALoGEN: Fantastic LLM Hallucinations and Where to Find Them', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08292.png', 'numComments': 1, 'submittedBy': {'_id': '645dbaa6f5760d1530d7580d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg', 'fullname': 'Simeon Emanuilov', 'name': 's-emanuilov', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08167', 'authors': [{'_id': '678748ee95b9364769858656', 'name': 'Rewina Bedemariam', 'hidden': False}, {'_id': '678748ee95b9364769858657', 'name': 'Natalie Perez', 'hidden': False}, {'_id': '678748ee95b9364769858658', 'name': 'Sreyoshi Bhaduri', 'hidden': False}, {'_id': '678748ee95b9364769858659', 'name': 'Satya Kapoor', 'hidden': False}, {'_id': '678748ee95b936476985865a', 'name': 'Alex Gil', 'hidden': False}, {'_id': '678748ee95b936476985865b', 'name': 'Elizabeth Conjar', 'hidden': False}, {'_id': '678748ee95b936476985865c', 'name': 'Ikkei Itoku', 'hidden': False}, {'_id': '678748ee95b936476985865d', 'name': 'David Theil', 'hidden': False}, {'_id': '678748ee95b936476985865e', 'user': {'_id': '63a4754927f1f64ed7238dac', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg', 'isPro': False, 'fullname': 'Aman Chadha', 'user': 'amanchadha', 'type': 'user'}, 'name': 'Aman Chadha', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-15T08:47:10.299Z', 'hidden': False}, {'_id': '678748ee95b936476985865f', 'name': 'Naumaan Nayyar', 'hidden': False}], 'publishedAt': '2025-01-14T14:49:14.000Z', 'title': 'Potential and Perils of Large Language Models as Judges of Unstructured\\n  Textual Data', 'summary': \"Rapid advancements in large language models have unlocked remarkable\\ncapabilities when it comes to processing and summarizing unstructured text\\ndata. This has implications for the analysis of rich, open-ended datasets, such\\nas survey responses, where LLMs hold the promise of efficiently distilling key\\nthemes and sentiments. However, as organizations increasingly turn to these\\npowerful AI systems to make sense of textual feedback, a critical question\\narises, can we trust LLMs to accurately represent the perspectives contained\\nwithin these text based datasets? While LLMs excel at generating human-like\\nsummaries, there is a risk that their outputs may inadvertently diverge from\\nthe true substance of the original responses. Discrepancies between the\\nLLM-generated outputs and the actual themes present in the data could lead to\\nflawed decision-making, with far-reaching consequences for organizations. This\\nresearch investigates the effectiveness of LLMs as judge models to evaluate the\\nthematic alignment of summaries generated by other LLMs. We utilized an\\nAnthropic Claude model to generate thematic summaries from open-ended survey\\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\\nLLM judges. The LLM-as-judge approach was compared to human evaluations using\\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\\nalternative to traditional human centric evaluation methods. Our findings\\nreveal that while LLMs as judges offer a scalable solution comparable to human\\nraters, humans may still excel at detecting subtle, context-specific nuances.\\nThis research contributes to the growing body of knowledge on AI assisted text\\nanalysis. We discuss limitations and provide recommendations for future\\nresearch, emphasizing the need for careful consideration when generalizing LLM\\njudge models across various contexts and use cases.\", 'upvotes': 3, 'discussionId': '678748ee95b9364769858684'}, 'publishedAt': '2025-01-15T00:40:26.270Z', 'title': 'Potential and Perils of Large Language Models as Judges of Unstructured Textual Data', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08167.png', 'numComments': 1, 'submittedBy': {'_id': '63a4754927f1f64ed7238dac', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg', 'fullname': 'Aman Chadha', 'name': 'amanchadha', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 1}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.08284', 'authors': [{'_id': '67875e554d9e0e1baf29ab55', 'name': 'Shamsuddeen Hassan Muhammad', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab56', 'name': 'Idris Abdulmumin', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab57', 'name': 'Abinew Ali Ayele', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab58', 'name': 'David Ifeoluwa Adelani', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab59', 'name': 'Ibrahim Said Ahmad', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab5a', 'name': 'Saminu Mohammad Aliyu', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab5b', 'name': 'Nelson Odhiambo Onyango', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab5c', 'name': 'Lilian D. A. Wanzare', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab5d', 'name': 'Samuel Rutunda', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab5e', 'name': 'Lukman Jibril Aliyu', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab5f', 'name': 'Esubalew Alemneh', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab60', 'name': 'Oumaima Hourrane', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab61', 'name': 'Hagos Tesfahun Gebremichael', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab62', 'name': 'Elyas Abdi Ismail', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab63', 'name': 'Meriem Beloucif', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab64', 'name': 'Ebrahim Chekol Jibril', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab65', 'name': 'Andiswa Bukula', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab66', 'name': 'Rooweither Mabuya', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab67', 'name': 'Salomey Osei', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab68', 'name': 'Abigail Oppong', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab69', 'name': 'Tadesse Destaw Belay', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab6a', 'name': 'Tadesse Kebede Guge', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab6b', 'name': 'Tesfa Tegegne Asfaw', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab6c', 'name': 'Chiamaka Ijeoma Chukwuneke', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab6d', 'name': 'Paul Röttger', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab6e', 'name': 'Seid Muhie Yimam', 'hidden': False}, {'_id': '67875e554d9e0e1baf29ab6f', 'name': 'Nedjma Ousidhoum', 'hidden': False}], 'publishedAt': '2025-01-14T18:00:07.000Z', 'title': 'AfriHate: A Multilingual Collection of Hate Speech and Abusive Language\\n  Datasets for African Languages', 'summary': 'Hate speech and abusive language are global phenomena that need\\nsocio-cultural background knowledge to be understood, identified, and\\nmoderated. However, in many regions of the Global South, there have been\\nseveral documented occurrences of (1) absence of moderation and (2) censorship\\ndue to the reliance on keyword spotting out of context. Further, high-profile\\nindividuals have frequently been at the center of the moderation process, while\\nlarge and targeted hate speech campaigns against minorities have been\\noverlooked. These limitations are mainly due to the lack of high-quality data\\nin the local languages and the failure to include local communities in the\\ncollection, annotation, and moderation processes. To address this issue, we\\npresent AfriHate: a multilingual collection of hate speech and abusive language\\ndatasets in 15 African languages. Each instance in AfriHate is annotated by\\nnative speakers familiar with the local culture. We report the challenges\\nrelated to the construction of the datasets and present various classification\\nbaseline results with and without using LLMs. The datasets, individual\\nannotations, and hate speech and offensive language lexicons are available on\\nhttps://github.com/AfriHate/AfriHate', 'upvotes': 1, 'discussionId': '67875e574d9e0e1baf29abcc'}, 'publishedAt': '2025-01-15T02:06:29.459Z', 'title': 'AfriHate: A Multilingual Collection of Hate Speech and Abusive Language Datasets for African Languages', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08284.png', 'numComments': 1, 'submittedBy': {'_id': '5e6a3d4ea9afd5125d9ec064', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg', 'fullname': 'Stefan Schweter', 'name': 'stefan-it', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 2020}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08120', 'authors': [{'_id': '6787b437bbb0287a08f3b32d', 'name': 'Markus J. Buehler', 'hidden': False}], 'publishedAt': '2025-01-14T13:52:41.000Z', 'title': 'In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR', 'summary': \"The pursuit of automated scientific discovery has fueled progress from\\nsymbolic logic to modern AI, forging new frontiers in reasoning and pattern\\nrecognition. Transformers function as potential systems, where every possible\\nrelationship remains latent potentiality until tasks impose constraints, akin\\nto measurement. Yet, refining their sampling requires more than probabilistic\\nselection: solutions must conform to specific structures or rules, ensuring\\nconsistency and the invocation of general principles. We present\\nGraph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for\\nExploratory Optimization of Reasoning), a framework that combines graph\\nreasoning with symbolic abstraction to dynamically expand domain knowledge.\\nInspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a\\nstructured mapping, where tasks yield knowledge graphs, abstract patterns, and\\nultimately, final answers. Inspired by category theory, it encodes concepts as\\nnodes and their relationships as edges, supporting hierarchical inference and\\nadaptive learning through isomorphic representations. Demonstrations include\\nhypothesis generation, materials design, and creative reasoning, such as\\ndiscovering relationships between mythological concepts like 'thin places' with\\nmaterials science. We propose a 'knowledge garden growth' strategy that\\nintegrates insights across domains, promoting interdisciplinary connections.\\nResults with a 3-billion-parameter Graph-PReFLexOR model show superior\\nreasoning depth and adaptability, underscoring the potential for transparent,\\nmultidisciplinary AI-driven discovery. It lays the groundwork for general\\nautonomous reasoning solutions.\", 'upvotes': 0, 'discussionId': '6787b438bbb0287a08f3b370'}, 'publishedAt': '2025-01-15T08:27:24.456Z', 'title': 'In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08120.png', 'numComments': 1, 'submittedBy': {'_id': '623ce1c6b66fedf374859fe7', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623ce1c6b66fedf374859fe7/lhbMLg6BxLCb9DD4rgjfx.jpeg', 'fullname': 'Markus Buehler', 'name': 'mjbuehler', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 22}, 'isAuthorParticipating': False}"
]