[
    "{'paper': {'id': '2501.08828', 'authors': [{'_id': '67889537383254ec3f017a1d', 'user': {'_id': '66337cb5bd8ef15a47e72ce0', 'avatarUrl': '/avatars/cc49056fcdc6bdabfe72a0d3de5c196d.svg', 'isPro': False, 'fullname': 'DONG KUICAI', 'user': 'daviddongdong', 'type': 'user'}, 'name': 'Kuicai Dong', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-16T09:41:42.249Z', 'hidden': False}, {'_id': '67889537383254ec3f017a1e', 'name': 'Yujing Chang', 'hidden': False}, {'_id': '67889537383254ec3f017a1f', 'name': 'Xin Deik Goh', 'hidden': False}, {'_id': '67889537383254ec3f017a20', 'name': 'Dexun Li', 'hidden': False}, {'_id': '67889537383254ec3f017a21', 'name': 'Ruiming Tang', 'hidden': False}, {'_id': '67889537383254ec3f017a22', 'name': 'Yong Liu', 'hidden': False}], 'publishedAt': '2025-01-15T14:30:13.000Z', 'title': 'MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents', 'summary': 'Multi-modal document retrieval is designed to identify and retrieve various\\nforms of multi-modal content, such as figures, tables, charts, and layout\\ninformation from extensive documents. Despite its significance, there is a\\nnotable lack of a robust benchmark to effectively evaluate the performance of\\nsystems in multi-modal document retrieval. To address this gap, this work\\nintroduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:\\npage-level and layout-level retrieval. The former focuses on localizing the\\nmost relevant pages within a long document, while the latter targets the\\ndetection of specific layouts, offering a more fine-grained granularity than\\nwhole-page analysis. A layout can refer to a variety of elements such as\\ntextual paragraphs, equations, figures, tables, or charts. The MMDocIR\\nbenchmark comprises a rich dataset featuring expertly annotated labels for\\n1,685 questions and bootstrapped labels for 173,843 questions, making it a\\npivotal resource for advancing multi-modal document retrieval for both training\\nand evaluation. Through rigorous experiments, we reveal that (i) visual\\nretrievers significantly outperform their text counterparts, (ii) MMDocIR train\\nset can effectively benefit the training process of multi-modal document\\nretrieval and (iii) text retrievers leveraging on VLM-text perform much better\\nthan those using OCR-text. These findings underscores the potential advantages\\nof integrating visual elements for multi-modal document retrieval.', 'upvotes': 13, 'discussionId': '67889539383254ec3f017a72'}, 'publishedAt': '2025-01-16T00:15:13.323Z', 'title': 'MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08828.png', 'numComments': 1, 'submittedBy': {'_id': '645dbaa6f5760d1530d7580d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg', 'fullname': 'Simeon Emanuilov', 'name': 's-emanuilov', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08983', 'authors': [{'_id': '678897a4d42825b51c19d65a', 'user': {'_id': '63f47b5321eb234ab739e91a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg', 'isPro': False, 'fullname': 'Haozhe Xie', 'user': 'hzxie', 'type': 'user'}, 'name': 'Haozhe Xie', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:50:10.101Z', 'hidden': False}, {'_id': '678897a4d42825b51c19d65b', 'user': {'_id': '62fc8cf7ee999004b5a8b982', 'avatarUrl': '/avatars/6c5dda9e58747054a989f077a078f3dc.svg', 'isPro': False, 'fullname': 'Zhaoxi Chen', 'user': 'FrozenBurning', 'type': 'user'}, 'name': 'Zhaoxi Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:50:16.000Z', 'hidden': False}, {'_id': '678897a4d42825b51c19d65c', 'user': {'_id': '623c530013a63ea865f96c8e', 'avatarUrl': '/avatars/164455a1a94f92b71733fc778c21bd89.svg', 'isPro': False, 'fullname': 'Fangzhou Hong', 'user': 'hongfz16', 'type': 'user'}, 'name': 'Fangzhou Hong', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:51:11.083Z', 'hidden': False}, {'_id': '678897a4d42825b51c19d65d', 'user': {'_id': '62ab1ac1d48b4d8b048a3473', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png', 'isPro': False, 'fullname': 'Ziwei Liu', 'user': 'liuziwei7', 'type': 'user'}, 'name': 'Ziwei Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:50:34.647Z', 'hidden': False}], 'publishedAt': '2025-01-15T17:59:56.000Z', 'title': 'CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities', 'summary': '3D scene generation has garnered growing attention in recent years and has\\nmade significant progress. Generating 4D cities is more challenging than 3D\\nscenes due to the presence of structurally complex, visually diverse objects\\nlike buildings and vehicles, and heightened human sensitivity to distortions in\\nurban environments. To tackle these issues, we propose CityDreamer4D, a\\ncompositional generative model specifically tailored for generating unbounded\\n4D cities. Our main insights are 1) 4D city generation should separate dynamic\\nobjects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)\\nall objects in the 4D scene should be composed of different types of neural\\nfields for buildings, vehicles, and background stuff. Specifically, we propose\\nTraffic Scenario Generator and Unbounded Layout Generator to produce dynamic\\ntraffic scenarios and static city layouts using a highly compact BEV\\nrepresentation. Objects in 4D cities are generated by combining stuff-oriented\\nand instance-oriented neural fields for background stuff, buildings, and\\nvehicles. To suit the distinct characteristics of background stuff and\\ninstances, the neural fields employ customized generative hash grids and\\nperiodic positional embeddings as scene parameterizations. Furthermore, we\\noffer a comprehensive suite of datasets for city generation, including OSM,\\nGoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world\\ncity layouts, while the Google Earth and CityTopia datasets deliver\\nlarge-scale, high-quality city imagery complete with 3D instance annotations.\\nLeveraging its compositional design, CityDreamer4D supports a range of\\ndownstream applications, such as instance editing, city stylization, and urban\\nsimulation, while delivering state-of-the-art performance in generating\\nrealistic 4D cities.', 'upvotes': 9, 'discussionId': '678897a7d42825b51c19d702'}, 'publishedAt': '2025-01-16T00:24:15.441Z', 'title': 'CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities', 'mediaUrls': ['https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/oA3_MgZyDEpWX4ITsGSng.webp'], 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08983.png', 'numComments': 1, 'submittedBy': {'_id': '63f47b5321eb234ab739e91a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg', 'fullname': 'Haozhe Xie', 'name': 'hzxie', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 11}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.08365', 'authors': [{'_id': '6788d4566cc82aa3a079f632', 'user': {'_id': '645954bafbf75ae1c71fb8aa', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645954bafbf75ae1c71fb8aa/twyFXx2-M8SruwLwRBV1W.jpeg', 'isPro': False, 'fullname': 'Stefan Baack', 'user': 'stefan-baack', 'type': 'user'}, 'name': 'Stefan Baack', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:43:49.638Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f633', 'user': {'_id': '60347d3660e3dd96631c9093', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60347d3660e3dd96631c9093/B3fuZer5N04tZIAYrLnz4.jpeg', 'isPro': False, 'fullname': 'Stella Biderman', 'user': 'stellaathena', 'type': 'user'}, 'name': 'Stella Biderman', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:43:56.119Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f634', 'name': 'Kasia Odrozek', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f635', 'user': {'_id': '63c5dfc8d5a5cd2043e6f03c', 'avatarUrl': '/avatars/edcfcd9cfb03286d670e6c5743efef6a.svg', 'isPro': False, 'fullname': 'Aviya Skowron', 'user': 'avi-skowron', 'type': 'user'}, 'name': 'Aviya Skowron', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:44:07.420Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f636', 'user': {'_id': '66fe985ff3ba4a0ed6d2bc89', 'avatarUrl': '/avatars/7ded4065561a6bd571fa94a27f328c18.svg', 'isPro': False, 'fullname': 'ayah bdeir', 'user': 'ayahbdeir', 'type': 'user'}, 'name': 'Ayah Bdeir', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:44:13.401Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f637', 'name': 'Jillian Bommarito', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f638', 'user': {'_id': '62a0da842e30aaf94ebaaa12', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1679073278459-62a0da842e30aaf94ebaaa12.jpeg', 'isPro': False, 'fullname': 'Jennifer Ding', 'user': 'jending12', 'type': 'user'}, 'name': 'Jennifer Ding', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:44:22.871Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f639', 'name': 'Maximilian Gahntz', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f63a', 'user': {'_id': '63692f631e9d04886d555da6', 'avatarUrl': '/avatars/13035d88679a570c20c74b7325d89542.svg', 'isPro': False, 'fullname': 'Paul Keller', 'user': 'paulkeller', 'type': 'user'}, 'name': 'Paul Keller', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:44:50.535Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f63b', 'user': {'_id': '64ce091a9e9ca8123d7a42b0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64ce091a9e9ca8123d7a42b0/OEPggp82RwigxNLL35LgT.jpeg', 'isPro': False, 'fullname': 'Pierre-Carl Langlais', 'user': 'Pclanglais', 'type': 'user'}, 'name': 'Pierre-Carl Langlais', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:44:59.405Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f63c', 'user': {'_id': '656fbeae7734a829bbd16252', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656fbeae7734a829bbd16252/s0_NEIevmFM3dncq0gHPn.jpeg', 'isPro': False, 'fullname': 'Greg Lindahl', 'user': 'greglindahl', 'type': 'user'}, 'name': 'Greg Lindahl', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:45:10.564Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f63d', 'user': {'_id': '636071759ddc44e710e0f5ce', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/636071759ddc44e710e0f5ce/-gmEhY5PidmSXIQPi2-QB.jpeg', 'isPro': True, 'fullname': 'Sebastian Majstorovic', 'user': 'storytracer', 'type': 'user'}, 'name': 'Sebastian Majstorovic', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:45:17.228Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f63e', 'name': 'Nik Marda', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f63f', 'user': {'_id': '62596f9e1c0a084224b93e00', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62596f9e1c0a084224b93e00/X2aLkJ0ofhkXwAg7lXvxD.jpeg', 'isPro': False, 'fullname': 'Guilherme Penedo', 'user': 'guipenedo', 'type': 'user'}, 'name': 'Guilherme Penedo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:45:26.371Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f640', 'name': 'Maarten Van Segbroeck', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f641', 'name': 'Jennifer Wang', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f642', 'user': {'_id': '5e48005437cb5b49818287a5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png', 'isPro': False, 'fullname': 'Leandro von Werra', 'user': 'lvwerra', 'type': 'user'}, 'name': 'Leandro von Werra', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:46:10.239Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f643', 'user': {'_id': '63741e742b908db633716c80', 'avatarUrl': '/avatars/97fa9158afab29053e47ce3067714bee.svg', 'isPro': False, 'fullname': 'Mitchell Baker', 'user': 'HOOisDead', 'type': 'user'}, 'name': 'Mitchell Baker', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:46:17.572Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f644', 'name': 'Julie Belião', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f645', 'name': 'Kasia Chmielinski', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f646', 'user': {'_id': '6441042d5d600fb0951a5f99', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6441042d5d600fb0951a5f99/4CbOaYcEz99BtVAQvnGTn.jpeg', 'isPro': False, 'fullname': 'Marzieh Fadaee', 'user': 'MarziehFadaee', 'type': 'user'}, 'name': 'Marzieh Fadaee', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:46:34.586Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f647', 'name': 'Lisa Gutermuth', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f648', 'user': {'_id': '626ede24d2fa9e7d598c8709', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/626ede24d2fa9e7d598c8709/JKS8-Y2Jw87EgNQZBRswq.jpeg', 'isPro': False, 'fullname': 'Hynek Kydlicek', 'user': 'hynky', 'type': 'user'}, 'name': 'Hynek Kydlíček', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:46:45.941Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f649', 'user': {'_id': '623b6a04ae0ec315881b9c97', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623b6a04ae0ec315881b9c97/IprOmck5cUmwKB6yoAU4L.jpeg', 'isPro': False, 'fullname': 'Greg Leppert', 'user': 'leppert', 'type': 'user'}, 'name': 'Greg Leppert', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:46:51.942Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f64a', 'name': 'EM Lewis-Jong', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f64b', 'name': 'Solana Larsen', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f64c', 'user': {'_id': '61f4283a81c4d30f58140242', 'avatarUrl': '/avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg', 'isPro': False, 'fullname': 'Shayne Longpre', 'user': 'Shayne', 'type': 'user'}, 'name': 'Shayne Longpre', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:47:15.078Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f64d', 'name': 'Angela Oduor Lungati', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f64e', 'user': {'_id': '6571bd30e82edf86f269fac0', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6571bd30e82edf86f269fac0/310j0bc6evOUY6dmtiBq5.jpeg', 'isPro': False, 'fullname': 'Cullen Miller', 'user': 'cullenmiller', 'type': 'user'}, 'name': 'Cullen Miller', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:47:27.114Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f64f', 'user': {'_id': '638a93f1ed88cf97afd53e42', 'avatarUrl': '/avatars/147ed42f13847b5e4d534511ef5388a3.svg', 'isPro': False, 'fullname': 'Victor Miller', 'user': 'victormiller', 'type': 'user'}, 'name': 'Victor Miller', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:47:46.377Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f650', 'user': {'_id': '607d59fb921db717010c7ccc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1625736058289-607d59fb921db717010c7ccc.png', 'isPro': False, 'fullname': 'Max Ryabinin', 'user': 'mryab', 'type': 'user'}, 'name': 'Max Ryabinin', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:47:53.155Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f651', 'name': 'Kathleen Siminyu', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f652', 'name': 'Andrew Strait', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f653', 'name': 'Mark Surman', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f654', 'name': 'Anna Tumadóttir', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f655', 'user': {'_id': '6329ee3dab49d487dd1439ec', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6329ee3dab49d487dd1439ec/vxGvdBK0XMZaCpc5dGOIa.jpeg', 'isPro': False, 'fullname': 'Maurice Weber', 'user': 'mauriceweber', 'type': 'user'}, 'name': 'Maurice Weber', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:48:17.851Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f656', 'user': {'_id': '6430688e0d7e3248d0616a64', 'avatarUrl': '/avatars/6d3ff97af3dd0da6f4781523e8cb2778.svg', 'isPro': False, 'fullname': 'Rebecca Weiss', 'user': 'rjweiss', 'type': 'user'}, 'name': 'Rebecca Weiss', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:48:24.368Z', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f657', 'name': 'Lee White', 'hidden': False}, {'_id': '6788d4566cc82aa3a079f658', 'user': {'_id': '5df7e9e5da6d0311fd3d53f9', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg', 'isPro': True, 'fullname': 'Thomas Wolf', 'user': 'thomwolf', 'type': 'user'}, 'name': 'Thomas Wolf', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:43:36.026Z', 'hidden': False}], 'publishedAt': '2025-01-14T17:18:05.000Z', 'title': 'Towards Best Practices for Open Datasets for LLM Training', 'summary': 'Many AI companies are training their large language models (LLMs) on data\\nwithout the permission of the copyright owners. The permissibility of doing so\\nvaries by jurisdiction: in countries like the EU and Japan, this is allowed\\nunder certain restrictions, while in the United States, the legal landscape is\\nmore ambiguous. Regardless of the legal status, concerns from creative\\nproducers have led to several high-profile copyright lawsuits, and the threat\\nof litigation is commonly cited as a reason for the recent trend towards\\nminimizing the information shared about training datasets by both corporate and\\npublic interest actors. This trend in limiting data information causes harm by\\nhindering transparency, accountability, and innovation in the broader ecosystem\\nby denying researchers, auditors, and impacted individuals access to the\\ninformation needed to understand AI models.\\n  While this could be mitigated by training language models on open access and\\npublic domain data, at the time of writing, there are no such models (trained\\nat a meaningful scale) due to the substantial technical and sociological\\nchallenges in assembling the necessary corpus. These challenges include\\nincomplete and unreliable metadata, the cost and complexity of digitizing\\nphysical records, and the diverse set of legal and technical skills required to\\nensure relevance and responsibility in a quickly changing landscape. Building\\ntowards a future where AI systems can be trained on openly licensed data that\\nis responsibly curated and governed requires collaboration across legal,\\ntechnical, and policy domains, along with investments in metadata standards,\\ndigitization, and fostering a culture of openness.', 'upvotes': 8, 'discussionId': '6788d4566cc82aa3a079f68d'}, 'publishedAt': '2025-01-16T04:42:23.941Z', 'title': 'Towards Best Practices for Open Datasets for LLM Training', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08365.png', 'numComments': 1, 'submittedBy': {'_id': '5e6a3d4ea9afd5125d9ec064', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg', 'fullname': 'Stefan Schweter', 'name': 'stefan-it', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 2028}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08994', 'authors': [{'_id': '6788945e2b5050a9154d939d', 'user': {'_id': '635f8ed47c05eb9f59963d3a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg', 'isPro': False, 'fullname': 'ChenyangSi', 'user': 'ChenyangSi', 'type': 'user'}, 'name': 'Chenyang Si', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:49:14.615Z', 'hidden': False}, {'_id': '6788945e2b5050a9154d939e', 'user': {'_id': '6481764e8af4675862efb22e', 'avatarUrl': '/avatars/fc2e076bc861693f598a528a068a696e.svg', 'isPro': False, 'fullname': 'weichenfan', 'user': 'weepiess2383', 'type': 'user'}, 'name': 'Weichen Fan', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:49:38.866Z', 'hidden': False}, {'_id': '6788945e2b5050a9154d939f', 'user': {'_id': '645aff5121ab438e732c47c1', 'avatarUrl': '/avatars/23b2a853139b0f2ae1fa88e2bd4e0056.svg', 'isPro': False, 'fullname': 'Zhengyao Lv', 'user': 'cszy98', 'type': 'user'}, 'name': 'Zhengyao Lv', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:49:45.309Z', 'hidden': False}, {'_id': '6788945e2b5050a9154d93a0', 'user': {'_id': '60efe7fa0d920bc7805cada5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png', 'isPro': False, 'fullname': 'Ziqi Huang', 'user': 'Ziqi', 'type': 'user'}, 'name': 'Ziqi Huang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:49:51.123Z', 'hidden': False}, {'_id': '6788945e2b5050a9154d93a1', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '6788945e2b5050a9154d93a2', 'user': {'_id': '62ab1ac1d48b4d8b048a3473', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png', 'isPro': False, 'fullname': 'Ziwei Liu', 'user': 'liuziwei7', 'type': 'user'}, 'name': 'Ziwei Liu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:49:58.339Z', 'hidden': False}], 'publishedAt': '2025-01-15T18:20:37.000Z', 'title': 'RepVideo: Rethinking Cross-Layer Representation for Video Generation', 'summary': 'Video generation has achieved remarkable progress with the introduction of\\ndiffusion models, which have significantly improved the quality of generated\\nvideos. However, recent research has primarily focused on scaling up model\\ntraining, while offering limited insights into the direct impact of\\nrepresentations on the video generation process. In this paper, we initially\\ninvestigate the characteristics of features in intermediate layers, finding\\nsubstantial variations in attention maps across different layers. These\\nvariations lead to unstable semantic representations and contribute to\\ncumulative differences between features, which ultimately reduce the similarity\\nbetween adjacent frames and negatively affect temporal coherence. To address\\nthis, we propose RepVideo, an enhanced representation framework for\\ntext-to-video diffusion models. By accumulating features from neighboring\\nlayers to form enriched representations, this approach captures more stable\\nsemantic information. These enhanced representations are then used as inputs to\\nthe attention mechanism, thereby improving semantic expressiveness while\\nensuring feature consistency across adjacent frames. Extensive experiments\\ndemonstrate that our RepVideo not only significantly enhances the ability to\\ngenerate accurate spatial appearances, such as capturing complex spatial\\nrelationships between multiple objects, but also improves temporal consistency\\nin video generation.', 'upvotes': 8, 'discussionId': '678894602b5050a9154d945b'}, 'publishedAt': '2025-01-16T00:09:01.580Z', 'title': 'RepVideo: Rethinking Cross-Layer Representation for Video Generation', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08994.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5677}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09012', 'authors': [{'_id': '6788a30ee9e04d1c80fb1d6d', 'user': {'_id': '6351382f40dffad651ef3fbd', 'avatarUrl': '/avatars/3ac2de7c49086bb37cc4f4bd29ed72f2.svg', 'isPro': False, 'fullname': 'JIANG', 'user': 'Ruixiang', 'type': 'user'}, 'name': 'Ruixiang Jiang', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:54:16.366Z', 'hidden': False}, {'_id': '6788a30ee9e04d1c80fb1d6e', 'user': {'_id': '64f95c12ec913b4f977ba028', 'avatarUrl': '/avatars/b128850ee2a3667d0b1972659cd5f0ae.svg', 'isPro': False, 'fullname': 'Chang Wen Cheng', 'user': 'Vincentchang', 'type': 'user'}, 'name': 'Changwen Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:54:22.754Z', 'hidden': False}], 'publishedAt': '2025-01-15T18:56:22.000Z', 'title': 'Multimodal LLMs Can Reason about Aesthetics in Zero-Shot', 'summary': \"We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability\\nshall be elicited to evaluate the aesthetics of artworks. To facilitate this\\ninvestigation, we construct MM-StyleBench, a novel high-quality dataset for\\nbenchmarking artistic stylization. We then develop a principled method for\\nhuman preference modeling and perform a systematic correlation analysis between\\nMLLMs' responses and human preference. Our experiments reveal an inherent\\nhallucination issue of MLLMs in art evaluation, associated with response\\nsubjectivity. ArtCoT is proposed, demonstrating that art-specific task\\ndecomposition and the use of concrete language boost MLLMs' reasoning ability\\nfor aesthetics. Our findings offer valuable insights into MLLMs for art and can\\nbenefit a wide range of downstream applications, such as style transfer and\\nartistic image generation. Code available at\\nhttps://github.com/songrise/MLLM4Art.\", 'upvotes': 4, 'discussionId': '6788a30fe9e04d1c80fb1dc5'}, 'publishedAt': '2025-01-16T01:14:37.187Z', 'title': 'Multimodal LLMs Can Reason about Aesthetics in Zero-Shot', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09012.png', 'numComments': 1, 'submittedBy': {'_id': '645dbaa6f5760d1530d7580d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg', 'fullname': 'Simeon Emanuilov', 'name': 's-emanuilov', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.07783', 'authors': [{'_id': '6788c7156bc665d65caf8b3c', 'user': {'_id': '665d4b515fdfe8f923e347a7', 'avatarUrl': '/avatars/d114b24c02dadfca0a8aee104755a8ec.svg', 'isPro': False, 'fullname': 'Zhaokai Wang', 'user': 'wzk1015', 'type': 'user'}, 'name': 'Zhaokai Wang', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-16T08:49:11.408Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b3d', 'user': {'_id': '64ae2359179421d320b1694b', 'avatarUrl': '/avatars/c387a75191005bcaa473091de5383a10.svg', 'isPro': False, 'fullname': 'Xizhou Zhu', 'user': 'Einsiedler', 'type': 'user'}, 'name': 'Xizhou Zhu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:51:26.043Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b3e', 'name': 'Xue Yang', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b3f', 'user': {'_id': '650aac7c23196fb2d86a0b37', 'avatarUrl': '/avatars/418035a2e8f514118bc67d16ee41b6b0.svg', 'isPro': False, 'fullname': 'Gen Luo', 'user': 'favor123', 'type': 'user'}, 'name': 'Gen Luo', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:51:36.685Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b40', 'name': 'Hao Li', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b41', 'user': {'_id': '64b7475efa7eabaae5f7ba94', 'avatarUrl': '/avatars/346e53b345ccd9e8557ab8d2ec17a8f3.svg', 'isPro': False, 'fullname': 'Changyao Tian', 'user': 'Changyao', 'type': 'user'}, 'name': 'Changyao Tian', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:51:46.333Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b42', 'user': {'_id': '66efe658de163a536aa84178', 'avatarUrl': '/avatars/fddc42450cabf41ca1ab2f70b185f51c.svg', 'isPro': False, 'fullname': 'dou wenhan', 'user': 'douwh', 'type': 'user'}, 'name': 'Wenhan Dou', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:51:57.729Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b43', 'user': {'_id': '6695ee745e5e72a434fdfbc2', 'avatarUrl': '/avatars/01c0af2dac291ed4f52615e07094ea93.svg', 'isPro': False, 'fullname': 'Junqi Ge', 'user': 'gejq16148', 'type': 'user'}, 'name': 'Junqi Ge', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:52:04.125Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b44', 'user': {'_id': '65ead3ea908526a39082e641', 'avatarUrl': '/avatars/dcf870695fd56b06ca03d82f831e9019.svg', 'isPro': False, 'fullname': 'Lewei Lu', 'user': 'luotto', 'type': 'user'}, 'name': 'Lewei Lu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:52:10.641Z', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b45', 'name': 'Yu Qiao', 'hidden': False}, {'_id': '6788c7156bc665d65caf8b46', 'user': {'_id': '64686f7172d9180d4ac8b4e4', 'avatarUrl': '/avatars/db67dd6c4b2b41054ddcce5a18ade6f8.svg', 'isPro': False, 'fullname': 'Jifeng Dai', 'user': 'daijifeng', 'type': 'user'}, 'name': 'Jifeng Dai', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:52:18.008Z', 'hidden': False}], 'publishedAt': '2025-01-14T01:57:41.000Z', 'title': 'Parameter-Inverted Image Pyramid Networks for Visual Perception and\\n  Multimodal Understanding', 'summary': 'Image pyramids are widely adopted in top-performing methods to obtain\\nmulti-scale features for precise visual perception and understanding. However,\\ncurrent image pyramids use the same large-scale model to process multiple\\nresolutions of images, leading to significant computational cost. To address\\nthis challenge, we propose a novel network architecture, called\\nParameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses\\npretrained models (ViTs or CNNs) as branches to process multi-scale images,\\nwhere images of higher resolutions are processed by smaller network branches to\\nbalance computational cost and performance. To integrate information from\\ndifferent spatial scales, we further propose a novel cross-branch feature\\ninteraction mechanism. To validate PIIP, we apply it to various perception\\nmodels and a representative multimodal large language model called LLaVA, and\\nconduct extensive experiments on various tasks such as object detection,\\nsegmentation, image classification and multimodal understanding. PIIP achieves\\nsuperior performance compared to single-branch and existing multi-resolution\\napproaches with lower computational cost. When applied to InternViT-6B, a\\nlarge-scale vision foundation model, PIIP can improve its performance by 1%-2%\\non detection and segmentation with only 40%-60% of the original computation,\\nfinally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For\\nmultimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and\\n74.5% on MMBench with only 2.8M training data. Our code is released at\\nhttps://github.com/OpenGVLab/PIIP.', 'upvotes': 3, 'discussionId': '6788c71a6bc665d65caf8c91'}, 'publishedAt': '2025-01-16T03:47:27.618Z', 'title': 'Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.07783.png', 'numComments': 1, 'submittedBy': {'_id': '665d4b515fdfe8f923e347a7', 'avatarUrl': '/avatars/d114b24c02dadfca0a8aee104755a8ec.svg', 'fullname': 'Zhaokai Wang', 'name': 'wzk1015', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False, 'followerCount': 2}, 'isAuthorParticipating': True}",
    "{'paper': {'id': '2501.08809', 'authors': [{'_id': '678893719b735715ac69debe', 'name': 'Sida Tian', 'hidden': False}, {'_id': '678893719b735715ac69debf', 'name': 'Can Zhang', 'hidden': False}, {'_id': '678893719b735715ac69dec0', 'name': 'Wei Yuan', 'hidden': False}, {'_id': '678893719b735715ac69dec1', 'name': 'Wei Tan', 'hidden': False}, {'_id': '678893719b735715ac69dec2', 'name': 'Wenjie Zhu', 'hidden': False}], 'publishedAt': '2025-01-15T14:08:44.000Z', 'title': 'XMusic: Towards a Generalized and Controllable Symbolic Music Generation\\n  Framework', 'summary': 'In recent years, remarkable advancements in artificial intelligence-generated\\ncontent (AIGC) have been achieved in the fields of image synthesis and text\\ngeneration, generating content comparable to that produced by humans. However,\\nthe quality of AI-generated music has not yet reached this standard, primarily\\ndue to the challenge of effectively controlling musical emotions and ensuring\\nhigh-quality outputs. This paper presents a generalized symbolic music\\ngeneration framework, XMusic, which supports flexible prompts (i.e., images,\\nvideos, texts, tags, and humming) to generate emotionally controllable and\\nhigh-quality symbolic music. XMusic consists of two core components, XProjector\\nand XComposer. XProjector parses the prompts of various modalities into\\nsymbolic music elements (i.e., emotions, genres, rhythms and notes) within the\\nprojection space to generate matching music. XComposer contains a Generator and\\na Selector. The Generator generates emotionally controllable and melodious\\nmusic based on our innovative symbolic music representation, whereas the\\nSelector identifies high-quality symbolic music by constructing a multi-task\\nlearning scheme involving quality assessment, emotion recognition, and genre\\nrecognition tasks. In addition, we build XMIDI, a large-scale symbolic music\\ndataset that contains 108,023 MIDI files annotated with precise emotion and\\ngenre labels. Objective and subjective evaluations show that XMusic\\nsignificantly outperforms the current state-of-the-art methods with impressive\\nmusic quality. Our XMusic has been awarded as one of the nine Highlights of\\nCollectibles at WAIC 2023. The project homepage of XMusic is\\nhttps://xmusic-project.github.io.', 'upvotes': 3, 'discussionId': '678893729b735715ac69deed'}, 'publishedAt': '2025-01-16T00:05:01.556Z', 'title': 'XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08809.png', 'numComments': 1, 'submittedBy': {'_id': '60f1abe7544c2adfd699860c', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg', 'fullname': 'AK', 'name': 'akhaliq', 'type': 'user', 'isPro': False, 'isHf': True, 'isMod': False, 'followerCount': 5677}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.08970', 'authors': [{'_id': '6788d316f2e4691811b58fd0', 'name': 'Ilia Shumailov', 'hidden': False}, {'_id': '6788d316f2e4691811b58fd1', 'user': {'_id': '643c627626f177a3e41912e9', 'avatarUrl': '/avatars/472bbef2630458773cb76bf0d44f0028.svg', 'isPro': False, 'fullname': 'Daniel Ramagem', 'user': 'danrama', 'type': 'user'}, 'name': 'Daniel Ramage', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:41:48.984Z', 'hidden': False}, {'_id': '6788d316f2e4691811b58fd2', 'name': 'Sarah Meiklejohn', 'hidden': False}, {'_id': '6788d316f2e4691811b58fd3', 'name': 'Peter Kairouz', 'hidden': False}, {'_id': '6788d316f2e4691811b58fd4', 'user': {'_id': '6728eb13efafaef60cff09a5', 'avatarUrl': '/avatars/2effe4ae7f0d0707accfcc308a11c3a3.svg', 'isPro': False, 'fullname': 'Florian Hartmann', 'user': 'fhartmann', 'type': 'user'}, 'name': 'Florian Hartmann', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:42:04.838Z', 'hidden': False}, {'_id': '6788d316f2e4691811b58fd5', 'user': {'_id': '668298c155ecd182e4b78afb', 'avatarUrl': '/avatars/f08100666c609ae1ef5fc50504feacf4.svg', 'isPro': False, 'fullname': 'Borja Balle', 'user': 'bballe', 'type': 'user'}, 'name': 'Borja Balle', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T09:42:21.854Z', 'hidden': False}, {'_id': '6788d316f2e4691811b58fd6', 'name': 'Eugene Bagdasarian', 'hidden': False}], 'publishedAt': '2025-01-15T17:28:53.000Z', 'title': 'Trusted Machine Learning Models Unlock Private Inference for Problems\\n  Currently Infeasible with Cryptography', 'summary': 'We often interact with untrusted parties. Prioritization of privacy can limit\\nthe effectiveness of these interactions, as achieving certain goals\\nnecessitates sharing private data. Traditionally, addressing this challenge has\\ninvolved either seeking trusted intermediaries or constructing cryptographic\\nprotocols that restrict how much data is revealed, such as multi-party\\ncomputations or zero-knowledge proofs. While significant advances have been\\nmade in scaling cryptographic approaches, they remain limited in terms of the\\nsize and complexity of applications they can be used for. In this paper, we\\nargue that capable machine learning models can fulfill the role of a trusted\\nthird party, thus enabling secure computations for applications that were\\npreviously infeasible. In particular, we describe Trusted Capable Model\\nEnvironments (TCMEs) as an alternative approach for scaling secure computation,\\nwhere capable machine learning model(s) interact under input/output\\nconstraints, with explicit information flow control and explicit statelessness.\\nThis approach aims to achieve a balance between privacy and computational\\nefficiency, enabling private inference where classical cryptographic solutions\\nare currently infeasible. We describe a number of use cases that are enabled by\\nTCME, and show that even some simple classic cryptographic problems can already\\nbe solved with TCME. Finally, we outline current limitations and discuss the\\npath forward in implementing them.', 'upvotes': 2, 'discussionId': '6788d31df2e4691811b591f6'}, 'publishedAt': '2025-01-16T04:37:06.686Z', 'title': 'Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.08970.png', 'numComments': 1, 'submittedBy': {'_id': '6475c2794766357252e69e9f', 'avatarUrl': '/avatars/db428715dfd2239df2aeaaff1282323f.svg', 'fullname': 'i', 'name': 'iliashum', 'type': 'user', 'isPro': False, 'isHf': False, 'isMod': False}, 'isAuthorParticipating': False}",
    "{'paper': {'id': '2501.09019', 'authors': [{'_id': '678893e40a465aa0613dc9b2', 'user': {'_id': '66a6838120c5e7be4e9d46d9', 'avatarUrl': '/avatars/8aebcbd93d54cee64969dd7c2f7e4df2.svg', 'isPro': False, 'fullname': 'Jingyuan Chen', 'user': 'jchensteve', 'type': 'user'}, 'name': 'Jingyuan Chen', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:54:34.580Z', 'hidden': False}, {'_id': '678893e40a465aa0613dc9b3', 'user': {'_id': '6449f2dfeb7db8f70fb990f8', 'avatarUrl': '/avatars/72c02e754bb35dab05c4a5f1e69c95f1.svg', 'isPro': False, 'fullname': 'Fuchen', 'user': 'FireCRT', 'type': 'user'}, 'name': 'Fuchen Long', 'status': 'claimed_verified', 'statusLastChangedAt': '2025-01-16T09:41:40.611Z', 'hidden': False}, {'_id': '678893e40a465aa0613dc9b4', 'name': 'Jie An', 'hidden': False}, {'_id': '678893e40a465aa0613dc9b5', 'user': {'_id': '64ac099294ae6b609d0f0713', 'avatarUrl': '/avatars/7f50e3e1597fb5fd0dad9fe7fd2e26ec.svg', 'isPro': False, 'fullname': 'Zhaofan Qiu', 'user': 'qiudavy', 'type': 'user'}, 'name': 'Zhaofan Qiu', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:54:51.062Z', 'hidden': False}, {'_id': '678893e40a465aa0613dc9b6', 'name': 'Ting Yao', 'hidden': False}, {'_id': '678893e40a465aa0613dc9b7', 'name': 'Jiebo Luo', 'hidden': False}, {'_id': '678893e40a465aa0613dc9b8', 'user': {'_id': '66a8d386fbde2b9cb1120393', 'avatarUrl': '/avatars/ccfb6e72764e547be2b3528713151693.svg', 'isPro': False, 'fullname': 'Tao Mei', 'user': 'GiantBision', 'type': 'user'}, 'name': 'Tao Mei', 'status': 'admin_assigned', 'statusLastChangedAt': '2025-01-16T08:55:14.020Z', 'hidden': False}], 'publishedAt': '2025-01-15T18:59:15.000Z', 'title': 'Ouroboros-Diffusion: Exploring Consistent Content Generation in\\n  Tuning-free Long Video Diffusion', 'summary': \"The first-in-first-out (FIFO) video diffusion, built on a pre-trained\\ntext-to-video model, has recently emerged as an effective approach for\\ntuning-free long video generation. This technique maintains a queue of video\\nframes with progressively increasing noise, continuously producing clean frames\\nat the queue's head while Gaussian noise is enqueued at the tail. However,\\nFIFO-Diffusion often struggles to keep long-range temporal consistency in the\\ngenerated videos due to the lack of correspondence modeling across frames. In\\nthis paper, we propose Ouroboros-Diffusion, a novel video denoising framework\\ndesigned to enhance structural and content (subject) consistency, enabling the\\ngeneration of consistent videos of arbitrary length. Specifically, we introduce\\na new latent sampling technique at the queue tail to improve structural\\nconsistency, ensuring perceptually smooth transitions among frames. To enhance\\nsubject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA)\\nmechanism, which aligns subjects across frames within short segments to achieve\\nbetter visual coherence. Furthermore, we introduce self-recurrent guidance.\\nThis technique leverages information from all previous cleaner frames at the\\nfront of the queue to guide the denoising of noisier frames at the end,\\nfostering rich and contextual global information interaction. Extensive\\nexperiments of long video generation on the VBench benchmark demonstrate the\\nsuperiority of our Ouroboros-Diffusion, particularly in terms of subject\\nconsistency, motion smoothness, and temporal consistency.\", 'upvotes': 2, 'discussionId': '678893e50a465aa0613dca2f'}, 'publishedAt': '2025-01-16T00:08:30.356Z', 'title': 'Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion', 'thumbnail': 'https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.09019.png', 'numComments': 1, 'submittedBy': {'_id': '645dbaa6f5760d1530d7580d', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/645dbaa6f5760d1530d7580d/Bqob8arLZoHIgMwNZpL9I.jpeg', 'fullname': 'Simeon Emanuilov', 'name': 's-emanuilov', 'type': 'user', 'isPro': True, 'isHf': False, 'isMod': False, 'followerCount': 14}, 'isAuthorParticipating': False}"
]